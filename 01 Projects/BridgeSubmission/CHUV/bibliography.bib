
@article{lencastre2024,
	title = {Identifying {Autism} {Gaze} {Patterns} in {Five}-{Second} {Data} {Records}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-4418},
	url = {https://www.mdpi.com/2075-4418/14/10/1047},
	doi = {10.3390/diagnostics14101047},
	abstract = {One of the most challenging problems when diagnosing autism spectrum disorder (ASD) is the need for long sets of data. Collecting data during such long periods is challenging, particularly when dealing with children. This challenge motivates the investigation of possible classifiers of ASD that do not need such long data sets. In this paper, we use eye-tracking data sets covering only 5 s and introduce one metric able to distinguish between ASD and typically developed (TD) gaze patterns based on such short time-series and compare it with two benchmarks, one using the traditional eye-tracking metrics and one state-of-the-art AI classifier. Although the data can only track possible disorders in visual attention and our approach is not a substitute to medical diagnosis, we find that our newly introduced metric can achieve an accuracy of 93\% in classifying eye gaze trajectories from children with ASD surpassing both benchmarks while needing fewer data. The classification accuracy of our method, using a 5 s data series, performs better than the standard metrics in eye-tracking and is at the level of the best AI benchmarks, even when these are trained with longer time series. We also discuss the advantages and limitations of our method in comparison with the state of the art: besides needing a low amount of data, this method is a simple, understandable, and straightforward criterion to apply, which often contrasts with “black box” AI methods.},
	language = {en},
	number = {10},
	urldate = {2024-06-05},
	journal = {Diagnostics},
	author = {Lencastre, Pedro and Lotfigolian, Maryam and Lind, Pedro G.},
	month = jan,
	year = {2024},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {autism, AI, eye-tracking, autism diagnosis, eye gaze dynamics, intelligent health},
	pages = {1047},
	file = {2024_Lencastre et al_Identifying Autism Gaze Patterns in Five-Second Data Records.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ToBeSorted\\2024_Lencastre et al_Identifying Autism Gaze Patterns in Five-Second Data Records.pdf:application/pdf},
}

@article{lencastre2024a,
	title = {Identifying {Autism} {Gaze} {Patterns} in {Five}-{Second} {Data} {Records}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-4418},
	url = {https://www.mdpi.com/2075-4418/14/10/1047},
	doi = {10.3390/diagnostics14101047},
	abstract = {One of the most challenging problems when diagnosing autism spectrum disorder (ASD) is the need for long sets of data. Collecting data during such long periods is challenging, particularly when dealing with children. This challenge motivates the investigation of possible classifiers of ASD that do not need such long data sets. In this paper, we use eye-tracking data sets covering only 5 s and introduce one metric able to distinguish between ASD and typically developed (TD) gaze patterns based on such short time-series and compare it with two benchmarks, one using the traditional eye-tracking metrics and one state-of-the-art AI classifier. Although the data can only track possible disorders in visual attention and our approach is not a substitute to medical diagnosis, we find that our newly introduced metric can achieve an accuracy of 93\% in classifying eye gaze trajectories from children with ASD surpassing both benchmarks while needing fewer data. The classification accuracy of our method, using a 5 s data series, performs better than the standard metrics in eye-tracking and is at the level of the best AI benchmarks, even when these are trained with longer time series. We also discuss the advantages and limitations of our method in comparison with the state of the art: besides needing a low amount of data, this method is a simple, understandable, and straightforward criterion to apply, which often contrasts with “black box” AI methods.},
	language = {en},
	number = {10},
	urldate = {2024-06-05},
	journal = {Diagnostics},
	author = {Lencastre, Pedro and Lotfigolian, Maryam and Lind, Pedro G.},
	month = jan,
	year = {2024},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {autism, AI, eye-tracking, autism diagnosis, eye gaze dynamics, intelligent health, obsidian},
	pages = {1047},
	file = {2024_Lencastre_Identifying Autism Gaze.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2024_Lencastre_Identifying Autism Gaze.pdf:application/pdf},
}

@article{uddin2023,
	title = {Exploring the psychological effects of {Metaverse} on mental health and well-being},
	volume = {25},
	issn = {1943-4294},
	url = {https://doi.org/10.1007/s40558-023-00259-8},
	doi = {10.1007/s40558-023-00259-8},
	abstract = {Recent advancements in the domain of virtual reality have culminated in the development of the Metaverse, a comprehensive virtual environment fostering interactions among individuals and digital entities. This study undertakes an analytical exploration of the Metaverse’s implications on mental health, overall well-being, and disability with a focused application to the tourism sector. The constructed model incorporates variables such as activity type within the Metaverse, usage frequency, and an individual’s existing mental health state and consideration for disability. Using Habitat Simulator and the Oculus Rift simulator, the research simulates diverse scenarios to discern the Metaverse’s influence on mental health, including its impact on individuals with disabilities in a tourism context. Our findings reveal a dichotomous impact: the nature of the engaged activity governs whether the effects on mental health and well-being, including those specific to disability, are beneficial or detrimental. The present research enhances the understanding of the Metaverse’s impact on mental health, well-being, and disability, and its potential to transform conventional tourism practices and marketing strategies. By emphasizing the activity type within its impact studies, the research provides insights for enhancing the beneficial effects and mitigating the harmful repercussions of the Metaverse, especially in tourism. The study also underscores the importance of establishing monitoring systems and personalized interventions to safeguard individuals’ well-being in the Metaverse, capitalizing on its potential to augment tourism experiences. This bears significance for policymakers, mental health practitioners, and tourism industry professionals in their collective efforts to encourage responsible usage and maximize the positive effects of the Metaverse. In contributing to the wider comprehension of the psychological effects of nascent technologies, this research accentuates their implications for individuals and industries, with a specific emphasis on the impact of the Metaverse on mental health, overall well-being, and disability in the tourism sector.},
	language = {en},
	number = {3},
	urldate = {2024-06-05},
	journal = {Information Technology \& Tourism},
	author = {Ud Din, Ikram and Almogren, Ahmad},
	month = sep,
	year = {2023},
	keywords = {Virtual reality, Mental health, Disability, Well-being, Metaverse},
	pages = {367--389},
	file = {2023_Ud Din_Exploring the psychological.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Metaverse\\2023_Ud Din_Exploring the psychological.pdf:application/pdf},
}

@article{cerasa2024,
	title = {Metaverse in {Mental} {Health}: {The} {Beginning} of a {Long} {History}},
	volume = {26},
	issn = {1535-1645},
	shorttitle = {Metaverse in {Mental} {Health}},
	url = {https://doi.org/10.1007/s11920-024-01501-8},
	doi = {10.1007/s11920-024-01501-8},
	abstract = {We review the first pilot studies applying metaverse-related technologies in psychiatric patients and discuss the rationale for using this complex federation of technologies to treat mental diseases. Concerning previous virtual-reality applications in medical care, metaverse technologies provide the unique opportunity to define, control, and shape virtual scenarios shared by multi-users to exploit the “synchronized brains” potential exacerbated by social interactions.},
	language = {en},
	number = {6},
	urldate = {2024-06-05},
	journal = {Current Psychiatry Reports},
	author = {Cerasa, Antonio and Gaggioli, Andrea and Pioggia, Giovanni and Riva, Giuseppe},
	month = jun,
	year = {2024},
	keywords = {Autism spectrum disorders, Brain-to-brain synchrony, Eating disorders, Metaverse, Predictive coding, Sexual disorders, Social brain},
	pages = {294--303},
	file = {2024_Cerasa_Metaverse in Mental Health.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Metaverse\\2024_Cerasa_Metaverse in Mental Health.pdf:application/pdf},
}

@article{goh2024,
	title = {Normative performance data on visual attention in neurotypical children: virtual reality assessment of cognitive and psychomotor development},
	volume = {5},
	issn = {2673-4192},
	shorttitle = {Normative performance data on visual attention in neurotypical children},
	url = {https://www.frontiersin.org/articles/10.3389/frvir.2024.1309176},
	doi = {10.3389/frvir.2024.1309176},
	abstract = {Virtual Reality (VR) is revolutionizing healthcare research and practice by offering innovative methodologies across various clinical conditions. Advances in VR technology enable the creation of controllable, multisensory 3D environments, making it an appealing tool for capturing and quantifying behavior in realistic scenarios. This paper details the application of VR as a tool for neurocognitive evaluation, specifically in attention process assessment, an area of relevance for informing the diagnosis of childhood health conditions such as Attention Deficit Hyperactivity Disorder (ADHD). The data presented focuses on attention performance results from a large sample (n=837) of neurotypical male and female children (ages 6-13) tested on a visual continuous performance task, administered within an immersive VR classroom environment. The results indicate systematic improvements on most metrics across the age span and sex differences are noted on key variables thought to reflect differential measures of hyperactivity and inattention in children with ADHD. This data was collected to create a normative baseline database for use to inform comparisons with the performances of children with ADHD to support diagnostic decision-making in this area. The results indicate that VR technology can provide a safe and viable option for testing attention processes in children under stimulus conditions that closely mimic ecologically relevant challenges found in everyday life. In response to these stimulus conditions, VR can support advanced methods for capturing and quantifying users' behavioral responses. VR offers a more systematic and objective approach for clinical assessment and intervention, and provides conceptual support for its use in a wide variety of healthcare contexts.},
	language = {English},
	urldate = {2024-06-05},
	journal = {Frontiers in Virtual Reality},
	author = {Goh, Crystal and Ma, Yu and Rizzo, Albert},
	month = apr,
	year = {2024},
	note = {Publisher: Frontiers},
	keywords = {virtual reality, ADHD, assessment, Attention Processes, virtual classroom},
	file = {2024_Goh_Normative performance data on.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2024_Goh_Normative performance data on.pdf:application/pdf},
}

@article{gupta2024,
	title = {{CAEVR}: {Biosignals}-{Driven} {Context}-{Aware} {Empathy} in {Virtual} {Reality}},
	volume = {30},
	issn = {1941-0506},
	shorttitle = {{CAEVR}},
	url = {https://ieeexplore.ieee.org/document/10458349},
	doi = {10.1109/TVCG.2024.3372130},
	abstract = {There is little research on how Virtual Reality (VR) applications can identify and respond meaningfully to users' emotional changes. In this paper, we investigate the impact of Context-Aware Empathic VR (CAEVR) on the emotional and cognitive aspects of user experience in VR. We developed a real-time emotion prediction model using electroencephalography (EEG), electrodermal activity (EDA), and heart rate variability (HRV) and used this in personalized and generalized models for emotion recognition. We then explored the application of this model in a context-aware empathic (CAE) virtual agent and an emotion-adaptive (EA) VR environment. We found a significant increase in positive emotions, cognitive load, and empathy toward the CAE agent, suggesting the potential of CAEVR environments to refine user-agent interactions. We identify lessons learned from this study and directions for future work.},
	number = {5},
	urldate = {2024-06-05},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Gupta, Kunal and Zhang, Yuewei and Gunasekaran, Tamil Selvan and Krishna, Nanditha and Pai, Yun Suen and Billinghurst, Mark},
	month = may,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {VR, Cognitive load, Electroencephalography, empathy, emotion, Brain modeling, context-aware, Emotion recognition, Heart rate variability, metaverse, physiology, Real-time systems, Solid modeling, virtual agents},
	pages = {2671--2681},
	file = {2024_Gupta_CAEVR.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Emotions\\2024_Gupta_CAEVR.pdf:application/pdf},
}

@article{zhang2024,
	title = {Swift-{Eye}: {Towards} {Anti}-blink {Pupil} {Tracking} for {Precise} and {Robust} {High}-{Frequency} {Near}-{Eye} {Movement} {Analysis} with {Event} {Cameras}},
	volume = {30},
	issn = {1941-0506},
	shorttitle = {Swift-{Eye}},
	url = {https://ieeexplore.ieee.org/document/10458392},
	doi = {10.1109/TVCG.2024.3372039},
	abstract = {Eye tracking has shown great promise in many scientific fields and daily applications, ranging from the early detection of mental health disorders to foveated rendering in virtual reality (VR). These applications all call for a robust system for high-frequency near-eye movement sensing and analysis in high precision, which cannot be guaranteed by the existing eye tracking solutions with CCD/CMOS cameras. To bridge the gap, in this paper, we propose Swift-Eye, an offline precise and robust pupil estimation and tracking framework to support high-frequency near-eye movement analysis, especially when the pupil region is partially occluded. Swift-Eye is built upon the emerging event cameras to capture the high-speed movement of eyes in high temporal resolution. Then, a series of bespoke components are designed to generate high-quality near-eye movement video at a high frame rate over kilohertz and deal with the occlusion over the pupil caused by involuntary eye blinks. According to our extensive evaluations on EV-Eye, a large-scale public dataset for eye tracking using event cameras, Swift-Eye shows high robustness against significant occlusion. It can improve the IoU and F1-score of the pupil estimation by 20\% and 12.5\% respectively, compared with the second-best competing approach, when over 80\% of the pupil region is occluded by the eyelid. Lastly, it provides continuous and smooth traces of pupils in extremely high temporal resolution and can support high-frequency eye movement analysis and a number of potential applications, such as mental health diagnosis, behaviour-brain association, etc. The implementation details and source codes can be found at https://github.com/ztysdu/Swift-Eye.},
	number = {5},
	urldate = {2024-06-05},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Zhang, Tongyu and Shen, Yiran and Zhao, Guangrong and Wang, Lin and Chen, Xiaoming and Bai, Lu and Zhou, Yuanfeng},
	month = may,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Mental health, Cameras, Tracking, event camera, Estimation, Eye tracking, feature fusion, Gaze tracking, Pupils, Streaming media},
	pages = {2077--2086},
	file = {2024_Zhang_Swift-Eye.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Event-Based\\2024_Zhang_Swift-Eye.pdf:application/pdf},
}

@article{moreno-arjonilla2024,
	title = {Eye-tracking on virtual reality: a survey},
	volume = {28},
	issn = {1434-9957},
	shorttitle = {Eye-tracking on virtual reality},
	url = {https://doi.org/10.1007/s10055-023-00903-y},
	doi = {10.1007/s10055-023-00903-y},
	abstract = {Virtual reality (VR) has evolved substantially beyond its initial remit of gaming and entertainment, catalyzed by advancements such as improved screen resolutions and more accessible devices. Among various interaction techniques introduced to VR, eye-tracking stands out as a pivotal development. It not only augments immersion but offers a nuanced insight into user behavior and attention. This precision in capturing gaze direction has made eye-tracking instrumental for applications far beyond mere interaction, influencing areas like medical diagnostics, neuroscientific research, educational interventions, and architectural design, to name a few. Though eye-tracking’s integration into VR has been acknowledged in prior reviews, its true depth, spanning the intricacies of its deployment to its broader ramifications across diverse sectors, has been sparsely explored. This survey undertakes that endeavor, offering a comprehensive overview of eye-tracking’s state of the art within the VR landscape. We delve into its technological nuances, its pivotal role in modern VR applications, and its transformative impact on domains ranging from medicine and neuroscience to marketing and education. Through this exploration, we aim to present a cohesive understanding of the current capabilities, challenges, and future potential of eye-tracking in VR, underscoring its significance and the novelty of our contribution.},
	language = {en},
	number = {1},
	urldate = {2024-06-05},
	journal = {Virtual Reality},
	author = {Moreno-Arjonilla, Jesús and López-Ruiz, Alfonso and Jiménez-Pérez, J. Roberto and Callejas-Aguilera, José E. and Jurado, Juan M.},
	month = feb,
	year = {2024},
	keywords = {Virtual reality, Perception, Attention, Eye-tracking},
	pages = {38},
	file = {2024_Moreno-Arjonilla_Eye-tracking on virtual.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2024_Moreno-Arjonilla_Eye-tracking on virtual.pdf:application/pdf},
}

@article{stretton2024,
	title = {Exploring mobile mixed reality for critical thinking in nursing and healthcare education: {A} systematic review},
	volume = {133},
	issn = {0260-6917},
	shorttitle = {Exploring mobile mixed reality for critical thinking in nursing and healthcare education},
	url = {https://www.sciencedirect.com/science/article/pii/S0260691723003660},
	doi = {10.1016/j.nedt.2023.106072},
	abstract = {Background
The shortage of nursing and healthcare clinical placements has prompted the investigation of ways to supplement authentic learning. Mobile mixed reality has become increasingly available, however, the affordances and design principles for the facilitation of critical thinking are yet to be explored.
Objective
To examine how mobile mixed reality facilitates critical thinking in nursing and healthcare higher education.
Design
Systematic review.
Review methods
A search in seven databases (MEDLINE, PsychINFO, AMED, ERIC, Scopus, Cochrane, and Web of Science) was conducted with 3488 titles and abstracts screened. The quality of the included studies was evaluated using the Mixed Methods Assessment Tool (MMAT).
Results
A total of 12 studies with 1108 participants were included. The breadth of healthcare disciplines was limited to five disciplines that utilised bespoke scenarios on head-mounted displays. Most scenarios were emergency or critical response, with limited time for pre-brief, debrief, or overall user time. Only two studies directly measured critical thinking, with others including indirect reference to diagnoses, interpretation, analysis, or evaluation of healthcare scenarios. Affordances and design principles for the future development of mobile mixed reality for critical thinking in nursing and healthcare higher education are identified.
Conclusions
While some pedagogical affordances of mobile mixed reality can be identified in a narrow number of healthcare disciplines, there remain to be limited valid measures of critical thinking used to quantify effectiveness. Future studies would benefit from considering scenarios beyond emergency and critical responses, including longitudinal studies that reflect the development of critical thinking over time, and exploration of co-designed scenarios with and by nursing and healthcare students.},
	urldate = {2024-06-05},
	journal = {Nurse Education Today},
	author = {Stretton, Todd and Cochrane, Thomas and Sevigny, Charles and Rathner, Joseph},
	month = feb,
	year = {2024},
	keywords = {Augmented reality, Virtual reality, Learning, Education, Mixed reality, Critical thinking, Curriculum, Premedical, Teaching},
	pages = {106072},
	file = {Stretton et al. - 2024 - Exploring mobile mixed reality for critical thinki.pdf:C\:\\Users\\antoine.widmer\\Zotero\\storage\\2JWYIJY8\\Stretton et al. - 2024 - Exploring mobile mixed reality for critical thinki.pdf:application/pdf},
}

@article{asmethajeyarani2023,
	title = {Eye {Tracking} {Biomarkers} for {Autism} {Spectrum} {Disorder} {Detection} using {Machine} {Learning} and {Deep} {Learning} {Techniques}: {Review}},
	volume = {108},
	issn = {1750-9467},
	shorttitle = {Eye {Tracking} {Biomarkers} for {Autism} {Spectrum} {Disorder} {Detection} using {Machine} {Learning} and {Deep} {Learning} {Techniques}},
	url = {https://www.sciencedirect.com/science/article/pii/S1750946723001289},
	doi = {10.1016/j.rasd.2023.102228},
	abstract = {Eye tracking is a promising tool for Autism Spectrum Disorder (ASD) detection in both children and adults. An important aspect of social communication is keeping eye contact, which is something that people with ASD frequently struggle with. Eye tracking can assess the duration of eye contact and the frequency and direction of gaze movements, offering quantifiable indicators of social communication deficits. People with ASD may also demonstrate other abnormalities in visual processing, such as an increased concentration on detail, sensory sensitivity, and trouble with complicated visual activities. These variations can be measured via Eye tracking, which offers critical information for the planning of therapy and diagnosis. The primary objective of this work is to provide a thorough description of the most recent studies that use Eye tracking combined with various Machine Learning (ML) and Deep Learning (DL) models for the detection of ASD. This will provide insights into the identification, and behavioral assessment, and distinguish between autistic people and those who are Typically Developing (TD). A detailed review of the various ML and DL models with their datasets and performance criteria is presented. Different types of eye movement datasets with diagnostic standards and eye tracker devices are also discussed. Finally, the study addresses the potential of gaze prediction in ASD patients for the design of interventions.},
	urldate = {2024-06-05},
	journal = {Research in Autism Spectrum Disorders},
	author = {Asmetha Jeyarani, R. and Senthilkumar, Radha},
	month = oct,
	year = {2023},
	keywords = {Deep learning, Machine learning, Autism Spectrum Disorder, Eye tracking, Biomarker},
	pages = {102228},
	file = {2023_Asmetha.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2023_Asmetha.pdf:application/pdf},
}

@article{leharanger2023,
	title = {Familiarization with {Mixed} {Reality} for {Individuals} with {Autism} {Spectrum} {Disorder}: {An} {Eye} {Tracking} {Study}},
	volume = {23},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	shorttitle = {Familiarization with {Mixed} {Reality} for {Individuals} with {Autism} {Spectrum} {Disorder}},
	url = {https://www.mdpi.com/1424-8220/23/14/6304},
	doi = {10.3390/s23146304},
	abstract = {Mixed Reality (MR) technology is experiencing significant growth in the industrial and healthcare sectors. The headset HoloLens 2 displays virtual objects (in the form of holograms) in the user’s environment in real-time. Individuals with Autism Spectrum Disorder (ASD) exhibit, according to the DSM-5, persistent deficits in communication and social interaction, as well as a different sensitivity compared to neurotypical (NT) individuals. This study aims to propose a method for familiarizing eleven individuals with severe ASD with the HoloLens 2 headset and the use of MR technology through a tutorial. The secondary objective is to obtain quantitative learning indicators in MR, such as execution speed and eye tracking (ET), by comparing individuals with ASD to neurotypical individuals. We observed that 81.81\% of individuals with ASD successfully familiarized themselves with MR after several sessions. Furthermore, the visual activity of individuals with ASD did not differ from that of neurotypical individuals when they successfully familiarized themselves. This study thus offers new perspectives on skill acquisition indicators useful for supporting neurodevelopmental disorders. It contributes to a better understanding of the neural mechanisms underlying learning in MR for individuals with ASD.},
	language = {en},
	number = {14},
	urldate = {2024-06-05},
	journal = {Sensors},
	author = {Leharanger, Maxime and Rodriguez Martinez, Eder Alejandro and Balédent, Olivier and Vandromme, Luc},
	month = jan,
	year = {2023},
	note = {Number: 14
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {mixed reality, learning, training, autism, ASD, eye tracking, familiarization, visual attention},
	pages = {6304},
	file = {2023_Leharanger_Familiarization with Mixed.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2023_Leharanger_Familiarization with Mixed.pdf:application/pdf},
}

@article{wolf2023,
	title = {Eye-tracking paradigms for the assessment of mild cognitive impairment: a systematic review},
	volume = {14},
	issn = {1664-1078},
	shorttitle = {Eye-tracking paradigms for the assessment of mild cognitive impairment},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1197567/full},
	doi = {10.3389/fpsyg.2023.1197567},
	abstract = {{\textless}p{\textgreater}Mild cognitive impairment (MCI), representing the ‘transitional zone’ between normal cognition and dementia, has become a novel topic in clinical research. Although early detection is crucial, it remains logistically challenging at the same time. While traditional pen-and-paper tests require in-depth training to ensure standardized administration and accurate interpretation of findings, significant technological advancements are leading to the development of procedures for the early detection of Alzheimer’s disease (AD) and facilitating the diagnostic process. Some of the diagnostic protocols, however, show significant limitations that hamper their widespread adoption. Concerns about the social and economic implications of the increasing incidence of AD underline the need for reliable, non-invasive, cost-effective, and timely cognitive scoring methodologies. For instance, modern clinical studies report significant oculomotor impairments among patients with MCI, who perform poorly in visual paired-comparison tasks by ascribing less attentional resources to novel stimuli. To accelerate the Global Action Plan on the Public Health Response to Dementia 2017–2025, this work provides an overview of research on saccadic and exploratory eye-movement deficits among older adults with MCI. The review protocol was drafted based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. Electronic databases were systematically searched to identify peer-reviewed articles published between 2017 and 2022 that examined visual processing in older adults with MCI and reported gaze parameters as potential biomarkers. Moreover, following the contemporary trend for remote healthcare technologies, we reviewed studies that implemented non-commercial eye-tracking instrumentation in order to detect information processing impairments among the MCI population. Based on the gathered literature, eye-tracking-based paradigms may ameliorate the screening limitations of traditional cognitive assessments and contribute to early AD detection. However, in order to translate the findings pertaining to abnormal gaze behavior into clinical applications, it is imperative to conduct longitudinal investigations in both laboratory-based and ecologically valid settings.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-06-05},
	journal = {Frontiers in Psychology},
	author = {Wolf, Alexandra and Tripanpitak, Kornkanok and Umeda, Satoshi and Otake-Matsuura, Mihoko},
	month = jul,
	year = {2023},
	note = {Publisher: Frontiers},
	keywords = {Dementia, Eye-tracking, AD, Alzheimer's disease, biomarker, cognitive scoring, eye movement, gaze, Information Processing, MCI, Mild Cognitive Impairment, remote healthcare, screening},
	file = {2023_Wolf_Eye-tracking paradigms for.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2023_Wolf_Eye-tracking paradigms for.pdf:application/pdf},
}

@article{zhao2023,
	title = {{EV}-{Eye}: {Rethinking} {High}-frequency {Eye} {Tracking} through the {Lenses} of {Event} {Cameras}},
	volume = {36},
	shorttitle = {{EV}-{Eye}},
	url = {https://papers.nips.cc/paper_files/paper/2023/hash/c41b5d8c1ba15b2aa83e4fa1541f02c8-Abstract-Datasets_and_Benchmarks.html},
	language = {en},
	urldate = {2024-06-05},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zhao, Guangrong and Yang, Yurun and Liu, Jingwei and Chen, Ning and Shen, Yiran and Wen, Hongkai and Lan, Guohao},
	month = dec,
	year = {2023},
	pages = {62169--62182},
	file = {2023_Zhao_EV-Eye.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Event-Based\\2023_Zhao_EV-Eye.pdf:application/pdf},
}

@inproceedings{berlincioni2023,
	title = {Neuromorphic {Event}-based {Facial} {Expression} {Recognition}},
	url = {https://ieeexplore.ieee.org/document/10208675},
	doi = {10.1109/CVPRW59228.2023.00432},
	abstract = {Recently, event cameras have shown large applicability in several computer vision fields especially concerning tasks that require high temporal resolution. In this work, we investigate the usage of such kind of data for emotion recognition by presenting NEFER, a dataset for Neuromorphic Event-based Facial Expression Recognition. NEFER is composed of paired RGB and event videos representing human faces labeled with the respective emotions and also annotated with face bounding boxes and facial landmarks. We detail the data acquisition process as well as providing a baseline method for RGB and event data. The collected data captures subtle micro-expressions, which are hard to spot with RGB data, yet emerge in the event domain. We report a double recognition accuracy for the event-based approach, proving the effectiveness of a neuromorphic approach for analyzing fast and hardly detectable expressions and the emotions they conceal.},
	urldate = {2024-06-05},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Berlincioni, Lorenzo and Cultrera, Luca and Albisani, Chiara and Cresti, Lisa and Leonardo, Andrea and Picchioni, Sara and Becattini, Federico and Del Bimbo, Alberto},
	month = jun,
	year = {2023},
	note = {ISSN: 2160-7516},
	keywords = {Computer vision, Face recognition, Cameras, Emotion recognition, Conferences, Data acquisition, Neuromorphics},
	pages = {4109--4119},
	file = {2023_Berlincioni_Neuromorphic Event-based.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Event-Based\\2023_Berlincioni_Neuromorphic Event-based.pdf:application/pdf},
}

@article{tahrisqalli2023,
	title = {Eye tracking technology in medical practice: a perspective on its diverse applications},
	volume = {5},
	issn = {2673-3129},
	shorttitle = {Eye tracking technology in medical practice},
	url = {https://www.frontiersin.org/articles/10.3389/fmedt.2023.1253001},
	doi = {10.3389/fmedt.2023.1253001},
	abstract = {Eye tracking technology has emerged as a valuable tool in the field of medicine, offering a wide range of applications across various disciplines. This perspective article aims to provide a comprehensive overview of the diverse applications of eye tracking technology in medical practice.By summarizing the latest research findings, this article explores the potential of eye tracking technology in enhancing diagnostic accuracy, assessing and improving medical performance, as well as improving rehabilitation outcomes. Additionally, it highlights the role of eye tracking in neurology, cardiology, pathology, surgery, as well as rehabilitation, offering objective measures for various medical conditions. Furthermore, the article discusses the utility of eye tracking in autism spectrum disorders, attention-deficit/hyperactivity disorder (ADHD), and human-computer interaction in medical simulations and training. Ultimately, this perspective article underscores the transformative impact of eye tracking technology on medical practice and suggests future directions for its continued development and integration.},
	language = {English},
	urldate = {2024-06-05},
	journal = {Frontiers in Medical Technology},
	author = {Tahri Sqalli, Mohammed and Aslonov, Begali and Gafurov, Mukhammadjon and Mukhammadiev, Nurmukhammad and Sqalli Houssaini, Yahya},
	month = nov,
	year = {2023},
	note = {Publisher: Frontiers},
	keywords = {Rehabilitation, Simulations, training, human-computer interaction, ADHD, eye tracking, Autism Spectrum Disorders, Cognitive Function, Diagnostic accuracy, Medical practice, Neurosurgery, Pathology},
	file = {2023_Tahri Sqalli_Eye tracking technology in.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2023_Tahri Sqalli_Eye tracking technology in.pdf:application/pdf},
}

@article{pastel2023,
	title = {Application of eye-tracking systems integrated into immersive virtual reality and possible transfer to the sports sector - {A} systematic review},
	volume = {82},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-022-13474-y},
	doi = {10.1007/s11042-022-13474-y},
	abstract = {In recent years, Virtual Reality (VR) has become a valuable tool in rehabilitation and sports training applications. New technologies offer opportunities to combine various systems and use them for sports-related scientific purposes. For instance, examining the visual perception of athletes within a standardized environment could be helpful to understand the differences between novices and experts in their visual behavior and could further reveal possible training applications for enhancing athletes’ visual attention. The current systematic literature review thematizes the importance of eye-tracking (ET) systems’ usage integrated into head-mounted displays (HMDs) in virtual environments for further inclusion in sports-related usage. An overview of possible implementations is given, and additional recommendations for using the combined technic regarding sports are made. Although only one study examined gaze behavior during sports activity within a standardized virtual environment, 38 relevant papers were identified using the ET systems integrated into the HMDs, which ideas can be transferred to the sports sector. The increased usability and fidelity in the virtual environment enabled through the combined technology were illustrated, and different approaches were listed in using and calculating gaze parameters. This literature review examines the possibility of integrating ET in VR, which can be further used to improve usability, interaction methods, image presentation, and visual perception analyses within future physical training scenarios. The compiled studies have shown that the existing methods are feasible due to the performance of the integrated ET systems but still need to be improved for practical use.},
	language = {en},
	number = {3},
	urldate = {2024-06-05},
	journal = {Multimedia Tools and Applications},
	author = {Pastel, Stefan and Marlok, Josua and Bandow, Nicole and Witte, Kerstin},
	month = jan,
	year = {2023},
	keywords = {Virtual reality, Head-mounted display, Eye-tracking, Gaze behavior, Visual perception},
	pages = {4181--4208},
	file = {2023_Pastel_Application of eye-tracking.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2023_Pastel_Application of eye-tracking.pdf:application/pdf},
}

@article{adhanom2023,
	title = {Eye {Tracking} in {Virtual} {Reality}: a {Broad} {Review} of {Applications} and {Challenges}},
	volume = {27},
	issn = {1434-9957},
	shorttitle = {Eye {Tracking} in {Virtual} {Reality}},
	url = {https://doi.org/10.1007/s10055-022-00738-z},
	doi = {10.1007/s10055-022-00738-z},
	abstract = {Eye tracking is becoming increasingly available in head-mounted virtual reality displays with various headsets with integrated eye trackers already commercially available. The applications of eye tracking in virtual reality are highly diversified and span multiple disciplines. As a result, the number of peer-reviewed publications that study eye tracking applications has surged in recent years. We performed a broad review to comprehensively search academic literature databases with the aim of assessing the extent of published research dealing with applications of eye tracking in virtual reality, and highlighting challenges, limitations and areas for future research.},
	language = {en},
	number = {2},
	urldate = {2024-06-05},
	journal = {Virtual Reality},
	author = {Adhanom, Isayas Berhe and MacNeilage, Paul and Folmer, Eelke},
	month = jun,
	year = {2023},
	keywords = {Virtual reality, Eye tracking},
	pages = {1481--1505},
	file = {2023_Adhanom_Eye Tracking in Virtual.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2023_Adhanom_Eye Tracking in Virtual.pdf:application/pdf},
}

@inproceedings{wei2023,
	address = {New York, NY, USA},
	series = {{IMX} '23},
	title = {A {Preliminary} {Study} of the {Eye} {Tracker} in the {Meta} {Quest} {Pro}},
	isbn = {979-8-4007-0028-6},
	url = {https://dl.acm.org/doi/10.1145/3573381.3596467},
	doi = {10.1145/3573381.3596467},
	abstract = {This paper presents the preliminary results of an accuracy testing of the Meta Quest Pro’s eye tracker. We conducted user testing to evaluate the spatial accuracy, spatial precision and subjective performance under head-free and head-restrained conditions. Our measurements indicated an average accuracy of 1.652° with a precision of 0.699° (standard deviation) and 0.849° (root mean square) for a visual field spanning 15° during head-free. The signal quality of Quest Pro’s eye-tracker is comparable to existing AR/VR eye-tracking headsets. Notably, careful considerations are required when designing the size of scene objects, mapping areas of interest, and determining the interaction flow. Researchers should also be cautious about interpreting the fixation results when multiple targets are within close proximity. Further investigation and better specification information transparency are needed to establish its capabilities and limitations.},
	urldate = {2024-06-05},
	booktitle = {Proceedings of the 2023 {ACM} {International} {Conference} on {Interactive} {Media} {Experiences}},
	publisher = {Association for Computing Machinery},
	author = {Wei, Shu and Bloemers, Desmond and Rovira, Aitor},
	month = aug,
	year = {2023},
	keywords = {Accuracy, Eye-tracking, Meta Quest Pro, Precision, Virtual Reality (VR)},
	pages = {216--221},
	file = {2023_Wei_A Preliminary Study of the.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2023_Wei_A Preliminary Study of the.pdf:application/pdf},
}

@inproceedings{guo2023,
	address = {New York, NY, USA},
	series = {{FME} '23},
	title = {{GLEFFN}: {A} {Global}-{Local} {Event} {Feature} {Fusion} {Network} for {Micro}-{Expression} {Recognition}},
	isbn = {979-8-4007-0285-3},
	shorttitle = {{GLEFFN}},
	url = {https://dl.acm.org/doi/10.1145/3607829.3616446},
	doi = {10.1145/3607829.3616446},
	abstract = {Micro-expressions are facial movements of short duration and low amplitude, which, upon analysis, can reveal genuine human emotions. However, the low frame rate of frame-based cameras hinders the further advancement of micro-expression recognition (MER). A novel technology, event-based cameras, boasting high frame rates and low latency, proves suitable for the MER task but remains challenging to obtain. In this article, a local event feature, namely the local count image, is proposed. This feature is calculated from up-sampled video using the SloMo method. Additionally, a global-local event feature fusion network is constructed, wherein the local count image and the global dense optical flow are merged to map deeper features and effectively address the MER task. Experimental results demonstrate that the proposed light-weighted method outperforms state-of-the-art approaches across multiple datasets. To our best knowledges that this work marks the first successful attempt to solve the MER task from an event perspective, thus facilitating the future promotion of event-based camera technology and providing inspiration for future research endeavors in related domains.},
	urldate = {2024-06-05},
	booktitle = {Proceedings of the 3rd {Workshop} on {Facial} {Micro}-{Expression}: {Advanced} {Techniques} for {Multi}-{Modal} {Facial} {Expression} {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Guo, Cunhan and Huang, Heyan},
	month = oct,
	year = {2023},
	keywords = {event feature, micro-expression, neural network, relation module},
	pages = {17--24},
	file = {2023_Guo_GLEFFN.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Emotions\\2023_Guo_GLEFFN.pdf:application/pdf},
}

@inproceedings{zou2021,
	title = {{EventHPE}: {Event}-based {3D} {Human} {Pose} and {Shape} {Estimation}},
	shorttitle = {{EventHPE}},
	url = {https://ieeexplore.ieee.org/document/9710646},
	doi = {10.1109/ICCV48922.2021.01081},
	abstract = {Event camera is an emerging imaging sensor for capturing dynamics of moving objects as events, which motivates our work in estimating 3D human pose and shape from the event signals. Events, on the other hand, have their unique challenges: rather than capturing static body postures, the event signals are best at capturing local motions. This leads us to propose a two-stage deep learning approach, called EventHPE. The first-stage, FlowNet, is trained by unsupervised learning to infer optical flow from events. Both events and optical flow are closely related to human body dynamics, which are fed as input to the ShapeNet in the second stage, to estimate 3D human shapes. To mitigate the discrepancy between image-based flow (optical flow) and shape-based flow (vertices movement of human body shape), a novel flow coherence loss is introduced by exploiting the fact that both flows are originated from the identical human motion. An in-house event-based 3D human dataset is curated that comes with 3D pose and shape annotations, which is by far the largest one to our knowledge. Empirical evaluations on DHP19 dataset and our in-house dataset demonstrate the effectiveness of our approach.},
	urldate = {2024-06-05},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Zou, Shihao and Guo, Chuan and Zuo, Xinxin and Wang, Sen and Wang, Pengyu and Hu, Xiaoqin and Chen, Shoushun and Gong, Minglun and Cheng, Li},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {Computer vision, Deep learning, Three-dimensional displays, Datasets and evaluation, Dynamics, Gestures and body pose, Image motion analysis, Optical losses, Shape, R, rcso},
	pages = {10976--10985},
	file = {2021_Zou_EventHPE.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Body-Tracking\\2021_Zou_EventHPE.pdf:application/pdf},
}

@misc{zou2023,
	title = {Event-based {Human} {Pose} {Tracking} by {Spiking} {Spatiotemporal} {Transformer}},
	url = {http://arxiv.org/abs/2303.09681},
	doi = {10.48550/arXiv.2303.09681},
	abstract = {Event camera, as an emerging biologically-inspired vision sensor for capturing motion dynamics, presents new potential for 3D human pose tracking, or video-based 3D human pose estimation. However, existing works in pose tracking either require the presence of additional gray-scale images to establish a solid starting pose, or ignore the temporal dependencies all together by collapsing segments of event streams to form static event frames. Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs, a.k.a. dense deep learning) has been showcased in many event-based tasks, the use of ANNs tends to neglect the fact that compared to the dense frame-based image sequences, the occurrence of events from an event camera is spatiotemporally much sparser. Motivated by the above mentioned issues, we present in this paper a dedicated end-to-end sparse deep learning approach for event-based pose tracking: 1) to our knowledge this is the first time that 3D human pose tracking is obtained from events only, thus eliminating the need of accessing to any frame-based images as part of input; 2) our approach is based entirely upon the framework of Spiking Neural Networks (SNNs), which consists of Spike-Element-Wise (SEW) ResNet and a novel Spiking Spatiotemporal Transformer; 3) a large-scale synthetic dataset is constructed that features a broad and diverse set of annotated 3D human motions, as well as longer hours of event stream data, named SynEventHPD. Empirical experiments demonstrate that, with superior performance over the state-of-the-art (SOTA) ANNs counterparts, our approach also achieves a significant computation reduction of 80\% in FLOPS. Furthermore, our proposed method also outperforms SOTA SNNs in the regression task of human pose tracking. Our implementation is available at https://github.com/JimmyZou/HumanPoseTracking\_SNN and dataset will be released upon paper acceptance.},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Zou, Shihao and Mu, Yuxuan and Zuo, Xinxin and Wang, Sen and Cheng, Li},
	month = sep,
	year = {2023},
	note = {arXiv:2303.09681 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {2023_Zou_Event-based Human Pose.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Body-Tracking\\2023_Zou_Event-based Human Pose.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\6G3FSP28\\2303.html:text/html},
}

@article{zhang2023,
	title = {Facial expression recognition in virtual reality environments: challenges and opportunities},
	volume = {14},
	issn = {1664-1078},
	shorttitle = {Facial expression recognition in virtual reality environments},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1280136/full},
	doi = {10.3389/fpsyg.2023.1280136},
	abstract = {{\textless}p{\textgreater}This study delved into the realm of facial emotion recognition within virtual reality (VR) environments. Using a novel system with MobileNet V2, a lightweight convolutional neural network, we tested emotion detection on 15 university students. High recognition rates were observed for emotions like “Neutral”, “Happiness”, “Sadness”, and “Surprise”. However, the model struggled with 'Anger' and 'Fear', often confusing them with “neutral”. These discrepancies might be attributed to overlapping facial indicators, limited training samples, and the precision of the devices used. Nonetheless, our research underscores the viability of using facial emotion recognition technology in VR and recommends model improvements, the adoption of advanced devices, and a more holistic approach to foster the future development of VR emotion recognition.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-06-05},
	journal = {Frontiers in Psychology},
	author = {Zhang, Zhihui and Fort, Josep M. and Giménez Mateu, Lluis},
	month = oct,
	year = {2023},
	note = {Publisher: Frontiers},
	keywords = {Virtual reality (VR), VR devices, emotion, facial emotion recognition, Quest Pro, Vision Pro},
	file = {2023_Zhang_Facial expression recognition.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Emotions\\2023_Zhang_Facial expression recognition.pdf:application/pdf},
}

@article{nidhi2023,
	title = {From methods to datasets: a detailed study on facial emotion recognition},
	volume = {53},
	issn = {1573-7497},
	shorttitle = {From methods to datasets},
	url = {https://doi.org/10.1007/s10489-023-05052-y},
	doi = {10.1007/s10489-023-05052-y},
	abstract = {Human ideas and sentiments are mirrored in facial expressions. Facial expression recognition (FER) is a crucial type of visual data that can be utilized to deduce a person’s emotional state. It gives the spectator a plethora of social cues, such as the viewer’s focus of attention, emotion, motivation, and intention. It’s said to be a powerful instrument for silent communication. AI-based facial recognition systems can be deployed at different areas like bus stations, railway stations, airports, or stadiums to help security forces identify potential threats. There has been a lot of research done in this area. But, it lacks a detailed review of the literature that highlights and analyses the previous work in FER (including work on compound emotion and micro-expressions), and a comparative analysis of different models applied to available datasets, further identifying aligned future directions. So, this paper includes a comprehensive overview of different models that can be used in the field of FER and a comparative study of the traditional methods based on hand-crafted feature extraction and deep learning methods in terms of their advantages and disadvantages which distinguishes our work from existing review studies.This paper also brings you to an eye on the analysis of different FER systems, the performance of different models on available datasets, evaluation of the classification performance of traditional and deep learning algorithms in the context of facial emotion recognition which reveals a good understanding of the classifier’s characteristics. Along with the proposed models, this study describes the commonly used datasets showing the year-wise performance achieved by state-of-the-art methods which lacks in the existing manuscripts. At last, the authors itemize recognized research gaps and challenges encountered by researchers which can be considered in future research work.},
	language = {en},
	number = {24},
	urldate = {2024-06-05},
	journal = {Applied Intelligence},
	author = {{Nidhi} and Verma, Bindu},
	month = dec,
	year = {2023},
	keywords = {Deep learning, Classification, Emotion recognition, Facial action unit, Facial expression analysis},
	pages = {30219--30249},
	file = {2023_Nidhi_From methods to datasets.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Emotions\\2023_Nidhi_From methods to datasets.pdf:application/pdf},
}

@article{fida2023,
	title = {Real time emotions recognition through facial expressions},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-023-16722-x},
	doi = {10.1007/s11042-023-16722-x},
	abstract = {Human behavior is deeply influenced by emotions. Detection of emotions plays a pivotal role in understanding how individuals respond to various stimuli, such as reading text, encompassing feelings of anger, anxiety, confusion, or nervousness. Real-time facial emotion detection during online text reading represents an innovative approach for receiving immediate feedback based on readers’ emotional responses. Real-time emotion detection finds applications in interactive displays and holds immense potential for online learning platforms, where it can be utilized to analyze students’ emotional states and gauge their level of comprehension. Despite vast existing literature on emotion detection, real-time emotion detection is not very well studied. This study demonstrates the design and implementation of face emotion detection for students while they are using online learning platforms. The primary objective is capturing human emotions and storing them in the database after five seconds while they are reading online text. The system is implemented using SSD based on VB.NetV1. The proposed system has strong relevance for integration with online web applications to detect learners’ real-time emotions. Experiments are performed using CK+ and JAFFE face datasets and results show 96.46\% and 98.43\% accuracy, respectively. The system not only provides accurate results but also enables high-quality, robust, and real-time feedback based on the facial expressions of readers, facilitating a deeper understanding of students’ emotional engagement during their online learning experiences.},
	language = {en},
	urldate = {2024-06-05},
	journal = {Multimedia Tools and Applications},
	author = {Fida, Alisha and Umer, Muhammad and Saidani, Oumaima and Hamdi, Monia and Alnowaiser, Khaled and Bisogni, Carmen and Abate, Andrea F. and Ashraf, Imran},
	month = sep,
	year = {2023},
	keywords = {Facial expressions, Design science research, Emotions detection, Human behavior, Online learning},
	file = {2023_Fida_Real time emotions.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Emotions\\2023_Fida_Real time emotions.pdf:application/pdf},
}

@article{xie2023,
	title = {An {Overview} of {Facial} {Micro}-{Expression} {Analysis}: {Data}, {Methodology} and {Challenge}},
	volume = {14},
	issn = {1949-3045},
	shorttitle = {An {Overview} of {Facial} {Micro}-{Expression} {Analysis}},
	url = {https://ieeexplore.ieee.org/document/9684697},
	doi = {10.1109/TAFFC.2022.3143100},
	abstract = {Facial micro-expressions indicate brief and subtle facial movements that appear during emotional communication. In comparison to macro-expressions, micro-expressions are more challenging to be analyzed due to the short span of time and the fine-grained changes. In recent years, micro-expression recognition (MER) has drawn much attention because it can benefit a wide range of applications, e.g., police interrogation, clinical diagnosis, depression analysis, and business negotiation. In this survey, we offer a fresh overview to discuss new research directions and challenges these days for MER tasks. For example, we review MER approaches from three novel aspects: macro-to-micro adaptation, recognition based on key apex frames, and recognition based on facial action units. Moreover, to mitigate the problem of limited and biased ME data, synthetic data generation is surveyed for the diversity enrichment of micro-expression data. Since micro-expression spotting can boost micro-expression analysis, the state-of-the-art spotting works are also introduced in this paper. At last, we discuss the challenges in MER research and provide potential solutions as well as possible directions for further investigation.},
	number = {3},
	urldate = {2024-06-05},
	journal = {IEEE Transactions on Affective Computing},
	author = {Xie, Hong-Xia and Lo, Ling and Shuai, Hong-Han and Cheng, Wen-Huang},
	month = jul,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {survey, Face recognition, Task analysis, deep learning, action units, Facial micro-expression, recognition, spotting, Databases, Emotion recognition, Gold, Image coding, Licenses},
	pages = {1857--1875},
	file = {2023_Xie_An Overview of Facial.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Emotions\\2023_Xie_An Overview of Facial.pdf:application/pdf},
}

@inproceedings{kari2023,
	address = {New York, NY, USA},
	series = {{UIST} '23},
	title = {Scene {Responsiveness} for {Visuotactile} {Illusions} in {Mixed} {Reality}},
	isbn = {979-8-4007-0132-0},
	url = {https://dl.acm.org/doi/10.1145/3586183.3606825},
	doi = {10.1145/3586183.3606825},
	abstract = {Manipulating their environment is one of the fundamental actions that humans, and actors more generally, perform. Yet, today’s mixed reality systems enable us to situate virtual content in the physical scene but fall short of expanding the visual illusion to believable environment manipulations. In this paper, we present the concept and system of Scene Responsiveness, the visual illusion that virtual actions affect the physical scene. Using co-aligned digital twins for coherence-preserving just-in-time virtualization of physical objects in the environment, Scene Responsiveness allows actors to seemingly manipulate physical objects as if they were virtual. Based on Scene Responsiveness, we propose two general types of end to-end illusionary experiences that ensure visuotactile consistency through the presented techniques of object elusiveness and object rephysicalization. We demonstrate how our Daydreaming illusion enables virtual characters to enter the scene through a physically closed door and vandalize the physical scene, or users to enchant and summon far-away physical objects. In a user evaluation of our Copperfield illusion, we found that Scene Responsiveness can be rendered so convincingly that it lends itself to magic tricks. We present our system architecture and conclude by discussing the implications of scene-responsive mixed reality for gaming and telepresence.},
	urldate = {2024-06-05},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Kari, Mohamed and Schütte, Reinhard and Sodhi, Raj},
	month = oct,
	year = {2023},
	keywords = {Mixed reality, situated computing, spatial computing;},
	pages = {1--15},
	file = {2023_Kari_Scene Responsiveness for.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\XR\\2023_Kari_Scene Responsiveness for.pdf:application/pdf},
}

@article{usmani2022,
	title = {Future of mental health in the metaverse},
	volume = {35},
	issn = {2517-729X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9472101/},
	doi = {10.1136/gpsych-2022-100825},
	abstract = {The metaverse and non-fungible tokens (NFTs) were some of the hottest tech terms in 2021, according to a Google Trends search. Our review aims to describe the metaverse and NFTs in the context of their potential application in the treatment of mental health disorders. Advancements in technology have been changing human lives at an ever-increasing pace. Metaverse, also known as the three-dimensional (3D) internet, is the convergence of virtual reality (VR) and physical reality in a digital space. It could potentially change the internet as we know it, with NFTs as the key building blocks in the new expansive virtual ecosystem. This immersive 3D virtual world boasts the features of the real world with the added ability to change the surrounding environment according to individual needs and requirements. VR, augmented reality (AR) and mixed reality (MR) have been employed as tools in the treatment of various mental health disorders for the past decade. Studies have reported positive results on their effectiveness in the diagnosis and treatment of mental health disorders. VR/AR/MR have been hailed as a solution to the acute shortage of mental health professionals and the lack of access to mental healthcare. But, on the flip side, young adults tend to spend a significant amount of time playing 3D immersive games and using social media, which can lead to insecurity, anxiety, depression, and behavioural addiction. Additionally, endless scrolling through social media platforms negatively affects individuals' attention span as well as aggravating the symptoms of adolescents with attention deficit hyperactivity disorder., We aimed to explore the ramifications of expanding applications of the metaverse on mental health. So far, no other review has explored the future of mental health in the context of the metaverse.},
	number = {4},
	urldate = {2024-06-05},
	journal = {General Psychiatry},
	author = {Usmani, Sadia Suhail and Sharath, Medha and Mehendale, Meghana},
	month = jul,
	year = {2022},
	pmid = {36189180},
	pmcid = {PMC9472101},
	pages = {e100825},
	file = {2022_Usmani_Future of mental health in.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Metaverse\\2022_Usmani_Future of mental health in.pdf:application/pdf},
}

@article{cerasa2022,
	title = {The promise of the metaverse in mental health: the new era of {MEDverse}},
	volume = {8},
	issn = {2405-8440},
	shorttitle = {The promise of the metaverse in mental health},
	url = {https://www.sciencedirect.com/science/article/pii/S240584402203050X},
	doi = {10.1016/j.heliyon.2022.e11762},
	abstract = {Since Mark Zuckerberg’s announcement about the development of new three-dimensional virtual worlds for social communication, a great debate has been raised about the promise of such a technology. The metaverse, a term formed by combining meta and universe, could open a new era in mental health, mainly in psychological disorders, where the creation of a full-body illusion via digital avatar could promote healthcare and personal well-being. Patients affected by body dysmorphism symptoms (i.e., eating disorders), social deficits (i.e. autism) could greatly benefit from this kind of technology. However, it is not clear which advantage the metaverse would have in treating psychological disorders with respect to the well-known and effective virtual reality (VR) exposure therapy. Indeed, in the last twenty years, a plethora of studies have demonstrated the effectiveness of VR technology in reducing symptoms of pain, anxiety, stress, as well as, in improving cognitive and social skills. We hypothesize that the metaverse will offer more opportunities, such as a more complex, virtual realm where sensory inputs, and recurrent feedback, mediated by a “federation” of multiple technologies - e.g., artificial intelligence, tangible interfaces, Internet of Things and blockchain, can be reinterpreted for facilitating a new kind of communication overcoming self-body representation. However, nowadays a clear starting point does not exist. For this reason, it is worth defining a theoretical framework for applying this new kind of technology in a social neuroscience context for developing accurate solutions to mental health in the future.},
	number = {11},
	urldate = {2024-06-05},
	journal = {Heliyon},
	author = {Cerasa, Antonio and Gaggioli, Andrea and Marino, Flavia and Riva, Giuseppe and Pioggia, Giovanni},
	month = nov,
	year = {2022},
	keywords = {Autism, Mental disorders, Metaverse, Body dysmorphism disorders},
	pages = {e11762},
	file = {2022_Cerasa_The promise of the metaverse.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Metaverse\\2022_Cerasa_The promise of the metaverse.pdf:application/pdf},
}

@inproceedings{stoffregen2022,
	title = {Event-{Based} {Kilohertz} {Eye} {Tracking} using {Coded} {Differential} {Lighting}},
	url = {https://ieeexplore.ieee.org/document/9706617},
	doi = {10.1109/WACV51458.2022.00399},
	abstract = {Pixels in an event camera operate asynchronously and independently, reporting changes in intensity as events - tuples of (x,y) position, polarity s and timestamp t at microsecond resolution. Event cameras operate at low power (≈ 5mW) and respond to changes in the scene with a latency on the order of microseconds. These properties make event cameras an exciting candidate for eye tracking sensors on mobile platforms such as Augmented/Virtual Reality (AR/VR) headsets, since these systems have hard real-time and power constraints. One proven method for eye tracking and gaze estimation is corneal glint detection. We exploit the fact that corneal glint tracking only requires a sparse set of pixels in the image, by making use of the natural sparsity of event cameras, which only detect changes in the scene. To enhance this effect, we design an illumination scheme, Coded Differential Lighting, which enhances specular reflections, suppresses all other events, and solves the light-to-glint correspondence. This is the first purely event-based corneal glint detection and tracking algorithm, which operates on standard hardware at kHz sampling rate.},
	urldate = {2024-06-05},
	booktitle = {2022 {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Stoffregen, Timo and Daraei, Hossein and Robinson, Clare and Fix, Alexander},
	month = jan,
	year = {2022},
	note = {ISSN: 2642-9381},
	keywords = {Cameras, Lighting, Gaze tracking, Real-time systems, Headphones, Low-level and Physics-based Vision Biometrics -{\textgreater} Human Motion Analysis/Capture, Real-time Tracking, Reflection, Sensor systems},
	pages = {3937--3945},
	file = {2022_Stoffregen_Event-Based Kilohertz Eye.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Event-Based\\2022_Stoffregen_Event-Based Kilohertz Eye.pdf:application/pdf},
}

@inproceedings{feng2022,
	title = {Real-{Time} {Gaze} {Tracking} with {Event}-{Driven} {Eye} {Segmentation}},
	url = {https://ieeexplore.ieee.org/document/9756796},
	doi = {10.1109/VR51125.2022.00059},
	abstract = {Gaze tracking is increasingly becoming an essential component in Augmented and Virtual Reality. Modern gaze tracking algorithms are heavyweight; they operate at most 5 Hz on mobile processors despite that near-eye cameras comfortably operate at a real-time rate ({\textgreater} 30 Hz). This paper presents a real-time eye tracking algorithm that, on average, operates at 30 Hz on a mobile processor, achieves 0.1°–0.5° gaze accuracies, all the while requiring only 30K parameters, one to two orders of magnitude smaller than state-of-the-art eye tracking algorithms. The crux of our algorithm is an Auto ROI mode, which continuously predicts the Regions of Interest (ROIs) of near-eye images and judiciously processes only the ROIs for gaze estimation. To that end, we introduce a novel, lightweight ROI prediction algorithm by emulating an event camera. We discuss how a software emulation of events enables accurate ROI prediction without requiring special hardware. The code of our paper is available at https://github.com/horizon-research/edgaze.},
	urldate = {2024-06-05},
	booktitle = {2022 {IEEE} {Conference} on {Virtual} {Reality} and {3D} {User} {Interfaces} ({VR})},
	author = {Feng, Yu and Goulding-Hotta, Nathan and Khan, Asif and Reyserhove, Hans and Zhu, Yuhao},
	month = mar,
	year = {2022},
	note = {ISSN: 2642-5254},
	keywords = {Virtual reality, Three-dimensional displays, eye tracking, event camera, Gaze tracking, Solid modeling, Gaze, Prediction algorithms, segmentation, Software algorithms, User interfaces},
	pages = {399--408},
	file = {2022_Feng_Real-Time Gaze Tracking with.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Event-Based\\2022_Feng_Real-Time Gaze Tracking with.pdf:application/pdf},
}

@article{souchet2022,
	title = {Measuring {Visual} {Fatigue} and {Cognitive} {Load} via {Eye} {Tracking} while {Learning} with {Virtual} {Reality} {Head}-{Mounted} {Displays}: {A} {Review}},
	volume = {38},
	issn = {1044-7318},
	shorttitle = {Measuring {Visual} {Fatigue} and {Cognitive} {Load} via {Eye} {Tracking} while {Learning} with {Virtual} {Reality} {Head}-{Mounted} {Displays}},
	url = {https://doi.org/10.1080/10447318.2021.1976509},
	doi = {10.1080/10447318.2021.1976509},
	abstract = {Virtual Reality Head-Mounted Displays (HMDs) reached the consumer market and are used for learning purposes. Risks regarding visual fatigue and high cognitive load arise while using HMDs. These risks could impact learning efficiency. Visual fatigue and cognitive load can be measured with eye tracking, a technique that is progressively implemented in HMDs. Thus, we investigate how to assess visual fatigue and cognitive load via eye tracking. We conducted this review based on five research questions. We first described visual fatigue and possible cognitive overload while learning with HMDs. The review indicates that visual fatigue can be measured with blinks and cognitive load with pupil diameter based on thirty-seven included papers. Yet, distinguishing visual fatigue from cognitive load with such measures is challenging due to possible links between them. Despite measure interpretation issues, eye tracking is promising for live assessment. More researches are needed to make data interpretation more robust and document human factor risks when learning with HMDs.},
	number = {9},
	urldate = {2024-06-05},
	journal = {International Journal of Human–Computer Interaction},
	author = {Souchet, Alexis D. and Philippe, Stéphanie and Lourdeaux, Domitile and Leroy, Laure},
	month = may,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447318.2021.1976509},
	pages = {801--824},
	file = {2022_Souchet_Measuring Visual Fatigue and.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2022_Souchet_Measuring Visual Fatigue and.pdf:application/pdf},
}

@article{dong2021,
	title = {Spontaneous {Facial} {Expressions} and {Micro}-expressions {Coding}: {From} {Brain} to {Face}},
	volume = {12},
	issn = {1664-1078},
	shorttitle = {Spontaneous {Facial} {Expressions} and {Micro}-expressions {Coding}},
	doi = {10.3389/fpsyg.2021.784834},
	abstract = {Facial expressions are a vital way for humans to show their perceived emotions. It is convenient for detecting and recognizing expressions or micro-expressions by annotating a lot of data in deep learning. However, the study of video-based expressions or micro-expressions requires that coders have professional knowledge and be familiar with action unit (AU) coding, leading to considerable difficulties. This paper aims to alleviate this situation. We deconstruct facial muscle movements from the motor cortex and systematically sort out the relationship among facial muscles, AU, and emotion to make more people understand coding from the basic principles: We derived the relationship between AU and emotion based on a data-driven analysis of 5,000 images from the RAF-AU database, along with the experience of professional coders.We discussed the complex facial motor cortical network system that generates facial movement properties, detailing the facial nucleus and the motor system associated with facial expressions.The supporting physiological theory for AU labeling of emotions is obtained by adding facial muscle movements patterns.We present the detailed process of emotion labeling and the detection and recognition of AU. Based on the above research, the video's coding of spontaneous expressions and micro-expressions is concluded and prospected.},
	language = {eng},
	journal = {Frontiers in Psychology},
	author = {Dong, Zizhao and Wang, Gang and Lu, Shaoyuan and Li, Jingting and Yan, Wenjing and Wang, Su-Jing},
	year = {2021},
	pmid = {35058850},
	pmcid = {PMC8763852},
	keywords = {action unit, cerebral cortex, coding, expressions, facial muscle, micro-expressions},
	pages = {784834},
	file = {2021_Dong_Spontaneous Facial.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Emotions\\2021_Dong_Spontaneous Facial.pdf:application/pdf},
}

@article{becattini2022,
	title = {Understanding {Human} {Reactions} {Looking} at {Facial} {Microexpressions} {With} an {Event} {Camera}},
	volume = {18},
	issn = {1941-0050},
	url = {https://ieeexplore.ieee.org/abstract/document/9844855},
	doi = {10.1109/TII.2022.3195063},
	abstract = {With the establishment of Industry 4.0, machines are now required to interact with workers. By observing biometrics they can assess if humans are authorized, or mentally and physically fit to work. Understanding body language, makes human–machine interaction more natural, secure, and effective. Nonetheless, traditional cameras have limitations; low frame rate and dynamic range hinder a comprehensive human understanding. This poses a challenge, since faces undergo frequent instantaneous microexpressions. In addition, this is privacy-sensitive information that must be protected. We propose to model expressions with event cameras, bio-inspired vision sensors that have found application within the Industry 4.0 scope. They capture motion at millisecond rates and work under challenging conditions like low illumination and highly dynamic scenes. Such cameras are also privacy-preserving, making them extremely interesting for industry. We show that using event cameras, we can understand human reactions by only observing facial expressions. Comparison with red-green-blue (RGB)-based modeling demonstrates improved effectiveness and robustness.},
	number = {12},
	urldate = {2024-06-05},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Becattini, Federico and Palai, Federico and Bimbo, Alberto Del},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Industrial Informatics},
	keywords = {Computer vision, Cameras, Biometrics, emotion recognition, event camera, Industry 4.0, microexpressions, neuromorphic sensor, privacy-preserving, Neuromorphics, Biometrics (access control), Faces, human–machine interfaces, Robot vision systems, Sensors},
	pages = {9112--9121},
	file = {2022_Becattini_Understanding Human Reactions.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Event-Based\\2022_Becattini_Understanding Human Reactions.pdf:application/pdf},
}

@article{gong2022,
	title = {Deep learning-based microexpression recognition: a survey},
	volume = {34},
	issn = {0941-0643, 1433-3058},
	shorttitle = {Deep learning-based microexpression recognition},
	url = {https://link.springer.com/10.1007/s00521-022-07157-w},
	doi = {10.1007/s00521-022-07157-w},
	abstract = {With the recent development of microexpression recognition, deep learning (DL) has been widely applied in this ﬁeld. In this paper, we provide a comprehensive survey of the current DL-based microexpression (ME) recognition methods. In addition, we introduce a novel dataset based on fusing all the existing ME datasets. We also evaluate a baseline DL for the microexpression recognition task. Finally, we make the new dataset and the code publicly available to the community at https://github.com/wenjgong/microExpressionSurvey.},
	language = {en},
	number = {12},
	urldate = {2024-06-05},
	journal = {Neural Computing and Applications},
	author = {Gong, Wenjuan and An, Zhihong and Elfiky, Noha M.},
	month = jun,
	year = {2022},
	pages = {9537--9560},
	file = {Gong et al. - 2022 - Deep learning-based microexpression recognition a.pdf:C\:\\Users\\antoine.widmer\\Zotero\\storage\\HPMXNK8C\\Gong et al. - 2022 - Deep learning-based microexpression recognition a.pdf:application/pdf},
}

@article{ben2022,
	title = {Video-{Based} {Facial} {Micro}-{Expression} {Analysis}: {A} {Survey} of {Datasets}, {Features} and {Algorithms}},
	volume = {44},
	issn = {1939-3539},
	shorttitle = {Video-{Based} {Facial} {Micro}-{Expression} {Analysis}},
	url = {https://ieeexplore.ieee.org/document/9382112},
	doi = {10.1109/TPAMI.2021.3067464},
	abstract = {Unlike the conventional facial expressions, micro-expressions are involuntary and transient facial expressions capable of revealing the genuine emotions that people attempt to hide. Therefore, they can provide important information in a broad range of applications such as lie detection, criminal detection, etc. Since micro-expressions are transient and of low intensity, however, their detection and recognition is difficult and relies heavily on expert experiences. Due to its intrinsic particularity and complexity, video-based micro-expression analysis is attractive but challenging, and has recently become an active area of research. Although there have been numerous developments in this area, thus far there has been no comprehensive survey that provides researchers with a systematic overview of these developments with a unified evaluation. Accordingly, in this survey paper, we first highlight the key differences between macro- and micro-expressions, then use these differences to guide our research survey of video-based micro-expression analysis in a cascaded structure, encompassing the neuropsychological basis, datasets, features, spotting algorithms, recognition algorithms, applications and evaluation of state-of-the-art approaches. For each aspect, the basic techniques, advanced developments and major challenges are addressed and discussed. Furthermore, after considering the limitations of existing micro-expression datasets, we present and release a new dataset — called micro-and-macro expression warehouse (MMEW) — containing more video samples and more labeled emotion types. We then perform a unified comparison of representative methods on CAS(ME){\textasciicircum}22 for spotting, and on MMEW and SAMM for recognition, respectively. Finally, some potential future research directions are explored and outlined.},
	number = {9},
	urldate = {2024-06-05},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ben, Xianye and Ren, Yi and Zhang, Junping and Wang, Su-Jing and Kpalma, Kidiyo and Meng, Weixiao and Liu, Yong-Jin},
	month = sep,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {survey, Face recognition, recognition, spotting, Emotion recognition, datasets, facial features, Facial muscles, Image sequences, Micro-expression analysis, Muscles, Neural pathways, Transient analysis},
	pages = {5826--5846},
	file = {2022_Ben_Video-Based Facial.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Emotions\\2022_Ben_Video-Based Facial.pdf:application/pdf},
}

@article{brepohl2023,
	title = {Virtual reality applied to physiotherapy: a review of current knowledge},
	volume = {27},
	issn = {1434-9957},
	shorttitle = {Virtual reality applied to physiotherapy},
	url = {https://doi.org/10.1007/s10055-022-00654-2},
	doi = {10.1007/s10055-022-00654-2},
	abstract = {Technological innovations have enabled physiotherapy to apply new possibilities in the rehabilitation of patients, especially in the use of virtual reality (VR). Although the literature provides several examples of VR applications, benefits, and barriers in physiotherapy, scholars obverse that there is still a dearth of studies that discuss and unify the results and impacts of this emerging technology on patients and physiotherapists. Thus, the aim of this study is to analyze the use of VR within physiotherapy and its impact on rehabilitation outcomes. A systematic literature review based on the PRISMA protocol was applied in this study. After searching on databases, such as Bireme, Cochrane, Emerald, Google Scholar, Lilacs, Medline, PEDro, PubMed, and Science Direct, we found 152 articles that complied with our protocol. The initial period of the search was open up to June 2020. Our results show an increased use of VR in neurology with elderly patients. We have identified underlying barriers (issues implementing VR, lack of protocols, and influence of patients) and benefits (effectiveness of treatment, motor development, and patient independence) of VR implementation. Finally, our study provides implications for VR in physiotherapy: a prominent increase in the use of VR in rehabilitation; value co-creation: interactions between patients and physiotherapists are crucial in the use of VR in physiotherapy; barriers related to technology, applicability, and the patient’s influence need to be overcome for VR practice to be used as a ‘business as usual’ modality in physiotherapy; the benefits of VR treatment can overcome the barriers faced by its use in rehabilitation.},
	language = {en},
	number = {1},
	urldate = {2024-06-05},
	journal = {Virtual Reality},
	author = {Brepohl, Polyana Cristina Alves and Leite, Higor},
	month = mar,
	year = {2023},
	keywords = {Virtual reality, Rehabilitation, Exergaming, Physiotherapy, Technology},
	pages = {71--95},
	file = {2023_Brepohl_Virtual reality applied to.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\XR\\2023_Brepohl_Virtual reality applied to.pdf:application/pdf},
}

@article{bailey2022,
	title = {Virtual {Reality} and {Augmented} {Reality} for {Children}, {Adolescents}, and {Adults} with {Communication} {Disability} and {Neurodevelopmental} {Disorders}: a {Systematic} {Review}},
	volume = {9},
	issn = {2195-7185},
	shorttitle = {Virtual {Reality} and {Augmented} {Reality} for {Children}, {Adolescents}, and {Adults} with {Communication} {Disability} and {Neurodevelopmental} {Disorders}},
	url = {https://doi.org/10.1007/s40489-020-00230-x},
	doi = {10.1007/s40489-020-00230-x},
	abstract = {This review investigated virtual reality and augmented reality (VR/AR) communication interventions for children, adolescents, and adults with communication disability and neurodevelopmental disorders, as well the feasibility of these technologies. A search of five scientific databases yielded 5385 potentially relevant records of which 69 met inclusion criteria. Studies reported on a wide range of VR/AR devices, platforms, and applications for people with autism spectrum disorder, communication disorders, and intellectual disability. Some VR/AR systems hosted effective communication interventions; however, participant outcomes varied across the included studies. Most participants with neurodevelopmental disorders and their supporters were able to access learning experiences using VR/AR and few adverse effects were reported. Directions for future research are discussed.},
	language = {en},
	number = {2},
	urldate = {2024-06-05},
	journal = {Review Journal of Autism and Developmental Disorders},
	author = {Bailey, Benjamin and Bryant, Lucy and Hemsley, Bronwyn},
	month = jun,
	year = {2022},
	keywords = {Augmented reality, Virtual reality, Communication disability, Neurodevelopmental disorders},
	pages = {160--183},
	file = {2022_Bailey_Virtual Reality and Augmented.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2022_Bailey_Virtual Reality and Augmented.pdf:application/pdf},
}

@article{barua2022,
	title = {Artificial {Intelligence} {Enabled} {Personalised} {Assistive} {Tools} to {Enhance} {Education} of {Children} with {Neurodevelopmental} {Disorders}—{A} {Review}},
	volume = {19},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1660-4601},
	url = {https://www.mdpi.com/1660-4601/19/3/1192},
	doi = {10.3390/ijerph19031192},
	abstract = {Mental disorders (MDs) with onset in childhood or adolescence include neurodevelopmental disorders (NDDs) (intellectual disability and specific learning disabilities, such as dyslexia, attention deficit disorder (ADHD), and autism spectrum disorders (ASD)), as well as a broad range of mental health disorders (MHDs), including anxiety, depressive, stress-related and psychotic disorders. There is a high co-morbidity of NDDs and MHDs. Globally, there have been dramatic increases in the diagnosis of childhood-onset mental disorders, with a 2- to 3-fold rise in prevalence for several MHDs in the US over the past 20 years. Depending on the type of MD, children often grapple with social and communication deficits and difficulties adapting to changes in their environment, which can impact their ability to learn effectively. To improve outcomes for children, it is important to provide timely and effective interventions. This review summarises the range and effectiveness of AI-assisted tools, developed using machine learning models, which have been applied to address learning challenges in students with a range of NDDs. Our review summarises the evidence that AI tools can be successfully used to improve social interaction and supportive education. Based on the limitations of existing AI tools, we provide recommendations for the development of future AI tools with a focus on providing personalised learning for individuals with NDDs.},
	language = {en},
	number = {3},
	urldate = {2024-06-05},
	journal = {International Journal of Environmental Research and Public Health},
	author = {Barua, Prabal Datta and Vicnesh, Jahmunah and Gururajan, Raj and Oh, Shu Lih and Palmer, Elizabeth and Azizan, Muhammad Mokhzaini and Kadri, Nahrizul Adib and Acharya, U. Rajendra},
	month = jan,
	year = {2022},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial intelligence, machine learning, mental disorders, neurodevelopmental disorders, personalisation},
	pages = {1192},
	file = {2022_Barua_Artificial Intelligence.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2022_Barua_Artificial Intelligence.pdf:application/pdf},
}

@article{stokes2022,
	title = {Measuring {Attentional} {Distraction} in {Children} {With} {ADHD} {Using} {Virtual} {Reality} {Technology} {With} {Eye}-{Tracking}},
	volume = {3},
	issn = {2673-4192},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9119405/},
	doi = {10.3389/frvir.2022.855895},
	abstract = {Objective:
Distractions inordinately impair attention in children with Attention-Deficit Hyperactivity Disorder (ADHD) but examining this behavior under real-life conditions poses a challenge for researchers and clinicians. Virtual reality (VR) technologies may mitigate the limitations of traditional laboratory methods by providing a more ecologically relevant experience. The use of eye-tracking measures to assess attentional functioning in a VR context in ADHD is novel. In this proof of principle project, we evaluate the temporal dynamics of distraction via eye-tracking measures in a VR classroom setting with 20 children diagnosed with ADHD between 8 and 12 years of age.

Method:
We recorded continuous eye movements while participants performed math, Stroop, and continuous performance test (CPT) tasks with a series of “real-world” classroom distractors presented. We analyzed the impact of the distractors on rates of on-task performance and on-task, eye-gaze (i.e., looking at a classroom whiteboard) versus off-task eye-gaze (i.e., looking away from the whiteboard).

Results:
We found that while children did not always look at distractors themselves for long periods of time, the presence of a distractor disrupted on-task gaze at task-relevant whiteboard stimuli and lowered rates of task performance. This suggests that children with attention deficits may have a hard time returning to tasks once those tasks are interrupted, even if the distractor itself does not hold attention. Eye-tracking measures within the VR context can reveal rich information about attentional disruption.

Conclusions:
Leveraging virtual reality technology in combination with eye-tracking measures is well-suited to advance the understanding of mechanisms underlying attentional impairment in naturalistic settings. Assessment within these immersive and well-controlled simulated environments provides new options for increasing our understanding of distractibility and its potential impact on the development of interventions for children with ADHD.},
	urldate = {2024-06-05},
	journal = {Frontiers in virtual reality},
	author = {Stokes, Jared D. and Rizzo, Albert and Geng, Joy J. and Schweitzer, Julie B.},
	month = mar,
	year = {2022},
	pmid = {35601272},
	pmcid = {PMC9119405},
	pages = {855895},
	file = {2022_Stokes_Measuring Attentional.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2022_Stokes_Measuring Attentional.pdf:application/pdf},
}

@article{kapp2021,
	title = {{ARETT}: {Augmented} {Reality} {Eye} {Tracking} {Toolkit} for {Head} {Mounted} {Displays}},
	volume = {21},
	issn = {1424-8220},
	shorttitle = {{ARETT}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8004990/},
	doi = {10.3390/s21062234},
	abstract = {Currently an increasing number of head mounted displays (HMD) for virtual and augmented reality (VR/AR) are equipped with integrated eye trackers. Use cases of these integrated eye trackers include rendering optimization and gaze-based user interaction. In addition, visual attention in VR and AR is interesting for applied research based on eye tracking in cognitive or educational sciences for example. While some research toolkits for VR already exist, only a few target AR scenarios. In this work, we present an open-source eye tracking toolkit for reliable gaze data acquisition in AR based on Unity 3D and the Microsoft HoloLens 2, as well as an R package for seamless data analysis. Furthermore, we evaluate the spatial accuracy and precision of the integrated eye tracker for fixation targets with different distances and angles to the user (n=21). On average, we found that gaze estimates are reported with an angular accuracy of 0.83 degrees and a precision of 0.27 degrees while the user is resting, which is on par with state-of-the-art mobile eye trackers.},
	number = {6},
	urldate = {2024-06-05},
	journal = {Sensors (Basel, Switzerland)},
	author = {Kapp, Sebastian and Barz, Michael and Mukhametov, Sergey and Sonntag, Daniel and Kuhn, Jochen},
	month = mar,
	year = {2021},
	pmid = {33806863},
	pmcid = {PMC8004990},
	pages = {2234},
	file = {2021_Kapp_ARETT.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2021_Kapp_ARETT.pdf:application/pdf},
}

@article{angelopoulos2021,
	title = {Event-{Based} {Near}-{Eye} {Gaze} {Tracking} {Beyond} 10,000 {Hz}},
	volume = {27},
	issn = {1941-0506},
	url = {https://ieeexplore.ieee.org/document/9389490},
	doi = {10.1109/TVCG.2021.3067784},
	abstract = {The cameras in modern gaze-tracking systems suffer from fundamental bandwidth and power limitations, constraining data acquisition speed to 300 Hz realistically. This obstructs the use of mobile eye trackers to perform, e.g., low latency predictive rendering, or to study quick and subtle eye motions like microsaccades using head-mounted devices in the wild. Here, we propose a hybrid frame-event-based near-eye gaze tracking system offering update rates beyond 10,000 Hz with an accuracy that matches that of high-end desktop-mounted commercial trackers when evaluated in the same conditions. Our system, previewed in Figure 1, builds on emerging event cameras that simultaneously acquire regularly sampled frames and adaptively sampled events. We develop an online 2D pupil fitting method that updates a parametric model every one or few events. Moreover, we propose a polynomial regressor for estimating the point of gaze from the parametric pupil model in real time. Using the first event-based gaze dataset, we demonstrate that our system achieves accuracies of 0.45°-1.75° for fields of view from 45° to 98°. With this technology, we hope to enable a new generation of ultra-low-latency gaze-contingent rendering and display techniques for virtual and augmented reality.},
	number = {5},
	urldate = {2024-06-05},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Angelopoulos, Anastasios N. and Martel, Julien N.P. and Kohli, Amit P. and Conradt, Jörg and Wetzstein, Gordon},
	month = may,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Cameras, Tracking, Eye tracking, Gaze tracking, Pupils, Real-time systems, Augmented and virtual reality, Bandwidth, Event-based camera, Rendering (computer graphics)},
	pages = {2577--2586},
	file = {2021_Angelopoulos_Event-Based Near-Eye Gaze.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Event-Based\\2021_Angelopoulos_Event-Based Near-Eye Gaze.pdf:application/pdf},
}

@article{dechsling2021,
	title = {Virtual reality and naturalistic developmental behavioral interventions for children with autism spectrum disorder},
	volume = {111},
	issn = {0891-4222},
	url = {https://www.sciencedirect.com/science/article/pii/S0891422221000342},
	doi = {10.1016/j.ridd.2021.103885},
	abstract = {Background
Naturalistic Developmental Behavioral Interventions (NDBI) have been evaluated as the most promising interventions for children with autism spectrum disorder. In recent years, a growing body of literature suggests that technological advancements such as Virtual Reality (VR) are promising intervention tools. However, to the best of our knowledge no studies have combined evidence-based practice with such tools.
Aim
This article aims to review the current literature combining NDBI and VR, and provide suggestions on merging NDBI-approaches with VR.
Methods
This article is divided into two parts, where we first conduct a review mapping the research applying NDBI-approaches in VR. In the second part we argue how to apply the common features of NDBI into VR-technology.
Results
Our findings show that no VR-studies explicitly rely on NDBI-approaches, but some utilize elements in their interventions that are considered to be common features to NDBI.
Conclusions and implications
As the results show, to date, no VR-based studies have utilized NDBI in their intervention. We therefore, in the second part of this article, suggests ways to merge VR and NDBI and introduce the term Virtual Naturalistic Developmental Behavioral Interventions (VNDBI). VNDBI is an innovative way of implementing NDBI which will contribute in making interventions more accessible in central as well as remote locations, while reducing unwanted variation between service sites. VNDBI will advance the possibilities of individually tailoring and widen the area of interventions. In addition, VNDBI can provide the field with new knowledge on effective components enhancing the accuracy in the intervention packages and thus move forward the research field and clinical practice.},
	urldate = {2024-06-05},
	journal = {Research in Developmental Disabilities},
	author = {Dechsling, Anders and Shic, Frederick and Zhang, Dajie and Marschik, Peter B. and Esposito, Gianluca and Orm, Stian and Sütterlin, Stefan and Kalandadze, Tamara and Øien, Roald A. and Nordahl-Hansen, Anders},
	month = apr,
	year = {2021},
	keywords = {Augmented reality, Virtual reality, Autism spectrum disorder, Intervention, Naturalistic developmental behavioral interventions},
	pages = {103885},
	file = {2021_Dechsling_Virtual reality and.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2021_Dechsling_Virtual reality and.pdf:application/pdf},
}

@article{zammarchi2021,
	title = {Application of {Eye} {Tracking} {Technology} in {Medicine}: {A} {Bibliometric} {Analysis}},
	volume = {5},
	issn = {2411-5150},
	shorttitle = {Application of {Eye} {Tracking} {Technology} in {Medicine}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8628933/},
	doi = {10.3390/vision5040056},
	abstract = {Eye tracking provides a quantitative measure of eye movements during different activities. We report the results from a bibliometric analysis to investigate trends in eye tracking research applied to the study of different medical conditions. We conducted a search on the Web of Science Core Collection (WoS) database and analyzed the dataset of 2456 retrieved articles using VOSviewer and the Bibliometrix R package. The most represented area was psychiatry (503, 20.5\%) followed by neuroscience (465, 18.9\%) and psychology developmental (337, 13.7\%). The annual scientific production growth was 11.14\% and showed exponential growth with three main peaks in 2011, 2015 and 2017. Extensive collaboration networks were identified between the three countries with the highest scientific production, the USA (35.3\%), the UK (9.5\%) and Germany (7.3\%). Based on term co-occurrence maps and analyses of sources of articles, we identified autism spectrum disorders as the most investigated condition and conducted specific analyses on 638 articles related to this topic which showed an annual scientific production growth of 16.52\%. The majority of studies focused on autism used eye tracking to investigate gaze patterns with regards to stimuli related to social interaction. Our analysis highlights the widespread and increasing use of eye tracking in the study of different neurological and psychiatric conditions.},
	number = {4},
	urldate = {2024-06-05},
	journal = {Vision},
	author = {Zammarchi, Gianpaolo and Conversano, Claudio},
	month = nov,
	year = {2021},
	pmid = {34842855},
	pmcid = {PMC8628933},
	pages = {56},
	file = {2021_Zammarchi_Application of Eye Tracking.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2021_Zammarchi_Application of Eye Tracking.pdf:application/pdf},
}

@article{sipatchin2021,
	title = {Eye-{Tracking} for {Clinical} {Ophthalmology} with {Virtual} {Reality} ({VR}): {A} {Case} {Study} of the {HTC} {Vive} {Pro} {Eye}’s {Usability}},
	volume = {9},
	issn = {2227-9032},
	shorttitle = {Eye-{Tracking} for {Clinical} {Ophthalmology} with {Virtual} {Reality} ({VR})},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7914806/},
	doi = {10.3390/healthcare9020180},
	abstract = {Background: A case study is proposed to empirically test and discuss the eye-tracking status-quo hardware capabilities and limitations of an off-the-shelf virtual reality (VR) headset with embedded eye-tracking for at-home ready-to-go online usability in ophthalmology applications. Methods: The eye-tracking status-quo data quality of the HTC Vive Pro Eye is investigated with novel testing specific to objective online VR perimetry. Testing was done across a wide visual field of the head-mounted-display’s (HMD) screen and in two different moving conditions. A new automatic and low-cost Raspberry Pi system is introduced for VR temporal precision testing for assessing the usability of the HTC Vive Pro Eye as an online assistance tool for visual loss. Results: The target position on the screen and head movement evidenced limitations of the eye-tracker capabilities as a perimetry assessment tool. Temporal precision testing showed the system’s latency of 58.1 milliseconds (ms), evidencing its good potential usage as a ready-to-go online assistance tool for visual loss. Conclusions: The test of the eye-tracking data quality provides novel analysis useful for testing upcoming VR headsets with embedded eye-tracking and opens discussion regarding expanding future introduction of these HMDs into patients’ homes for low-vision clinical usability.},
	number = {2},
	urldate = {2024-06-05},
	journal = {Healthcare},
	author = {Sipatchin, Alexandra and Wahl, Siegfried and Rifai, Katharina},
	month = feb,
	year = {2021},
	pmid = {33572072},
	pmcid = {PMC7914806},
	pages = {180},
	file = {2021_Sipatchin_Eye-Tracking for Clinical.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2021_Sipatchin_Eye-Tracking for Clinical.pdf:application/pdf},
}

@article{ryan2021,
	title = {Real-time face \& eye tracking and blink detection using event cameras},
	volume = {141},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608021001076},
	doi = {10.1016/j.neunet.2021.03.019},
	abstract = {Event cameras contain emerging, neuromorphic vision sensors that capture local-light​ intensity changes at each pixel, generating a stream of asynchronous events. This way of acquiring visual information constitutes a departure from traditional frame-based cameras and offers several significant advantages — low energy consumption, high temporal resolution, high dynamic range and low latency. Driver monitoring systems (DMS) are in-cabin safety systems designed to sense and understand a drivers physical and cognitive state. Event cameras are particularly suited to DMS due to their inherent advantages. This paper proposes a novel method to simultaneously detect and track faces and eyes for driver monitoring. A unique, fully convolutional recurrent neural network architecture is presented. To train this network, a synthetic event-based dataset is simulated with accurate bounding box annotations, called Neuromorphic-HELEN. Additionally, a method to detect and analyse drivers’ eye blinks is proposed, exploiting the high temporal resolution of event cameras. Behaviour of blinking provides greater insights into a driver level of fatigue or drowsiness. We show that blinks have a unique temporal signature that can be better captured by event cameras.},
	urldate = {2024-06-05},
	journal = {Neural Networks},
	author = {Ryan, Cian and O’Sullivan, Brian and Elrasad, Amr and Cahill, Aisling and Lemley, Joe and Kielty, Paul and Posch, Christoph and Perot, Etienne},
	month = sep,
	year = {2021},
	keywords = {Convolutional neural network, Driver monitoring system, Event cameras},
	pages = {87--97},
	file = {2021_Ryan.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Event-Based\\2021_Ryan.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\KU7QSNDG\\S0893608021001076.html:text/html},
}

@article{francis2021,
	title = {Prevention in {Autism} {Spectrum} {Disorder}: {A} {Lifelong} {Focused} {Approach}},
	volume = {11},
	issn = {2076-3425},
	shorttitle = {Prevention in {Autism} {Spectrum} {Disorder}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7911370/},
	doi = {10.3390/brainsci11020151},
	abstract = {Autism Spectrum Disorder (ASD) is a complex highly heritable disorder, in which multiple environmental factors interact with the genes to increase its risk and lead to variable clinical presentations and outcomes. Furthermore, the inherent fundamental deficits of ASD in social attention and interaction critically diverge children from the typical pathways of learning, “creating” what we perceive as autism syndrome during the first three years of life. Later in life, training and education, the presence and management of comorbidities, as well as social and vocational support throughout the lifespan, will define the quality of life and the adaptation of an individual with ASD. Given the overall burden of ASD, prevention strategies seem like a cost-effective endeavour that we have to explore. In this paper, we take a life course approach to prevention. We will review the possibilities of the management of risk factors from preconception until the perinatal period, that of early intervention in the first three years of life and that of effective training and support from childhood until adulthood.},
	number = {2},
	urldate = {2024-06-05},
	journal = {Brain Sciences},
	author = {Francis, Konstantinos and Karantanos, Georgios and Al-Ozairi, Abdullah and AlKhadhari, Sulaiman},
	month = jan,
	year = {2021},
	pmid = {33498888},
	pmcid = {PMC7911370},
	pages = {151},
	file = {2021_Francis_Prevention in Autism Spectrum.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2021_Francis_Prevention in Autism Spectrum.pdf:application/pdf},
}

@article{stasolla2021,
	title = {Virtual {Reality} and {Wearable} {Technologies} to {Support} {Adaptive} {Responding} of {Children} and {Adolescents} {With} {Neurodevelopmental} {Disorders}: {A} {Critical} {Comment} and {New} {Perspectives}},
	volume = {12},
	issn = {1664-1078},
	shorttitle = {Virtual {Reality} and {Wearable} {Technologies} to {Support} {Adaptive} {Responding} of {Children} and {Adolescents} {With} {Neurodevelopmental} {Disorders}},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2021.720626/full},
	doi = {10.3389/fpsyg.2021.720626},
	language = {English},
	urldate = {2024-06-05},
	journal = {Frontiers in Psychology},
	author = {Stasolla, Fabrizio},
	month = jul,
	year = {2021},
	note = {Publisher: Frontiers},
	keywords = {virtual reality, Rehabilitation, assessment, Assistive Technology, Neurodevelomental Disorders, Quality of Life, Wearable technologies},
	file = {2021_Stasolla_Virtual Reality and Wearable.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2021_Stasolla_Virtual Reality and Wearable.pdf:application/pdf},
}

@article{geraets2021,
	series = {Psychopathology},
	title = {Advances in immersive virtual reality interventions for mental disorders: {A} new reality?},
	volume = {41},
	issn = {2352-250X},
	shorttitle = {Advances in immersive virtual reality interventions for mental disorders},
	url = {https://www.sciencedirect.com/science/article/pii/S2352250X21000142},
	doi = {10.1016/j.copsyc.2021.02.004},
	abstract = {Immersive virtual reality (VR) has been identified as a potentially revolutionary tool for psychological interventions. This study reviews current advances in immersive VR-based therapies for mental disorders. VR has the potential to make psychiatric treatments better and more cost-effective and to make them available to a larger group of patients. However, this may require a new generation of VR therapeutic techniques that use the full potential of VR, such as embodiment, and self-led interventions. VR-based interventions are promising, but further well-designed studies are needed that use novel techniques and investigate efficacy, efficiency, and cost-effectiveness of VR interventions compared with current treatments. This will be crucial for implementation and dissemination of VR in regular clinical practice.},
	urldate = {2024-06-05},
	journal = {Current Opinion in Psychology},
	author = {Geraets, Chris N. W. and van der Stouwe, Elisabeth C. D. and Pot-Kolder, Roos and Veling, Wim},
	month = oct,
	year = {2021},
	keywords = {Virtual reality, Intervention, Automated, Embodiment, Perspective change, Psychiatry, Treatment},
	pages = {40--45},
	file = {2021_Geraets_Advances in immersive virtual.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\XR\\2021_Geraets_Advances in immersive virtual.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\TPXB64KF\\S2352250X21000142.html:text/html},
}

@article{romero-ayuso2021,
	title = {Effectiveness of {Virtual} {Reality}-{Based} {Interventions} for {Children} and {Adolescents} with {ADHD}: {A} {Systematic} {Review} and {Meta}-{Analysis}},
	volume = {8},
	issn = {2227-9067},
	shorttitle = {Effectiveness of {Virtual} {Reality}-{Based} {Interventions} for {Children} and {Adolescents} with {ADHD}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7909839/},
	doi = {10.3390/children8020070},
	abstract = {This review aims to evaluate the effectiveness of virtual reality-based interventions (VR-based interventions) on cognitive deficits in children with attention deficit hyperactivity disorder (ADHD). A systematic review and meta-analysis were performed according to the PRISMA statement and the Cochrane Handbook guidelines for conducting meta-analyses. The Grading of Recommendations, Assessment, Development and Evaluation (GRADE) was used to assess the quality of the evidence. Clinical trials published up to 29 October 2020, were included. The meta-analysis included four studies, with a population of 125 participants with ADHD. The magnitude of the effect was large for omissions (SMD = −1.38; p = 0.009), correct hits (SMD = −1.50; p = 0.004), and perceptual sensitivity (SMD = −1.07; p = 0.01); and moderate for commissions (SMD = −0.62; p = 0.002) and reaction time (SMD = −0.67; p = 0.03). The use of VR-based interventions for cognitive rehabilitation in children with ADHD is limited. The results showed that VR-based interventions are more effective in improving sustained attention. Improvements were observed in attentional vigilance measures, increasing the number of correct responses and decreasing the number of errors of omission. No improvements were observed in impulsivity responses.},
	number = {2},
	urldate = {2024-06-05},
	journal = {Children},
	author = {Romero-Ayuso, Dulce and Toledano-González, Abel and Rodríguez-Martínez, María del Carmen and Arroyo-Castillo, Palma and Triviño-Juárez, José Matías and González, Pascual and Ariza-Vega, Patrocinio and Del Pino González, Antonio and Segura-Fragoso, Antonio},
	month = jan,
	year = {2021},
	pmid = {33494272},
	pmcid = {PMC7909839},
	pages = {70},
	file = {2021_Romero-Ayuso_Effectiveness of Virtual.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2021_Romero-Ayuso_Effectiveness of Virtual.pdf:application/pdf},
}

@inproceedings{bauer2021,
	title = {Designing an {Extended} {Reality} {Application} to {Expand} {Clinic}-{Based} {Sensory} {Strategies} for {Autistic} {Children} {Requiring} {Substantial} {Support}: {Participation} of {Practitioners}},
	shorttitle = {Designing an {Extended} {Reality} {Application} to {Expand} {Clinic}-{Based} {Sensory} {Strategies} for {Autistic} {Children} {Requiring} {Substantial} {Support}},
	url = {https://ieeexplore.ieee.org/document/9585792},
	doi = {10.1109/ISMAR-Adjunct54149.2021.00059},
	abstract = {Extended Reality (XR) has already been used to support interventions for autistic children, but mainly focuses on training the socioemotional abilities of children requiring low support. To also consider children requiring substantial support, this paper examines how to design XR applications in order to expand clinic-based sensory strategies that are often used by practitioners to put them in a secure state, and how to maximize the acceptability of such applications among practitioners. To that respect, a "Mixed Reality platform for Engagement and Relaxation of Autistic children" was designed and developed, which allows to add audio, visual and haptic individualized or common stimuli onto reality. A first Augmented Reality freeplay use case called Magic Bubbles was created based on interviews with stakeholders and on a collaboration with three practitioners. A preliminary study with eleven practitioners confirmed its well-being potential and acceptability. XR design guidelines are finally derived.},
	urldate = {2024-06-05},
	booktitle = {2021 {IEEE} {International} {Symposium} on {Mixed} and {Augmented} {Reality} {Adjunct} ({ISMAR}-{Adjunct})},
	author = {Bauer, Valentin and Bouchara, Tifanie and Bourdot, Patrick},
	month = oct,
	year = {2021},
	keywords = {Mixed reality, Autism Spectrum Disorder, augmented reality, Collaboration, design, Design methodology, Extended reality, Haptic interfaces, mediation, multi-sensorimotor, Training, Visualization, well-being},
	pages = {254--259},
	file = {2021_Bauer_Designing an Extended Reality.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2021_Bauer_Designing an Extended Reality.pdf:application/pdf},
}

@article{karami2021,
	title = {Effectiveness of {Virtual}/{Augmented} {Reality}–{Based} {Therapeutic} {Interventions} on {Individuals} {With} {Autism} {Spectrum} {Disorder}: {A} {Comprehensive} {Meta}-{Analysis}},
	volume = {12},
	issn = {1664-0640},
	shorttitle = {Effectiveness of {Virtual}/{Augmented} {Reality}–{Based} {Therapeutic} {Interventions} on {Individuals} {With} {Autism} {Spectrum} {Disorder}},
	url = {https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2021.665326/full},
	doi = {10.3389/fpsyt.2021.665326},
	abstract = {{\textless}p{\textgreater}In recent years, the application of virtual reality (VR) for therapeutic purposes has escalated dramatically. Favorable properties of VR for engaging patients with autism, in particular, have motivated an enormous body of investigations targeting autism-related disabilities with this technology. This study aims to provide a comprehensive meta-analysis for evaluating the effectiveness of VR on the rehabilitation and training of individuals diagnosed with an autism spectrum disorder. Accordingly, we conducted a systematic search of related databases and, after screening for inclusion criteria, reviewed 33 studies for more detailed analysis. Results revealed that individuals undergoing VR training have remarkable improvements with a relatively large effect size with Hedges {\textless}italic{\textgreater}g{\textless}/italic{\textgreater} of 0.74. Furthermore, the results of the analysis of different skills indicated diverse effectiveness. The strongest effect was observed for daily living skills ({\textless}italic{\textgreater}g{\textless}/italic{\textgreater} = 1.15). This effect was moderate for other skills: {\textless}italic{\textgreater}g{\textless}/italic{\textgreater} = 0.45 for cognitive skills, {\textless}italic{\textgreater}g{\textless}/italic{\textgreater} = 0.46 for emotion regulation and recognition skills, and {\textless}italic{\textgreater}g{\textless}/italic{\textgreater} = 0.69 for social and communication skills. Moreover, five studies that had used augmented reality also showed promising efficacy ({\textless}italic{\textgreater}g{\textless}/italic{\textgreater} = 0.92) that calls for more research on this tool. In conclusion, the application of VR-based settings in clinical practice is highly encouraged, although their standardization and customization need more research.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-06-05},
	journal = {Frontiers in Psychiatry},
	author = {Karami, Behnam and Koushki, Roxana and Arabgol, Fariba and Rahmani, Maryam and Vahabie, Abdol-Hossein},
	month = jun,
	year = {2021},
	note = {Publisher: Frontiers},
	keywords = {virtual reality, Technology, Augmented Realitiy, autism spactrum disorder, Rehabili tation},
	file = {2021_Karami_Effectiveness of.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2021_Karami_Effectiveness of.pdf:application/pdf},
}

@inproceedings{bouzbib2022,
	address = {New York, NY, USA},
	series = {{IHM} '21},
	title = {“{Can} {I} {Touch} {This}?”: {Survey} of {Virtual} {Reality} {Interactions} via {Haptic} {Solutions}: {Revue} de {Littérature} des {Interactions} en {Réalité} {Virtuelle} par le biais de {Solutions} {Haptiques}},
	isbn = {978-1-4503-8362-2},
	shorttitle = {“{Can} {I} {Touch} {This}?},
	url = {https://dl.acm.org/doi/10.1145/3450522.3451323},
	doi = {10.1145/3450522.3451323},
	abstract = {Haptic feedback has become crucial to enhance the user experiences in Virtual Reality (VR). This justifies the sudden burst of novel haptic solutions proposed these past years in the HCI community. This article is a survey of Virtual Reality interactions, relying on haptic devices. We propose two dimensions to describe and compare the current haptic solutions: their degree of physicality, as well as their degree of actuation. We depict a compromise between the user and the designer, highlighting how the range of required or proposed stimulation in VR is opposed to the haptic interfaces flexibility and their deployment in real-life use-cases. This paper (1) outlines the variety of haptic solutions and provides a novel perspective for analysing their associated interactions, (2) highlights the limits of the current evaluation criteria regarding these interactions, and finally (3) reflects the interaction, operation and conception potentials of ”encountered-type of haptic devices”.},
	urldate = {2024-06-05},
	booktitle = {Proceedings of the 32nd {Conference} on l'{Interaction} {Homme}-{Machine}},
	publisher = {Association for Computing Machinery},
	author = {Bouzbib, Elodie and Bailly, Gilles and Haliyo, Sinan and Frey, Pascal},
	month = jan,
	year = {2022},
	keywords = {haptic devices, haptics, human factors, Virtual Reality},
	pages = {1--16},
	file = {2022_Bouzbib_“Can I Touch This.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\XR\\2022_Bouzbib_“Can I Touch This.pdf:application/pdf},
}

@inproceedings{nordahl-hansen2020,
	address = {Cham},
	title = {An {Overview} of {Virtual} {Reality} {Interventions} for {Two} {Neurodevelopmental} {Disorders}: {Intellectual} {Disabilities} and {Autism}},
	isbn = {978-3-030-50439-7},
	shorttitle = {An {Overview} of {Virtual} {Reality} {Interventions} for {Two} {Neurodevelopmental} {Disorders}},
	doi = {10.1007/978-3-030-50439-7_17},
	abstract = {In this overview of the two neurodevelopmental disorders, intellectual disabilities and autism spectrum disorders we systematically searched the literature for scientific publications of group-based designs that tested various interventions through the use of Virtual Reality technology. After screening of a total of n = 366 publications, n = 13 studies (intellectual disabilities n = 7, autism spectrum disorders n = 6) were included in the final analyses. We present descriptive data in terms of type of intervention content for the various studies as well as information regarding research design, number of participants enrolled in the studies, age cohorts, and outcome measures. We discuss the findings as a whole but also by comparing the studies that are published within each of the two neurodevelopmental disorder groups. Finally we discuss some challenges and opportunities for future research.},
	language = {en},
	booktitle = {Augmented {Cognition}. {Human} {Cognition} and {Behavior}},
	publisher = {Springer International Publishing},
	author = {Nordahl-Hansen, Anders and Dechsling, Anders and Sütterlin, Stefan and Børtveit, Line and Zhang, Dajie and Øien, Roald A. and Marschik, Peter B.},
	editor = {Schmorrow, Dylan D. and Fidopiastis, Cali M.},
	year = {2020},
	pages = {257--267},
}

@article{dellazizzo2020,
	title = {Evidence on {Virtual} {Reality}–{Based} {Therapies} for {Psychiatric} {Disorders}: {Meta}-{Review} of {Meta}-{Analyses}},
	volume = {22},
	issn = {1439-4456},
	shorttitle = {Evidence on {Virtual} {Reality}–{Based} {Therapies} for {Psychiatric} {Disorders}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7468638/},
	doi = {10.2196/20889},
	abstract = {Background
Among all diseases globally, mental illnesses are one of the major causes of burden. As many people are resistant to conventional evidence-based treatments, there is an unmet need for the implementation of novel mental health treatments. Efforts to increase the effectiveness and benefits of evidence-based psychotherapy in psychiatry have led to the emergence of virtual reality (VR)–based interventions. These interventions have shown a wide range of advantages over conventional psychotherapies. Currently, VR-based interventions have been developed mainly for anxiety-related disorders; however, they are also used for developmental disorders, severe mental disorders, and neurocognitive disorders.

Objective
This meta-review aims to summarize the current state of evidence on the efficacy of VR-based interventions for various psychiatric disorders by evaluating the quality of evidence provided by meta-analytical studies.

Methods
A systematic search was performed using the following electronic databases: PubMed, PsycINFO, Web of Science, and Google Scholar (any time until February 2020). Meta-analyses were included as long as they quantitatively examined the efficacy of VR-based interventions for symptoms of a psychiatric disorder. To avoid overlap among meta-analyses, for each subanalysis included within this meta-review, only one analysis provided from one meta-analysis was selected based on the best quality of evidence.

Results
The search retrieved 11 eligible meta-analyses. The quality of evidence varied from very low to moderate quality. Several reasons account for the lower quality evidence, such as a limited number of randomized controlled trials, lack of follow-up analysis or control group, and the presence of heterogeneity and publication bias. Nonetheless, evidence has shown that VR-based interventions for anxiety-related disorders display overall medium-to-large effects when compared with inactive controls but no significant difference when compared with standard evidence-based approaches. Preliminary data have highlighted that such effects appear to be sustained in time, and subjects may fare better than active controls. Neurocognitive disorders also appear to improve with VR-based approaches, with small effects being found for various clinical outcomes (eg, cognition, emotion). Finally, there are insufficient data to classify VR-based interventions as an evidence-based practice for social skills training in neurodevelopmental disorders and compliance among patients with schizophrenia.

Conclusions
VR provides unlimited opportunities by tailoring approaches to specific complex problems and individualizing the intervention. However, VR-based interventions have not shown superiority compared with usual evidence-based treatments. Future VR-based interventions should focus on developing innovative approaches for complex and treatment-resistant symptoms that are difficult to address with traditional treatments. Future research should also aim to gain a better understanding of the potential factors that may mediate VR outcomes to improve treatment.},
	number = {8},
	urldate = {2024-06-05},
	journal = {Journal of Medical Internet Research},
	author = {Dellazizzo, Laura and Potvin, Stéphane and Luigi, Mimosa and Dumais, Alexandre},
	month = aug,
	year = {2020},
	pmid = {32812889},
	pmcid = {PMC7468638},
	pages = {e20889},
	file = {2020_Dellazizzo_Evidence on Virtual.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\XR\\2020_Dellazizzo_Evidence on Virtual.pdf:application/pdf},
}

@article{newbutt2020,
	title = {Using {Virtual} {Reality} {Head}-{Mounted} {Displays} in {Schools} with {Autistic} {Children}: {Views}, {Experiences}, and {Future} {Directions}},
	volume = {23},
	issn = {2152-2723},
	shorttitle = {Using {Virtual} {Reality} {Head}-{Mounted} {Displays} in {Schools} with {Autistic} {Children}},
	doi = {10.1089/cyber.2019.0206},
	abstract = {This article seeks to place children on the autism spectrum at the center of a study examining the potential of virtual reality (VR) head-mounted displays (HMDs) used in classrooms. In doing so, we provide data that address 3 important and often overlooked research questions in the field of autism and technology, working in school-based settings with 31 autistic children from 6 to 16 years of age. First, what type of VR HMD device (and experiences therein) are preferred by children on the autism spectrum using HMDs (given possible sensory concerns). Second, how do children on the autism spectrum report the physical experience, enjoyment, and potential of VR HMDs in their classrooms? Finally, we were interested in exploring what children on the autism spectrum would like to use VR in schools for? Through a mixed methods approach, we found that costly and technologically advanced HMDs were preferred (namely: HTC Vive). In addition, HMDs were reported as being enjoyable, physically and visually comfortable, easy to use, and exciting, and children wanted to use them again. They identified several potential usages for HMDs, including relaxing/feeling calm, being able to explore somewhere virtually before visiting in the real world, and to develop learning opportunities in school. We discuss these findings in the context of VR in classrooms, in addition to considering limitations and implication of our findings.},
	language = {eng},
	number = {1},
	journal = {Cyberpsychology, Behavior and Social Networking},
	author = {Newbutt, Nigel and Bradley, Ryan and Conley, Iian},
	month = jan,
	year = {2020},
	pmid = {31502866},
	keywords = {virtual reality, autism, Virtual Reality, Adolescent, Autistic Disorder, Child, classrooms, education, head-mounted display, Humans, Self-Help Devices, Smart Glasses},
	pages = {23--33},
	file = {2018_Newbutt.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2018_Newbutt.pdf:application/pdf},
}

@article{berenguer2020,
	title = {Exploring the {Impact} of {Augmented} {Reality} in {Children} and {Adolescents} with {Autism} {Spectrum} {Disorder}: {A} {Systematic} {Review}},
	volume = {17},
	issn = {1661-7827},
	shorttitle = {Exploring the {Impact} of {Augmented} {Reality} in {Children} and {Adolescents} with {Autism} {Spectrum} {Disorder}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7504463/},
	doi = {10.3390/ijerph17176143},
	abstract = {Autistic Spectrum Disorder (ASD) is a neurodevelopmental condition characterized by persistent difficulties in communication and social interaction along with a restriction in interests and the presence of repetitive behaviors. The development and use of augmented reality technology for autism has increased in recent years. However, little is known about the impact of these virtual reality technologies on clinical health symptoms. The aim of this systematic review was to investigate the impact of augmented reality through social, cognitive, and behavioral domains in children and adolescents with autism. This study is the first contribution that has carried out an evidence-based systematic review including relevant science databases about the effectiveness of augmented reality-based intervention in ASD. The initial search identified a total of 387 records. After the exclusion of papers that are not research studies and are duplicated articles and after screening the abstract and full text, 20 articles were selected for analysis. The studies examined suggest promising findings about the effectiveness of augmented reality-based treatments for the promotion, support, and protection of health and wellbeing in children and adolescents with autism. Finally, possible directions for future work are discussed.},
	number = {17},
	urldate = {2024-06-05},
	journal = {International Journal of Environmental Research and Public Health},
	author = {Berenguer, Carmen and Baixauli, Inmaculada and Gómez, Soledad and Andrés, María de El Puig and De Stasio, Simona},
	month = sep,
	year = {2020},
	pmid = {32847074},
	pmcid = {PMC7504463},
	pages = {6143},
	file = {2020_Berenguer_Exploring the Impact of.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2020_Berenguer_Exploring the Impact of.pdf:application/pdf},
}

@inproceedings{calabrese2019,
	title = {{DHP19}: {Dynamic} {Vision} {Sensor} {3D} {Human} {Pose} {Dataset}},
	shorttitle = {{DHP19}},
	url = {https://ieeexplore.ieee.org/document/9025364},
	doi = {10.1109/CVPRW.2019.00217},
	abstract = {Human pose estimation has dramatically improved thanks to the continuous developments in deep learning. However, marker-free human pose estimation based on standard frame-based cameras is still slow and power hungry for real-time feedback interaction because of the huge number of operations necessary for large Convolutional Neural Network (CNN) inference. Event-based cameras such as the Dynamic Vision Sensor (DVS) quickly output sparse moving-edge information. Their sparse and rapid output is ideal for driving low-latency CNNs, thus potentially allowing real-time interaction for human pose estimators. Although the application of CNNs to standard frame-based cameras for human pose estimation is well established, their application to event-based cameras is still under study. This paper proposes a novel benchmark dataset of human body movements, the Dynamic Vision Sensor Human Pose dataset (DHP19). It consists of recordings from 4 synchronized 346x260 pixel DVS cameras, for a set of 33 movements with 17 subjects. DHP19 also includes a 3D pose estimation model that achieves an average 3D pose estimation error of about 8 cm, despite the sparse and reduced input data from the DVS.},
	urldate = {2024-06-05},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Calabrese, Enrico and Taverni, Gemma and Easthope, Christopher Awai and Skriabine, Sophie and Corradi, Federico and Longinotti, Luca and Eng, Kynan and Delbruck, Tobi},
	month = jun,
	year = {2019},
	note = {ISSN: 2160-7516},
	keywords = {Cameras, Three-dimensional displays, Solid modeling, Pose estimation, Two dimensional displays, Vision sensors, Voltage control},
	pages = {1695--1704},
	file = {2019_Calabrese_DHP19.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Event-Based\\2019_Calabrese_DHP19.pdf:application/pdf},
}

@article{startsev2019,
	title = {{1D} {CNN} with {BLSTM} for automated classification of fixations, saccades, and smooth pursuits},
	volume = {51},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-018-1144-2},
	doi = {10.3758/s13428-018-1144-2},
	abstract = {Deep learning approaches have achieved breakthrough performance in various domains. However, the segmentation of raw eye-movement data into discrete events is still done predominantly either by hand or by algorithms that use hand-picked parameters and thresholds. We propose and make publicly available a small 1D-CNN in conjunction with a bidirectional long short-term memory network that classifies gaze samples as fixations, saccades, smooth pursuit, or noise, simultaneously assigning labels in windows of up to 1 s. In addition to unprocessed gaze coordinates, our approach uses different combinations of the speed of gaze, its direction, and acceleration, all computed at different temporal scales, as input features. Its performance was evaluated on a large-scale hand-labeled ground truth data set (GazeCom) and against 12 reference algorithms. Furthermore, we introduced a novel pipeline and metric for event detection in eye-tracking recordings, which enforce stricter criteria on the algorithmically produced events in order to consider them as potentially correct detections. Results show that our deep approach outperforms all others, including the state-of-the-art multi-observer smooth pursuit detector. We additionally test our best model on an independent set of recordings, where our approach stays highly competitive compared to literature methods.},
	language = {en},
	number = {2},
	urldate = {2024-06-05},
	journal = {Behavior Research Methods},
	author = {Startsev, Mikhail and Agtzidis, Ioannis and Dorr, Michael},
	month = apr,
	year = {2019},
	keywords = {Deep learning, Eye-movement classification, Smooth pursuit},
	pages = {556--572},
	file = {2019_Startsev_1D CNN with BLSTM for.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2019_Startsev_1D CNN with BLSTM for.pdf:application/pdf},
}

@article{birawo2022,
	title = {Review and {Evaluation} of {Eye} {Movement} {Event} {Detection} {Algorithms}},
	volume = {22},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/22/8810},
	doi = {10.3390/s22228810},
	abstract = {Eye tracking is a technology aimed at understanding the direction of the human gaze. Event detection is a process of detecting and classifying eye movements that are divided into several types. Nowadays, event detection is almost exclusively done by applying a detection algorithm to the raw recorded eye-tracking data. However, due to the lack of a standard procedure for how to perform evaluations, evaluating and comparing various detection algorithms in eye-tracking signals is very challenging. In this paper, we used data from a high-speed eye-tracker SMI HiSpeed 1250 system and compared event detection performance. The evaluation focused on fixations, saccades and post-saccadic oscillation classification. It used sample-by-sample comparisons to compare the algorithms and inter-agreement between algorithms and human coders. The impact of varying threshold values on threshold-based algorithms was examined and the optimum threshold values were determined. This evaluation differed from previous evaluations by using the same dataset to evaluate the event detection algorithms and human coders. We evaluated and compared the different algorithms from threshold-based, machine learning-based and deep learning event detection algorithms. The evaluation results show that all methods perform well for fixation and saccade detection; however, there are substantial differences in classification results. Generally, CNN (Convolutional Neural Network) and RF (Random Forest) algorithms outperform threshold-based methods.},
	language = {en},
	number = {22},
	urldate = {2024-06-05},
	journal = {Sensors},
	author = {Birawo, Birtukan and Kasprowski, Pawel},
	month = jan,
	year = {2022},
	note = {Number: 22
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {eye tracking, event detection algorithms, eye movement events, fixations, saccades},
	pages = {8810},
	file = {2022_Birawo_Review and Evaluation of Eye.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2022_Birawo_Review and Evaluation of Eye.pdf:application/pdf},
}

@article{wei2023a,
	title = {Machine learning based on eye-tracking data to identify {Autism} {Spectrum} {Disorder}: {A} systematic review and meta-analysis},
	volume = {137},
	issn = {1532-0480},
	shorttitle = {Machine learning based on eye-tracking data to identify {Autism} {Spectrum} {Disorder}},
	doi = {10.1016/j.jbi.2022.104254},
	abstract = {BACKGROUND: Machine learning has been widely used to identify Autism Spectrum Disorder (ASD) based on eye-tracking, but its accuracy is uncertain. We aimed to summarize the available evidence on the performances of machine learning algorithms in classifying ASD and typically developing (TD) individuals based on eye-tracking data.
METHODS: We searched Medline, Embase, Web of Science, Scopus, Cochrane Library, IEEE Xplore Digital Library, Wan Fang Database, China National Knowledge Infrastructure, Chinese BioMedical Literature Database, VIP Database for Chinese Technical Periodicals, from database inception to December 24, 2021. Studies using machine learning methods to classify ASD and TD individuals based on eye-tracking technologies were included. We extracted the data on study population, model performances, algorithms of machine learning, and paradigms of eye-tracking. This study is registered with PROSPERO, CRD42022296037.
RESULTS: 261 articles were identified, of which 24 studies with sample sizes ranging from 28 to 141 were included (n = 1396 individuals). Machine learning based on eye-tracking yielded the pooled classified accuracy of 81 \% (I2 = 73 \%), specificity of 79 \% (I2 = 61 \%), and sensitivity of 84 \% (I2 = 61 \%) in classifying ASD and TD individuals. In subgroup analysis, the accuracy was 88 \% (95 \% CI: 85-91 \%), 79 \% (95 \% CI: 72-84 \%), 71 \% (95 \% CI: 59-91 \%) for preschool-aged, school-aged, and adolescent-adult group. Eye-tracking stimuli and machine learning algorithms varied widely across studies, with social, static, and active stimuli and Support Vector Machine and Random Forest most commonly reported. Regarding the model performance evaluation, 15 studies reported their final results on validation datasets, four based on testing datasets, and five did not report whether they used validation datasets. Most studies failed to report the information on eye-tracking hardware and the implementation process.
CONCLUSION: Using eye-tracking data, machine learning has shown potential in identifying ASD individuals with high accuracy, especially in preschool-aged children. However, the heterogeneity between studies, the absence of test set-based performance evaluations, the small sample size, and the non-standardized implementation of eye-tracking might deteriorate the reliability of results. Further well-designed and well-executed studies with comprehensive and transparent reporting are needed to determine the optimal eye-tracking paradigms and machine learning algorithms.},
	language = {eng},
	journal = {Journal of Biomedical Informatics},
	author = {Wei, Qiuhong and Cao, Huiling and Shi, Yuan and Xu, Ximing and Li, Tingyu},
	month = jan,
	year = {2023},
	pmid = {36509416},
	keywords = {Machine learning, Autism Spectrum Disorder, Eye-tracking, Autism spectrum disorder, Adolescent, Child, Humans, Adult, Algorithms, Child, Preschool, Eye-Tracking Technology, Machine Learning, Meta-analysis, Reproducibility of Results},
	pages = {104254},
	file = {2023_wei.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2023_wei.pdf:application/pdf},
}

@article{cilia2022,
	title = {Eye-tracking {Dataset} to {Support} the {Research} on {Autism} {Spectrum} {Disorder}:},
	shorttitle = {Eye-tracking {Dataset} to {Support} the {Research} on {Autism} {Spectrum} {Disorder}},
	url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0011540900003523},
	doi = {10.5220/0011540900003523},
	abstract = {Semantic Scholar extracted view of "Eye-tracking Dataset to Support the Research on Autism Spectrum Disorder" by Federica Cilia et al.},
	urldate = {2024-06-05},
	journal = {Proceedings of the 1st Workshop on Scarce Data in Artificial Intelligence for Healthcare},
	author = {Cilia, Federica and Carette, Romuald and Elbattah, Mahmoud and Guérin, Jean-Luc and Dequen, Gilles},
	year = {2022},
	note = {Conference Name: Scarce Data in Artificial Intelligence for Healthcare
ISBN: 9789897586293
Place: Vienna, Austria
Publisher: SCITEPRESS - Science and Technology Publications},
	keywords = {ASD, eye tracking, datasets},
	pages = {59--64},
	file = {2022_Cilia_Eye-tracking Dataset to.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2022_Cilia_Eye-tracking Dataset to.pdf:application/pdf},
}

@article{xu2021,
	title = {A new bio-inspired metric based on eye movement data for classifying {ASD} and typically developing children},
	volume = {94},
	issn = {0923-5965},
	url = {https://www.sciencedirect.com/science/article/pii/S0923596521000242},
	doi = {10.1016/j.image.2021.116171},
	abstract = {In this paper, we propose a new bio-inspired metric for classifying autism spectrum disorder (ASD) children and typically developed (TD) children. The model used in the Saliency4ASD Grand Challenge at ICME 2019 uses linear regression and prior probability to process distance and time data respectively. Unfortunately, this model performs unsatisfactorily because the visual attention characteristics of ASD and TD children are similar under certain circumstances. Therefore, we screen stimulus materials to select these with significant differences between eye movement distribution of ASD and TD children. We calculate the SSIM value of the ASD and TD data of each picture and conduct the subjective experiments to classify the stimulus materials into two categories: the images with the similar attention map for ASD and TD children; and the images with the dissimilar attention map for ASD and TD children. Owing to the biological property of eye, a viewing angle will be formed when people are observing a picture. Meanwhile, gazing at one point of longer time means more attention. Thus, we pick the point of the longest fixation time for each data group and extract the patch centered on this point. Three point-add strategies are afterward utilized to add points on this patch. Subsequently, a new bio-inspired metric based on graph theory is developed. Experimental results show that the new model outperforms our previous model with a classification accuracy of 72.3\%.},
	urldate = {2024-06-05},
	journal = {Signal Processing: Image Communication},
	author = {Xu, Shuning and Yan, Junbing and Hu, Menghan},
	month = may,
	year = {2021},
	keywords = {Autism Spectrum Disorder (ASD), Curve similarity, Saliency model},
	pages = {116171},
	file = {ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\S7QG88UJ\\S0923596521000242.html:text/html},
}

@article{gehrig2024,
	title = {Low-latency automotive vision with event cameras},
	volume = {629},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07409-w},
	doi = {10.1038/s41586-024-07409-w},
	abstract = {The computer vision algorithms used currently in advanced driver assistance systems rely on image-based RGB cameras, leading to a critical bandwidth–latency trade-off for delivering safe driving experiences. To address this, event cameras have emerged as alternative vision sensors. Event cameras measure the changes in intensity asynchronously, offering high temporal resolution and sparsity, markedly reducing bandwidth and latency requirements1. Despite these advantages, event-camera-based algorithms are either highly efficient but lag behind image-based ones in terms of accuracy or sacrifice the sparsity and efficiency of events to achieve comparable results. To overcome this, here we propose a hybrid event- and frame-based object detector that preserves the advantages of each modality and thus does not suffer from this trade-off. Our method exploits the high temporal resolution and sparsity of events and the rich but low temporal resolution information in standard images to generate efficient, high-rate object detections, reducing perceptual and computational latency. We show that the use of a 20 frames per second (fps) RGB camera plus an event camera can achieve the same latency as a 5,000-fps camera with the bandwidth of a 45-fps camera without compromising accuracy. Our approach paves the way for efficient and robust perception in edge-case scenarios by uncovering the potential of event cameras2.},
	language = {en},
	number = {8014},
	urldate = {2024-06-11},
	journal = {Nature},
	author = {Gehrig, Daniel and Scaramuzza, Davide},
	month = may,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Engineering},
	pages = {1034--1040},
	file = {2024_Gehrig and Scaramuzza_Low-latency automotive vision with event cameras.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Event-Based\\2024_Gehrig and Scaramuzza_Low-latency automotive vision with event cameras.pdf:application/pdf},
}

@article{2023,
	title = {Neuromorphic high-frequency {3D} dancing pose estimation in dynamic environment},
	volume = {547},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231223005118},
	doi = {10.1016/j.neucom.2023.126388},
	abstract = {Technology-mediated dance experiences, as a medium of entertainment, are a key element in both traditional and virtual reality-based gaming platforms.…},
	language = {en-US},
	urldate = {2024-06-11},
	journal = {Neurocomputing},
	month = aug,
	year = {2023},
	note = {Publisher: Elsevier},
	pages = {126388},
}

@article{zhang2023a,
	title = {Neuromorphic high-frequency {3D} dancing pose estimation in dynamic environment},
	volume = {547},
	issn = {0925-2312},
	url = {https://doi.org/10.1016/j.neucom.2023.126388},
	doi = {10.1016/j.neucom.2023.126388},
	abstract = {Technology-mediated dance experiences, as a medium of entertainment, are a key element in both traditional and virtual reality-based gaming platforms. These platforms predominantly depend on unobtrusive and continuous human pose estimation as a means of capturing input. Current solutions primarily employ RGB or RGB-Depth cameras for dance gaming applications; however, the former is hindered by low-light conditions due to motion blur and reduced sensitivity, while the latter exhibits excessive power consumption, diminished frame rates, and restricted operational distance. Boasting ultra-low latency, energy efficiency, and a wide dynamic range, neuromorphic cameras present a viable solution to surmount these limitations. Here, we introduce YeLan, a neuromorphic camera-driven, three-dimensional, high-frequency human pose estimation (HPE) system capable of withstanding low-light environments and dynamic backgrounds. We have compiled the first-ever neuromorphic camera dance HPE dataset and devised a fully adaptable motion-to-event, physics-conscious simulator. YeLan surpasses baseline models under strenuous conditions and exhibits resilience against varying clothing types, background motion, viewing angles, occlusions, and lighting fluctuations.},
	number = {C},
	urldate = {2024-06-11},
	journal = {Neurocomputing},
	author = {Zhang, Zhongyang and Chai, Kaidong and Yu, Haowen and Majaj, Ramzi and Walsh, Francesca and Wang, Edward and Mahbub, Upal and Siegelmann, Hava and Kim, Donghyun and Rahman, Tauhidur},
	month = aug,
	year = {2023},
	keywords = {3D Human Pose Estimation, Dataset, Deep Learning, DVS, Dynamic Vision Sensor, Event Camera, HPE, Human Pose Estimation, Neuromorphic Camera, Simulator, Technology-Mediated Dancing, TMD},
	file = {2023_Zhang et al._Neuromorphic high-frequency 3D dancing pose estimation in dynamic environment.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Event-Based\\2023_Zhang et al._Neuromorphic high-frequency 3D dancing pose estimation in dynamic environment.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\DDTFEVVV\\S0925231223005118.html:text/html},
}

@article{gehrig2021,
	title = {{DSEC}: {A} {Stereo} {Event} {Camera} {Dataset} for {Driving} {Scenarios}},
	volume = {6},
	issn = {2377-3766},
	shorttitle = {{DSEC}},
	url = {https://ieeexplore.ieee.org/document/9387069},
	doi = {10.1109/LRA.2021.3068942},
	abstract = {Once an academic venture, autonomous driving has received unparalleled corporate funding in the last decade. Still, operating conditions of current autonomous cars are mostly restricted to ideal scenarios. This means that driving in challenging illumination conditions such as night, sunrise, and sunset remains an open problem. In these cases, standard cameras are being pushed to their limits in terms of low light and high dynamic range performance. To address these challenges, we propose, DSEC, a new dataset that contains such demanding illumination conditions and provides a rich set of sensory data. DSEC offers data from a wide-baseline stereo setup of two color frame cameras and two high-resolution monochrome event cameras. In addition, we collect lidar data and RTK GPS measurements, both hardware synchronized with all camera data. One of the distinctive features of this dataset is the inclusion of high-resolution event cameras. Event cameras have received increasing attention for their high temporal resolution and high dynamic range performance. However, due to their novelty, event camera datasets in driving scenarios are rare. This work presents the first high resolution, large scale stereo dataset with event cameras. The dataset contains 53 sequences collected by driving in a variety of illumination conditions and provides ground truth disparity for the development and evaluation of event-based stereo algorithms.},
	number = {3},
	urldate = {2024-06-11},
	journal = {IEEE Robotics and Automation Letters},
	author = {Gehrig, Mathias and Aarents, Willem and Gehrig, Daniel and Scaramuzza, Davide},
	month = jul,
	year = {2021},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Cameras, Lighting, Robot vision systems, computer vision for transportation, data sets for robot learning, Data sets for robotic vision, Dynamic range, Global Positioning System, Image color analysis, Standards},
	pages = {4947--4954},
	file = {2021_Gehrig et al._DSEC A Stereo Event Camera Dataset for Driving Scenarios.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Event-Based\\2021_Gehrig et al._DSEC A Stereo Event Camera Dataset for Driving Scenarios.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\L3YQM3RI\\9387069.html:text/html},
}

@inproceedings{xu2020,
	title = {{EventCap}: {Monocular} {3D} {Capture} of {High}-{Speed} {Human} {Motions} {Using} an {Event} {Camera}},
	shorttitle = {{EventCap}},
	url = {https://ieeexplore.ieee.org/document/9157340},
	doi = {10.1109/CVPR42600.2020.00502},
	abstract = {The high frame rate is a critical requirement for capturing fast human motions. In this setting, existing markerless image-based methods are constrained by the lighting requirement, the high data bandwidth and the consequent high computation overhead. In this paper, we propose EventCap - the first approach for 3D capturing of high-speed human motions using a single event camera. Our method combines model-based optimization and CNN-based human pose detection to capture high frequency motion details and to reduce the drifting in the tracking. As a result, we can capture fast motions at millisecond resolution with significantly higher data efficiency than using high frame rate videos. Experiments on our new event-based fast human motion dataset demonstrate the effectiveness and accuracy of our method, as well as its robustness to challenging lighting conditions.},
	urldate = {2024-06-11},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Xu, Lan and Xu, Weipeng and Golyanik, Vladislav and Habermann, Marc and Fang, Lu and Theobalt, Christian},
	month = jun,
	year = {2020},
	note = {ISSN: 2575-7075},
	keywords = {Cameras, Tracking, Three-dimensional displays, Streaming media, Bandwidth, Two dimensional displays, Trajectory},
	pages = {4967--4977},
	file = {2020_Xu et al._EventCap Monocular 3D Capture of High-Speed Human Motions Using an Event Camera.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Body-Tracking\\2020_Xu et al._EventCap Monocular 3D Capture of High-Speed Human Motions Using an Event Camera.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\2E4KQ74V\\9157340.html:text/html},
}

@article{shao2024,
	title = {A temporal densely connected recurrent network for event-based human pose estimation},
	volume = {147},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320323007458},
	doi = {10.1016/j.patcog.2023.110048},
	abstract = {Event camera is an emerging bio-inspired vision sensors that report per-pixel brightness changes asynchronously. It holds noticeable advantage of high dynamic range, high speed response, and low power budget that enable it to best capture local motions in uncontrolled environments. This motivates us to unlock the potential of event cameras for human pose estimation, as the human pose estimation with event cameras is rarely explored. Due to the novel paradigm shift from conventional frame-based cameras, however, event signals in a time interval contain very limited information, as event cameras can only capture the moving body parts and ignores those static body parts, resulting in some parts to be incomplete or even disappeared in the time interval. This paper proposes a novel densely connected recurrent architecture to address the problem of incomplete information. By this recurrent architecture, we can explicitly model not only the sequential but also non-sequential geometric consistency across time steps to accumulate information from previous frames to recover the entire human bodies, achieving a stable and accurate human pose estimation from event data. Moreover, to better evaluate our model, we collect a large-scale multimodal event-based dataset that comes with human pose annotations, which is by far the most challenging one to the best of our knowledge. The experimental results on two public datasets and our own dataset demonstrate the effectiveness and strength of our approach. Code is available online for facilitating the future research.},
	urldate = {2024-06-11},
	journal = {Pattern Recognition},
	author = {Shao, Zhanpeng and Wang, Xueping and Zhou, Wen and Wang, Wuzhen and Yang, Jianyu and Li, Youfu},
	month = mar,
	year = {2024},
	keywords = {Dataset, Dense connections, Event signal, Human pose estimation, Recurrent network},
	pages = {110048},
	file = {2024_Shao et al._A temporal densely connected recurrent network for event-based human pose estimation.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Body-Tracking\\2024_Shao et al._A temporal densely connected recurrent network for event-based human pose estimation.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\TBA7N6BN\\S0031320323007458.html:text/html},
}

@article{baldwin2023,
	title = {Time-{Ordered} {Recent} {Event} ({TORE}) {Volumes} for {Event} {Cameras}},
	volume = {45},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/document/9767613},
	doi = {10.1109/TPAMI.2022.3172212},
	abstract = {Event cameras are an exciting, new sensor modality enabling high-speed imaging with extremely low-latency and wide dynamic range. Unfortunately, most machine learning architectures are not designed to directly handle sparse data, like that generated from event cameras. Many state-of-the-art algorithms for event cameras rely on interpolated event representations—obscuring crucial timing information, increasing the data volume, and limiting overall network performance. This paper details an event representation called Time-Ordered Recent Event (TORE) volumes. TORE volumes are designed to compactly store raw spike timing information with minimal information loss. This bio-inspired design is memory efficient, computationally fast, avoids time-blocking (i.e., fixed and predefined frame rates), and contains “local memory” from past data. The design is evaluated on a wide range of challenging tasks (e.g., event denoising, image reconstruction, classification, and human pose estimation) and is shown to dramatically improve state-of-the-art performance. TORE volumes are an easy-to-implement replacement for any algorithm currently utilizing event representations.},
	number = {2},
	urldate = {2024-06-11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Baldwin, R. Wes and Liu, Ruixu and Almatrafi, Mohammed and Asari, Vijayan and Hirakawa, Keigo},
	month = feb,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Cameras, Task analysis, event camera, Voltage control, Computer architecture, denoising, Dynamic vision sensor, human pose estimation, neuromorphic, Noise reduction, Retina, Timing},
	pages = {2519--2532},
	file = {2023_Baldwin et al._Time-Ordered Recent Event (TORE) Volumes for Event Cameras.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Event-Based\\2023_Baldwin et al._Time-Ordered Recent Event (TORE) Volumes for Event Cameras.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\MFPYB68Z\\9767613.html:text/html},
}

@book{prince2023understanding,
	title = {Understanding deep learning},
	url = {http://udlbook.com},
	publisher = {The MIT Press},
	author = {Prince, Simon J.D.},
	year = {2023},
	file = {2023_Prince_Understanding deep learning.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ToBeSorted\\2023_Prince_Understanding deep learning.pdf:application/pdf},
}

@inproceedings{gehrig2023,
	address = {Vancouver, BC, Canada},
	title = {Recurrent {Vision} {Transformers} for {Object} {Detection} with {Event} {Cameras}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-0129-8},
	url = {https://ieeexplore.ieee.org/document/10204090/},
	doi = {10.1109/CVPR52729.2023.01334},
	abstract = {We present Recurrent Vision Transformers (RVTs), a novel backbone for object detection with event cameras. Event cameras provide visual information with submillisecond latency at a high-dynamic range and with strong robustness against motion blur. These unique properties offer great potential for low-latency object detection and tracking in time-critical scenarios. Prior work in event-based vision has achieved outstanding detection performance but at the cost of substantial inference time, typically beyond 40 milliseconds. By revisiting the high-level design of recurrent vision backbones, we reduce inference time by a factor of 6 while retaining similar performance. To achieve this, we explore a multi-stage design that utilizes three key concepts in each stage: ﬁrst, a convolutional prior that can be regarded as a conditional positional embedding. Second, local and dilated global self-attention for spatial feature interaction. Third, recurrent temporal feature aggregation to minimize latency while retaining temporal information. RVTs can be trained from scratch to reach state-of-the-art performance on event-based object detection - achieving an mAP of 47.2\% on the Gen1 automotive dataset. At the same time, RVTs offer fast inference ({\textless} 12 ms on a T4 GPU) and favorable parameter efﬁciency (5× fewer than prior art). Our study brings new insights into effective design choices that can be fruitful for research beyond event-based vision.},
	language = {en},
	urldate = {2024-06-14},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Gehrig, Mathias and Scaramuzza, Davide},
	month = jun,
	year = {2023},
	pages = {13884--13893},
	file = {2023_Gehrig and Scaramuzza_Recurrent Vision Transformers for Object Detection with Event Cameras.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Event-Based\\2023_Gehrig and Scaramuzza_Recurrent Vision Transformers for Object Detection with Event Cameras.pdf:application/pdf},
}

@article{paquet2017,
	title = {Evaluation of neuromuscular tone phenotypes in children with autism spectrum disorder: {An} exploratory study},
	volume = {47},
	issn = {0987-7053},
	shorttitle = {Evaluation of neuromuscular tone phenotypes in children with autism spectrum disorder},
	url = {https://www.sciencedirect.com/science/article/pii/S0987705317300540},
	doi = {10.1016/j.neucli.2017.07.001},
	abstract = {Objective
Motor disorders are known in autism spectrum disorder (ASD), but muscle tone assessments are rarely performed. Muscle tone underpins movement. We investigated muscle tone in 34 ASD children using a standardized neuro-developmental battery, which uses the French norms for muscular tone in children.
Methods
Dangling and extensibility were used to examine passive muscle tone in the upper and lower limbs and the body axis. A comparison between muscles of the right and left sides enabled the determination of tonic laterality.
Results
We found a disharmonious tonic typology, with a tonic component for the muscles of the trunk and the proximal muscles of the lower limbs and a laxity component for the ankles and the proximal and distal muscles of the upper limbs (wrists and shoulders). No establishment of tonic laterality was found in the upper limbs in 61\% of ASD children (P{\textless}0.001).
Conclusion
The disturbed tonic organization influenced by subcortical structures, such as the cerebellum, may partially explain the motor disorders, and indefinite tonic laterality may also be linked to low hemispheric brain dominance described in autism. This preliminary examination is necessary before any gross motor assessments to understand the nature of movement disorders, explore typologies and highlight possible soft neuro-motor signs.
Résumé
Objectifs
Les troubles moteurs sont connus dans le trouble du spectre de l’autisme (TSA), mais l’évaluation du tonus musculaire est rarement réalisée. Pourtant le tonus musculaire sous-tend le mouvement. Nous avons étudié le tonus musculaire chez 34 enfants avec TSA à partir d’une batterie neurodéveloppementale standardisée, qui utilise les normes françaises chez l’enfant pour le tonus musculaire.
Méthodes
Le ballant et l’extensibilité ont été utilisés pour examiner le tonus musculaire passif au niveau des membres supérieurs et inférieurs puis de l’axe du corps. Une comparaison entre les muscles des côtés droit et gauche a permis la détermination d’une latéralité tonique.
Résultats
Nous avons trouvé une typologie tonique dysharmonique, avec une composante d’hypertonie au niveau des muscles du tronc et des muscles proximaux des membres inférieurs et une composante d’hyperlaxité pour les chevilles et les muscles proximaux et distal des membres supérieurs (poignets et épaules). Une absence de prédominance tonique au niveau des membres supérieurs a été trouvé chez 61 \% des enfants atteints de TSA (p{\textless}0,001).
Conclusion
L’organisation tonique perturbée influencée par les structures sous-corticales, comme le cervelet, pourrait expliquer en partie les troubles moteurs décrits dans les TSA, de plus, la latéralité tonique indéterminée pourrait également être liée à une dominance cérébrale hémisphérique absente ou peu dominante décrite dans l’autisme. Cet examen préliminaire est nécessaire avant toute évaluation de la motricité globale pour comprendre la nature des troubles du mouvement, explorer les typologies et mettre en évidence des signes neurologiques doux.},
	number = {4},
	urldate = {2024-06-17},
	journal = {Neurophysiologie Clinique/Clinical Neurophysiology},
	author = {Paquet, Aude and Olliac, Bertrand and Golse, Bernard and Vaivre-Douret, Laurence},
	month = sep,
	year = {2017},
	keywords = {Autism spectrum disorder, Children, Enfants, Évaluation neurodéveloppementale standardisée, Muscular tone, Neurodevelopmental standardized assessment, Tonus musculaire, Trouble du spectre de l’autisme},
	pages = {261--268},
	file = {2017_Paquet et al._Evaluation of neuromuscular tone phenotypes in children with autism spectrum disorder An explorator.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2017_Paquet et al._Evaluation of neuromuscular tone phenotypes in children with autism spectrum disorder An explorator.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\ATJU7UEW\\S0987705317300540.html:text/html},
}

@article{paquet2019,
	title = {Nature of motor impairments in autism spectrum disorder: {A} comparison with developmental coordination disorder},
	volume = {41},
	issn = {1380-3395},
	shorttitle = {Nature of motor impairments in autism spectrum disorder},
	url = {https://doi.org/10.1080/13803395.2018.1483486},
	doi = {10.1080/13803395.2018.1483486},
	abstract = {Introduction: Several authors have suggested the existence of motor disorders associated with developmental coordination disorder (DCD) in individuals with autism spectrum disorder (ASD). However, there are few comparative studies of psychomotor profiles that include assessments of neurological soft signs in children with ASD or DCD. We used a neuropsychomotor assessment for children with ASD from a standardized neurodevelopmental examination to understand the nature of the difficulties these children encounter. To uncover the differences and similarities in psychomotor profiles, we compared the profiles of children with ASD with those of children with DCD and focused on two recently described DCD subgroups: visuospatial–constructional (VSC) and mixed (MX). Methods: We compared 18 children with ASD and 58 children with DCD (33 with VSC-DCD and 25 with MX-DCD) who were assessed with a battery of French-language tests (the NP-MOT) to evaluate the neuropsychomotor functions associated with visual perception and visual–spatial–motor structuring. Results: Although there were similarities between the profiles of children with ASD and those with DCD (VSC-DCD or MX-DCD), these similarities were not associated with the predictive diagnostic markers that characterized subtypes of DCD. Instead, many variables (visuospatial–motor structuration, synkinetic movements, dynamic balance, manual dexterity, coordination, praxis, bodily spatial integration, and digital perception) differed among the three groups; the best performance was observed in the children with ASD. Conclusion: The neuropsychomotor profiles of children with ASD and those with VSC-DCD or MX-DCD differed, and these differences are discussed. Our results highlight that impairments of ASD are specific about lateralization disturbances and support the hypothesis of proprioceptive impairment due to visual fixation problems influenced by muscular tone in relation to the subcortical and cortical structures and possible interhemispheric disorder. Thus, some neuropsychomotor functions that underpin both gestures and a set of motor skills are affected.},
	number = {1},
	urldate = {2024-06-17},
	journal = {Journal of Clinical and Experimental Neuropsychology},
	author = {Paquet, Aude and Olliac, Bertrand and Golse, Bernard and Vaivre-Douret, Laurence},
	month = jan,
	year = {2019},
	pmid = {29923455},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/13803395.2018.1483486},
	keywords = {Autism spectrum disorder, developmental coordination disorder, motor impairments, neurological soft signs, psychomotor profile},
	pages = {1--14},
	file = {2019_Paquet et al._Nature of motor impairments in autism spectrum disorder A comparison with developmental coordinatio.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2019_Paquet et al._Nature of motor impairments in autism spectrum disorder A comparison with developmental coordinatio.pdf:application/pdf},
}

@article{coll2020a,
	title = {Sensorimotor skills in autism spectrum disorder: {A} meta-analysis},
	volume = {76},
	issn = {17509467},
	shorttitle = {Sensorimotor skills in autism spectrum disorder},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S175094672030060X},
	doi = {10.1016/j.rasd.2020.101570},
	abstract = {Background: Sensorimotor skills are often reported as atypical in individuals with autism spectrum disorder (ASD). Little is known about how sensorimotor skills in ASD may vary across development and with symptom severity. The main objective of this study was to conduct a comprehensive quantitative meta-analysis of sensorimotor skills in ASD. The speciﬁc aims were: to assess impairment of gross and ﬁne sensorimotor skills in ASD, to examine the eﬀect of age on sensorimotor skills in ASD and to examine the relationship between sensorimotor skills and ASD symptom severity.
Method: An exhaustive search was conducted in Psycnet, PubMed, Web of Science and Cochrane Database to identify studies in ASD from 1980 to 2018 that involved quantitative evaluations of motor coordination, motor impairments, arm movement, gait, postural stability, visuomotor or auditory motor integration. A total of 139 studies were included and this represent 3436 individuals with ASD.
Results: Results strongly support the presence of deﬁcits in overall sensorimotor abilities in ASD (Hedges’ g = 1.22, p {\textless} 0.001) and these atypicalities extended to ﬁne and gross sensorimotor abilities. Sensorimotor abilities increased with age, but did not appear to covary with symptom severity.
Conclusions: These results highlight the importance to target these deﬁcits in future interventions and consider the impact of sensorimotor impairments across research, therapy, and educational settings.},
	language = {en},
	urldate = {2024-06-17},
	journal = {Research in Autism Spectrum Disorders},
	author = {Coll, Sarah-Maude and Foster, Nicholas E.V. and Meilleur, Alexa and Brambati, Simona M. and Hyde, Krista L.},
	month = aug,
	year = {2020},
	pages = {101570},
	file = {2020_Coll et al._Sensorimotor skills in autism spectrum disorder A meta-analysis.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\2020_Coll et al._Sensorimotor skills in autism spectrum disorder A meta-analysis.pdf:application/pdf},
}

@article{kaur2018a,
	title = {Comparing motor performance, praxis, coordination, and interpersonal synchrony between children with and without {Autism} {Spectrum} {Disorder} ({ASD})},
	volume = {72},
	issn = {08914222},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0891422217302949},
	doi = {10.1016/j.ridd.2017.10.025},
	abstract = {Children with Autism Spectrum Disorder (ASD) have basic motor impairments in balance, gait, and coordination as well as autism-speciﬁc impairments in praxis/motor planning and interpersonal synchrony. Majority of the current literature focuses on isolated motor behaviors or domains. Additionally, the relationship between cognition, symptom severity, and motor performance in ASD is unclear. We used a comprehensive set of measures to compare gross and ﬁne motor, praxis/imitation, motor coordination, and interpersonal synchrony skills across three groups of children between 5 and 12 years of age: children with ASD with high IQ (HASD), children with ASD with low IQ (LASD), and typically developing (TD) children. We used the Bruininks-Oseretsky Test of Motor Proﬁciency and the Bilateral Motor Coordination subtest of the Sensory Integration and Praxis Tests to assess motor performance and praxis skills respectively. Children were also examined while performing simple and complex rhythmic upper and lower limb actions on their own (solo context) and with a social partner (social context). Both ASD groups had lower gross and ﬁne motor scores, greater praxis errors in total and within various error types, lower movement rates, greater movement variability, and weaker interpersonal synchrony compared to the TD group. In addition, the LASD group had lower gross motor scores and greater mirroring errors compared to the HASD group. Overall, a variety of motor impairments are present across the entire spectrum of children with ASD, regardless of their IQ scores. Both, ﬁne and gross motor performance signiﬁcantly correlated with IQ but not with autism severity; however, praxis errors (mainly, total, overﬂow, and rhythmicity) strongly correlated with autism severity and not IQ. Our study ﬁndings highlight the need for clinicians and therapists to include motor evaluations and interventions in the standard-of-care of children with ASD and for the broader autism community to recognize dyspraxia as an integral part of the deﬁnition of ASD.},
	language = {en},
	urldate = {2024-06-17},
	journal = {Research in Developmental Disabilities},
	author = {Kaur, Maninderjit and M. Srinivasan, Sudha and N. Bhat, Anjana},
	month = jan,
	year = {2018},
	pages = {79--95},
	file = {2018_Kaur et al._Comparing motor performance, praxis, coordination, and interpersonal synchrony between children with.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2018_Kaur et al._Comparing motor performance, praxis, coordination, and interpersonal synchrony between children with.pdf:application/pdf},
}

@article{kangarani-farahani2024,
	title = {Motor {Impairments} in {Children} with {Autism} {Spectrum} {Disorder}: {A} {Systematic} {Review} and {Meta}-analysis},
	volume = {54},
	issn = {1573-3432},
	shorttitle = {Motor {Impairments} in {Children} with {Autism} {Spectrum} {Disorder}},
	url = {https://doi.org/10.1007/s10803-023-05948-1},
	doi = {10.1007/s10803-023-05948-1},
	abstract = {This article comprehensively reviews motor impairments in children with autism spectrum disorder (ASD) to: (1) determine the prevalence of motor problems in children with ASD; (2) understand the nature of motor difficulties in ASD and whether they are consistent with developmental coordination disorder (DCD); and (3) determine if the term DCD was used as a co-occurring diagnosis in children with ASD after publication of the DSM-5 in 2013. The following databases were systematically searched: MEDLINE, EMBASE, CINAHL, and PsycINFO from 2010 to December 2021. Articles were included if they: (1) were peer-reviewed and published in a scientific journal; (2) included children with ASD who were between 5 and 12 years; (3) used motor or function measures to assess motor abilities in children with ASD. Studies that included children with intellectual disabilities were excluded. Two independent reviewers reviewed titles, abstracts, and full-text articles for inclusion. Twenty-seven studies met the inclusion criteria and were assessed for quality by two independent reviewers using the Appraisal tool for Cross-Sectional Studies. The majority of articles (92.5\%) indicated that 50–88\% of children with ASD had significant motor impairments on standardized motor assessments and/or functional questionnaires. The nature of motor and function problems in ASD were consistent with DCD; however, only three out of 20 papers (15\%) that were published from 2014 described the motor problems as DCD. One study reported that 15.1\% of children with ASD with motor impairments had a co-occurring diagnosis of DCD, suggesting that DCD is under-recognized in this clinical population.},
	language = {en},
	number = {5},
	urldate = {2024-06-17},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Kangarani-Farahani, Melika and Malik, Myrah Anum and Zwicker, Jill G.},
	month = may,
	year = {2024},
	keywords = {Autism spectrum disorder, Children, Developmental coordination disorder, Motor deficits, Motor skills disorder},
	pages = {1977--1997},
	file = {2024_Kangarani-Farahani et al._Motor Impairments in Children with Autism Spectrum Disorder A Systematic Review and Meta-analysis.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2024_Kangarani-Farahani et al._Motor Impairments in Children with Autism Spectrum Disorder A Systematic Review and Meta-analysis.pdf:application/pdf},
}

@article{little2024,
	title = {Flourishing and {Functional} {Difficulties} among {Autistic} {Youth}: {A} {Confirmatory} {Factor} {Analysis}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-9067},
	shorttitle = {Flourishing and {Functional} {Difficulties} among {Autistic} {Youth}},
	url = {https://www.mdpi.com/2227-9067/11/3/325},
	doi = {10.3390/children11030325},
	abstract = {The International Classification of Functioning, Disability, and Health for Children and Youth outlines body structures and functions and activities and participation to fully describe elements that support or detract from participation. While flourishing has gained attention in recent literature, research also points to the role of functional difficulties among autistic youth in influencing participation. Clearly, function is a multi-dimensional and complex construct and likely consists of both indicators of flourishing and functional difficulties. We used data from the National Survey of Children’s Health (NSCH) from 2016 to 2020 to identify aspects of flourishing functional difficulties to achieve the following aims: (1) Investigate the factor structure of flourishing and functional difficulties among autistic youth ages 10–17 years; and (2) examine the extent to which child variables (i.e., sex, age, race, ethnicity, autism severity, poverty) are associated with flourishing and functional difficulties. Autistic children (n = 2960) between the ages of 10 and 17 years were included. We used confirmatory factor analysis followed by a multivariate general linear model (GLM) to examine the association between child variables and factors. Results indicated a six-factor structure (medical conditions, instrumental activities of daily living, activities of daily living, social competence, behavioral control, and school motivation) with good model fit (root mean square error of approximation = 0.08 [p = 0.926], comparative fit index = 0.94, Tucker–Lewis index = 0.91). Multivariate GLM showed that child factors were differentially and significantly associated with factors of functional difficulties and flourishing. Current findings suggest that 16 items measured by the NSCH result in a six-factor structure of flourishing and functional difficulties among autistic youth. A comprehensive approach to capture function among autistic youth must assess aspects of flourishing and difficulties.},
	language = {en},
	number = {3},
	urldate = {2024-06-17},
	journal = {Children},
	author = {Little, Lauren M. and Schwefel, Laura-Lee},
	month = mar,
	year = {2024},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {autism, flourishing, functional difficulties},
	pages = {325},
	file = {2024_Little and Schwefel_Flourishing and Functional Difficulties among Autistic Youth A Confirmatory Factor Analysis.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2024_Little and Schwefel_Flourishing and Functional Difficulties among Autistic Youth A Confirmatory Factor Analysis.pdf:application/pdf},
}

@article{li2024,
	title = {Identification of autism spectrum disorder based on electroencephalography: {A} systematic review},
	volume = {170},
	issn = {0010-4825},
	shorttitle = {Identification of autism spectrum disorder based on electroencephalography},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482524001598},
	doi = {10.1016/j.compbiomed.2024.108075},
	abstract = {Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder characterized by difficulties in social communication and repetitive and stereotyped behaviors. According to the World Health Organization, about 1 in 100 children worldwide has autism. With the global prevalence of ASD, timely and accurate diagnosis has been essential in enhancing the intervention effectiveness for ASD children. Traditional ASD diagnostic methods rely on clinical observations and behavioral assessment, with the disadvantages of time-consuming and lack of objective biological indicators. Therefore, automated diagnostic methods based on machine learning and deep learning technologies have emerged and become significant since they can achieve more objective, efficient, and accurate ASD diagnosis. Electroencephalography (EEG) is an electrophysiological monitoring method that records changes in brain spontaneous potential activity, which is of great significance for identifying ASD children. By analyzing EEG data, it is possible to detect abnormal synchronous neuronal activity of ASD children. This paper gives a comprehensive review of the EEG-based ASD identification using traditional machine learning methods and deep learning approaches, including their merits and potential pitfalls. Additionally, it highlights the challenges and the opportunities ahead in search of more effective and efficient methods to automatically diagnose autism based on EEG signals, which aims to facilitate automated ASD identification.},
	urldate = {2024-06-17},
	journal = {Computers in Biology and Medicine},
	author = {Li, Jing and Kong, Xiaoli and Sun, Linlin and Chen, Xu and Ouyang, Gaoxiang and Li, Xiaoli and Chen, Shengyong},
	month = mar,
	year = {2024},
	keywords = {Machine learning, Electroencephalography, Autism spectrum disorder, Deep leaning, Multi-model fusion},
	pages = {108075},
	file = {2024_Li et al._Identification of autism spectrum disorder based on electroencephalography A systematic review.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2024_Li et al._Identification of autism spectrum disorder based on electroencephalography A systematic review.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\WUYBAWRS\\S0010482524001598.html:text/html},
}

@article{vabalas2020,
	title = {Applying {Machine} {Learning} to {Kinematic} and {Eye} {Movement} {Features} of a {Movement} {Imitation} {Task} to {Predict} {Autism} {Diagnosis}},
	volume = {10},
	copyright = {2020 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-020-65384-4},
	doi = {10.1038/s41598-020-65384-4},
	abstract = {Autism is a developmental condition currently identified by experts using observation, interview, and questionnaire techniques and primarily assessing social and communication deficits. Motor function and movement imitation are also altered in autism and can be measured more objectively. In this study, motion and eye tracking data from a movement imitation task were combined with supervised machine learning methods to classify 22 autistic and 22 non-autistic adults. The focus was on a reliable machine learning application. We have used nested validation to develop models and further tested the models with an independent data sample. Feature selection was aimed at selection stability to assure result interpretability. Our models predicted diagnosis with 73\% accuracy from kinematic features, 70\% accuracy from eye movement features and 78\% accuracy from combined features. We further explored features which were most important for predictions to better understand movement imitation differences in autism. Consistent with the behavioural results, most discriminative features were from the experimental condition in which non-autistic individuals tended to successfully imitate unusual movement kinematics while autistic individuals tended to fail. Machine learning results show promise that future work could aid in the diagnosis process by providing quantitative tests to supplement current qualitative ones.},
	language = {en},
	number = {1},
	urldate = {2024-06-19},
	journal = {Scientific Reports},
	author = {Vabalas, Andrius and Gowen, Emma and Poliakoff, Ellen and Casson, Alexander J.},
	month = may,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	keywords = {Machine learning, autism, Human behaviour},
	pages = {8346},
	file = {2020_Vabalas et al._Applying Machine Learning to Kinematic and Eye Movement Features of a Movement Imitation Task to Pre.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2020_Vabalas et al._Applying Machine Learning to Kinematic and Eye Movement Features of a Movement Imitation Task to Pre.pdf:application/pdf},
}

@article{gowen2020,
	title = {Instructions to attend to an observed action increase imitation in autistic adults},
	volume = {24},
	issn = {1362-3613},
	url = {https://doi.org/10.1177/1362361319882810},
	doi = {10.1177/1362361319882810},
	abstract = {This study investigated whether reduced visual attention to an observed action might account for altered imitation in autistic adults. A total of 22 autistic and 22 non-autistic adults observed and then imitated videos of a hand producing sequences of movements that differed in vertical elevation while their hand and eye movements were recorded. Participants first performed a block of imitation trials with general instructions to imitate the action. They then performed a second block with explicit instructions to attend closely to the characteristics of the movement. Imitation was quantified according to how much participants modulated their movement between the different heights of the observed movements. In the general instruction condition, the autistic group modulated their movements significantly less compared to the non-autistic group. However, following instructions to attend to the movement, the autistic group showed equivalent imitation modulation to the non-autistic group. Eye movement recording showed that the autistic group spent significantly less time looking at the hand movement for both instruction conditions. These findings show that visual attention contributes to altered voluntary imitation in autistic individuals and have implications for therapies involving imitation as well as for autistic people’s ability to understand the actions of others.},
	language = {en},
	number = {3},
	urldate = {2024-06-19},
	journal = {Autism},
	author = {Gowen, Emma and Vabalas, Andrius and Casson, Alexander J and Poliakoff, Ellen},
	month = apr,
	year = {2020},
	note = {Publisher: SAGE Publications Ltd},
	pages = {730--743},
	file = {2020_Gowen et al._Instructions to attend to an observed action increase imitation in autistic adults.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2020_Gowen et al._Instructions to attend to an observed action increase imitation in autistic adults.pdf:application/pdf},
}

@article{washington2023,
	title = {A {Review} of and {Roadmap} for {Data} {Science} and {Machine} {Learning} for the {Neuropsychiatric} {Phenotype} of {Autism}},
	volume = {6},
	issn = {2574-3414},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-biodatasci-020722-125454},
	doi = {10.1146/annurev-biodatasci-020722-125454},
	abstract = {Autism spectrum disorder (autism) is a neurodevelopmental delay that affects at least 1 in 44 children. Like many neurological disorder phenotypes, the diagnostic features are observable, can be tracked over time, and can be managed or even eliminated through proper therapy and treatments. However, there are major bottlenecks in the diagnostic, therapeutic, and longitudinal tracking pipelines for autism and related neurodevelopmental delays, creating an opportunity for novel data science solutions to augment and transform existing workflows and provide increased access to services for affected families. Several efforts previously conducted by a multitude of research labs have spawned great progress toward improved digital diagnostics and digital therapies for children with autism. We review the literature on digital health methods for autism behavior quantification and beneficial therapies using data science. We describe both case–control studies and classification systems for digital phenotyping. We then discuss digital diagnostics and therapeutics that integrate machine learning models of autism-related behaviors, including the factors that must be addressed for translational use. Finally, we describe ongoing challenges and potential opportunities for the field of autism data science. Given the heterogeneous nature of autism and the complexities of the relevant behaviors, this review contains insights that are relevant to neurological behavior analysis and digital psychiatry more broadly.},
	language = {en},
	number = {Volume 6, 2023},
	urldate = {2024-06-19},
	journal = {Annual Review of Biomedical Data Science},
	author = {Washington, Peter and Wall, Dennis P.},
	month = aug,
	year = {2023},
	note = {Publisher: Annual Reviews},
	pages = {211--228},
	file = {2023_Washington and Wall_A Review of and Roadmap for Data Science and Machine Learning for the Neuropsychiatric Phenotype of.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2023_Washington and Wall_A Review of and Roadmap for Data Science and Machine Learning for the Neuropsychiatric Phenotype of.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\CK3EQN6F\\annurev-biodatasci-020722-125454.html:text/html},
}

@article{rudovic2018,
	title = {Personalized machine learning for robot perception of affect and engagement in autism therapy},
	volume = {3},
	issn = {2470-9476},
	doi = {10.1126/scirobotics.aao6760},
	abstract = {Robots have the potential to facilitate future therapies for children on the autism spectrum. However, existing robots are limited in their ability to automatically perceive and respond to human affect, which is necessary for establishing and maintaining engaging interactions. Their inference challenge is made even harder by the fact that many individuals with autism have atypical and unusually diverse styles of expressing their affective-cognitive states. To tackle the heterogeneity in children with autism, we used the latest advances in deep learning to formulate a personalized machine learning (ML) framework for automatic perception of the children's affective states and engagement during robot-assisted autism therapy. Instead of using the traditional one-size-fits-all ML approach, we personalized our framework to each child using their contextual information (demographics and behavioral assessment scores) and individual characteristics. We evaluated this framework on a multimodal (audio, video, and autonomic physiology) data set of 35 children (ages 3 to 13) with autism, from two cultures (Asia and Europe), and achieved an average agreement (intraclass correlation) of {\textasciitilde}60\% with human experts in the estimation of affect and engagement, also outperforming nonpersonalized ML solutions. These results demonstrate the feasibility of robot perception of affect and engagement in children with autism and have implications for the design of future autism therapies.},
	language = {eng},
	number = {19},
	journal = {Science Robotics},
	author = {Rudovic, Ognjen and Lee, Jaeryoung and Dai, Miles and Schuller, Björn and Picard, Rosalind W.},
	month = jun,
	year = {2018},
	pmid = {33141688},
	pages = {eaao6760},
	file = {2018_Rudovic et al._Personalized machine learning for robot perception of affect and engagement in autism therapy.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2018_Rudovic et al._Personalized machine learning for robot perception of affect and engagement in autism therapy.pdf:application/pdf},
}

@article{li2024a,
	title = {Teachers and educators' experiences and perceptions of artificial-powered interventions for autism groups},
	volume = {12},
	issn = {2050-7283},
	doi = {10.1186/s40359-024-01664-2},
	abstract = {BACKGROUND: Artificial intelligence-powered interventions have emerged as promising tools to support autistic individuals. However, more research must examine how teachers and educators perceive and experience these AI systems when implemented.
OBJECTIVES: The first objective was to investigate informants' perceptions and experiences of AI-empowered interventions for children with autism. Mainly, it explores the informants' perceived benefits and challenges of using AI-empowered interventions and their recommendations for avoiding the perceived challenges.
METHODOLOGY: A qualitative phenomenological approach was used. Twenty educators and parents with experience implementing AI interventions for autism were recruited through purposive sampling. Semi-structured and focus group interviews conducted, transcribed verbatim, and analyzed using thematic analysis.
FINDINGS: The analysis identified four major themes: perceived benefits of AI interventions, implementation challenges, needed support, and recommendations for improvement. Benefits included increased engagement and personalized learning. Challenges included technology issues, training needs, and data privacy concerns.
CONCLUSIONS: AI-powered interventions show potential to improve autism support, but significant challenges must be addressed to ensure effective implementation from an educator's perspective. The benefits of personalized learning and student engagement demonstrate the potential value of these technologies. However, with adequate training, technical support, and measures to ensure data privacy, many educators will likely find integrating AI systems into their daily practices easier.
IMPLICATIONS: To realize the full benefits of AI for autism, developers must work closely with educators to understand their needs, optimize implementation, and build trust through transparent privacy policies and procedures. With proper support, AI interventions can transform how autistic individuals are educated by tailoring instruction to each student's unique profile and needs.},
	language = {eng},
	number = {1},
	journal = {BMC psychology},
	author = {Li, Guang and Zarei, Mohammad Amin and Alibakhshi, Goudarz and Labbafi, Akram},
	month = apr,
	year = {2024},
	pmid = {38605422},
	pmcid = {PMC11010416},
	keywords = {Learning, Autistic Disorder, Child, Humans, AI-empowered interventions, Artificial intelligence, Artificial Intelligence, Children with autism, Educational Personnel, Educators, Parents, Students},
	pages = {199},
	file = {2024_Li et al._Teachers and educators' experiences and perceptions of artificial-powered interventions for autism g.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2024_Li et al._Teachers and educators' experiences and perceptions of artificial-powered interventions for autism g.pdf:application/pdf},
}

@article{bargiela2016,
	title = {The {Experiences} of {Late}-diagnosed {Women} with {Autism} {Spectrum} {Conditions}: {An} {Investigation} of the {Female} {Autism} {Phenotype}},
	volume = {46},
	issn = {1573-3432},
	shorttitle = {The {Experiences} of {Late}-diagnosed {Women} with {Autism} {Spectrum} {Conditions}},
	url = {https://doi.org/10.1007/s10803-016-2872-8},
	doi = {10.1007/s10803-016-2872-8},
	abstract = {We used Framework Analysis to investigate the female autism phenotype and its impact upon the under-recognition of autism spectrum conditions (ASC) in girls and women. Fourteen women with ASC (aged 22–30 years) diagnosed in late adolescence or adulthood gave in-depth accounts of: ‘pretending to be normal’; of how their gender led various professionals to miss their ASC; and of conflicts between ASC and a traditional feminine identity. Experiences of sexual abuse were widespread in this sample, partially reflecting specific vulnerabilities from being a female with undiagnosed ASC. Training would improve teachers’ and clinicians’ recognition of ASC in females, so that timely identification can mitigate risks and promote wellbeing of girls and women on the autism spectrum.},
	language = {en},
	number = {10},
	urldate = {2024-06-19},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Bargiela, Sarah and Steward, Robyn and Mandy, William},
	month = oct,
	year = {2016},
	keywords = {Autism spectrum conditions (ASC), Autism spectrum disorder (ASD), Diagnosis, Female autism phenotype},
	pages = {3281--3294},
	file = {2016_Bargiela et al._The Experiences of Late-diagnosed Women with Autism Spectrum Conditions An Investigation of the Fem.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2016_Bargiela et al._The Experiences of Late-diagnosed Women with Autism Spectrum Conditions An Investigation of the Fem.pdf:application/pdf},
}

@article{edwards2024,
	title = {Research {Review}: {A} systematic review and meta-analysis of sex differences in narrow constructs of restricted and repetitive behaviours and interests in autistic children, adolescents, and adults},
	volume = {65},
	issn = {1469-7610},
	shorttitle = {Research {Review}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jcpp.13855},
	doi = {10.1111/jcpp.13855},
	abstract = {Background Evidence that autism often manifests differently between males and females is growing, particularly in terms of social interaction and communication, but it is unclear if there are sex differences in restricted and repetitive behaviours and interests (RRBIs) when rigorously focusing on the narrow construct level (i.e., stereotyped behaviour, restricted interests, insistence on sameness, and/or sensory experiences). Methods We conducted a systematic review and four random effects meta-analyses investigating sex differences in narrow construct measures of RRBIs in autistic children, adolescents, and adults (Prospero registration ID: CRD42021254221). Study quality was appraised using the Newcastle-Ottawa Quality Assessment Scale. Results Forty-six studies were narratively synthesised and 25 of these were included in four random effects meta-analyses. Results found that autistic males had significantly higher levels of stereotyped behaviours (SMD = 0.21, 95\% confidence interval (CI) [0.09, 0.33], p {\textless} .001) and restricted interests (SMD = 0.18, 95\% CI [0.07, 0.29], p {\textless} .001) compared to autistic females. In contrast, there were no significant sex differences for sensory experiences (SMD = −0.09, 95\% CI [−0.27, 0.09], p = .32) and insistence on sameness (SMD = 0.01, 95\% CI [−0.03, 0.05], p = .68). The findings from the narrative synthesis were generally consistent with those from the meta-analyses and also found qualitative sex differences in the way RRBIs manifest. Conclusions Our findings show significant differences in narrowly defined RRBIs in males and females. Practitioners need to be aware of such differences, which could be contributing to the under-recognition of autism in females and may not be captured by current diagnostic instruments.},
	language = {en},
	number = {1},
	urldate = {2024-06-19},
	journal = {Journal of Child Psychology and Psychiatry},
	author = {Edwards, Hannah and Wright, Sarah and Sargeant, Cora and Cortese, Samuele and Wood-Downie, Henry},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jcpp.13855},
	keywords = {Autism, gender differences, meta-analysis, restricted and repetitive behaviours and interests, sex differences, systematic review},
	pages = {4--17},
	file = {2024_Edwards et al._Research Review A systematic review and meta-analysis of sex differences in narrow constructs of re.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2024_Edwards et al._Research Review A systematic review and meta-analysis of sex differences in narrow constructs of re.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\HU3KCKMS\\jcpp.html:text/html},
}

@article{sainsbury2023,
	title = {Age of {Diagnosis} for {Co}-occurring {Autism} and {Attention} {Deficit} {Hyperactivity} {Disorder} {During} {Childhood} and {Adolescence}: a {Systematic} {Review}},
	volume = {10},
	issn = {2195-7185},
	shorttitle = {Age of {Diagnosis} for {Co}-occurring {Autism} and {Attention} {Deficit} {Hyperactivity} {Disorder} {During} {Childhood} and {Adolescence}},
	url = {https://doi.org/10.1007/s40489-022-00309-7},
	doi = {10.1007/s40489-022-00309-7},
	abstract = {Early identification and intervention are recognised as important elements of the clinical pathway for autism spectrum disorder (ASD). Children with ASD and attention deficit hyperactivity disorder (ADHD) may be diagnosed at a different age than children who only have one of these diagnoses. This systematic review aimed to identify the age at which children were diagnosed with both ASD and ADHD. Of the 9552 articles screened, 12 were included in the review. The findings suggest that ASD is typically diagnosed later when ADHD is present, and ADHD is typically diagnosed earlier when ASD is present. Further research is needed to understand the factors impacting a delayed ASD diagnosis and an earlier ADHD diagnosis when the two conditions co-occur.},
	language = {en},
	number = {3},
	urldate = {2024-06-20},
	journal = {Review Journal of Autism and Developmental Disorders},
	author = {Sainsbury, Willow J. and Carrasco, Kelly and Whitehouse, Andrew J. O. and McNeil, Lauren and Waddington, Hannah},
	month = sep,
	year = {2023},
	keywords = {Autism, Diagnosis, Age of diagnosis, Attention deficit hyperactivity disorder, Co-occurring conditions},
	pages = {563--575},
	file = {2023_Sainsbury et al._Age of Diagnosis for Co-occurring Autism and Attention Deficit Hyperactivity Disorder During Childho.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2023_Sainsbury et al._Age of Diagnosis for Co-occurring Autism and Attention Deficit Hyperactivity Disorder During Childho.pdf:application/pdf},
}

@article{knott2024,
	title = {Age at diagnosis and diagnostic delay across attention-deficit hyperactivity and autism spectrums},
	volume = {58},
	issn = {0004-8674},
	url = {https://doi.org/10.1177/00048674231206997},
	doi = {10.1177/00048674231206997},
	abstract = {Background: Despite the known benefits of accurate and timely diagnosis for children with attention-deficit hyperactivity disorder and autism spectrum disorders (autism), for some children this goal is not always achieved. Existing research has explored diagnostic delay for autism and attention-deficit hyperactivity disorder only, and when attention-deficit hyperactivity disorder and autism co-occur, autism has been the focus. No study has directly compared age at diagnosis and diagnostic delay for males and females across attention-deficit hyperactivity disorder, autism and specifically, attention-deficit hyperactivity disorder + autism.
Methods: Australian caregivers (N = 677) of children with attention-deficit hyperactivity disorder, autism or attention-deficit hyperactivity disorder + autism were recruited via social media (n = 594) and the Monash Autism and ADHD Genetics and Neurodevelopment Project (n = 83). Caregivers reported on their child’s diagnostic process. Diagnostic delay was the mean difference between general initial developmental concerns and the child’s attention-deficit hyperactivity disorder and autism diagnosis.
Results: Children with autism were significantly younger at autism diagnosis than the attention-deficit hyperactivity disorder + autism group (ηp2 = 0.06), whereas children with attention-deficit hyperactivity disorder were significantly older at attention-deficit hyperactivity disorder diagnosis than the attention-deficit hyperactivity disorder + autism group (ηp2 = 0.01). Delay to attention-deficit hyperactivity disorder and autism diagnosis was significantly longer in the attention-deficit hyperactivity disorder + autism group compared to attention-deficit hyperactivity disorder (ηp2 = 0.02) and autism (η2 = 0.04) only. Delay to autism diagnosis for females with autism (η2 = 0.06) and attention-deficit hyperactivity disorder + autism (η2 = 0.04) was longer compared to males.
Conclusions: Having attention-deficit hyperactivity disorder + autism and being female were associated with longer delays to diagnosis. The reasons for these delays and possible adverse effects on outcomes require further study.},
	language = {en},
	number = {2},
	urldate = {2024-06-20},
	journal = {Australian \& New Zealand Journal of Psychiatry},
	author = {Knott, Rachael and Mellahn, Olivia J and Tiego, Jeggan and Kallady, Kathryn and Brown, Louise E and Coghill, David and Williams, Katrina and Bellgrove, Mark A and Johnson, Beth P},
	month = feb,
	year = {2024},
	note = {Publisher: SAGE Publications Ltd},
	pages = {142--151},
	file = {2024_Knott et al._Age at diagnosis and diagnostic delay across attention-deficit hyperactivity and autism spectrums.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2024_Knott et al._Age at diagnosis and diagnostic delay across attention-deficit hyperactivity and autism spectrums.pdf:application/pdf},
}

@article{sarwat2024,
	title = {Post-stroke hand gesture recognition via one-shot transfer learning using prototypical networks},
	volume = {21},
	issn = {1743-0003},
	url = {https://doi.org/10.1186/s12984-024-01398-7},
	doi = {10.1186/s12984-024-01398-7},
	abstract = {In-home rehabilitation systems are a promising, potential alternative to conventional therapy for stroke survivors. Unfortunately, physiological differences between participants and sensor displacement in wearable sensors pose a significant challenge to classifier performance, particularly for people with stroke who may encounter difficulties repeatedly performing trials. This makes it challenging to create reliable in-home rehabilitation systems that can accurately classify gestures.},
	language = {en},
	number = {1},
	urldate = {2024-06-25},
	journal = {Journal of NeuroEngineering and Rehabilitation},
	author = {Sarwat, Hussein and Alkhashab, Amr and Song, Xinyu and Jiang, Shuo and Jia, Jie and Shull, Peter B.},
	month = jun,
	year = {2024},
	keywords = {Machine learning, Few-shot learning, Hand gesture recognition, Post-stroke, Prototypical networks},
	pages = {100},
	file = {2024_Sarwat et al._Post-stroke hand gesture recognition via one-shot transfer learning using prototypical networks.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Body-Tracking\\2024_Sarwat et al._Post-stroke hand gesture recognition via one-shot transfer learning using prototypical networks.pdf:application/pdf},
}

@article{akdere2021a,
	title = {Evaluation and assessment of virtual reality-based simulated training: exploring the human–technology frontier},
	volume = {46},
	issn = {2046-9012},
	shorttitle = {Evaluation and assessment of virtual reality-based simulated training},
	url = {https://doi.org/10.1108/EJTD-12-2020-0178},
	doi = {10.1108/EJTD-12-2020-0178},
	abstract = {Purpose As new technologies such as immersive and augmented platforms emerge, training approaches are also transforming. The virtual reality (VR) platform provides a completely immersive learning experience for simulated training. Despite increased prevalence of these technologies, the extent literature is lagging behind in terms of evaluating and assessing such innovative training models. The purpose of this paper is to address this gap through exploring the traditional approaches of quantitative, qualitative and mixed methods as well as the cutting-edge biometric approach in the evaluation and assessment of the VR-based simulated training and discuss implications for simulated training based on immersive technologies. Design/methodology/approach Evaluation and assessment is one of the most critical components of training and development. Inaccurate or ineffective approaches to evaluate and assess training programs not only risk the successful attainment of training goals and outcomes, but they also harm trainees by misleading them about their training performances and experiences. This paper uses a review of existing literature to explore effective approaches for the evaluation and assessment of VR-based simulated training and conceptually discusses new capacities in capturing involuntary trainee reaction toward stimuli, in addition to traditional evaluation and assessment methods. Findings Immersive VR-based simulated training is uncharted territory for trainers and human resource development professionals. The findings indicate that existing approaches are still viable options for the evaluation and assessment of this new training technology. However, biometrics presents new frontiers in this arena through its capacity for obtaining trainee emotional responses to stimuli during training, as well as providing a venue free of personal bias and external influences in determining trainee perceptions. Originality/value This paper addresses an important gap in the field of training and development by studying the affordances of the latest biometric technology for evaluation and assessment in VR-based simulated training. The existing literature is very limited in its focus on immersive training technologies such as VR in general and evaluation and assessment in particular. The paper presents new insights to both researchers and practitioners in the field of training and development.},
	number = {5/6},
	urldate = {2024-07-22},
	journal = {European Journal of Training and Development},
	author = {Akdere, Mesut and Jiang, Yeling and Lobo, Flavio Destri},
	month = jan,
	year = {2021},
	note = {Publisher: Emerald Publishing Limited},
	keywords = {Biometric data, Data triangulation, Mixed research method, Training assessment, Training evaluation, Virtual reality simulation},
	pages = {434--449},
	file = {2021_Akdere et al._Evaluation and assessment of virtual reality-based simulated training exploring the human–technolog.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2021_Akdere et al._Evaluation and assessment of virtual reality-based simulated training exploring the human–technolog.pdf:application/pdf},
}

@article{mak2023,
	title = {A systematic review: the application of virtual reality on the skill-specific performance in people with {ASD}},
	volume = {31},
	issn = {1049-4820},
	shorttitle = {A systematic review},
	url = {https://doi.org/10.1080/10494820.2020.1811733},
	doi = {10.1080/10494820.2020.1811733},
	abstract = {Given the prevalence of Autism Spectrum Disorder (ASD) and the demand for treatment, there is a continuous seeking and uncertainty of effective interventions for people with ASD. As technology continues to advance, the application of Virtual Reality is emerging in clinical settings. This systematic review summarised findings to evaluate the application of virtual reality (VR) on the skill-specific performance in people with ASD. The purpose is to determine (1) if VR is an effective treatment for people with ASD in skill-specific performance and (2) can Occupational Therapists employ VR in their practice. Eight databases were systematically searched for peer-reviewed articles that were published from January 2012 to February 2018. Eight articles met the inclusion criteria. The measurements of specific skills were categorised into three main domains: job interviewing, driving, and other ADLs. A diverse range of outcome measures were utilised and provided various results. Despite the consistent positive results reported in the studies, the current evidence base lacks justification of sample sizes, reliability and validity of the findings. Although VR shows potential as an effective intervention, limitations and bias of studies should be considered. Results of studies must be interpreted with caution if Occupational Therapists are interested in employing VR in their practice.},
	number = {2},
	urldate = {2024-07-22},
	journal = {Interactive Learning Environments},
	author = {Mak, Grace and Zhao, Lea},
	month = feb,
	year = {2023},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/10494820.2020.1811733},
	keywords = {virtual reality, Autism Spectrum Disorder, Occupational therapy},
	pages = {804--817},
	file = {2023_Mak and Zhao_A systematic review the application of virtual reality on the skill-specific performance in people.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2023_Mak and Zhao_A systematic review the application of virtual reality on the skill-specific performance in people.pdf:application/pdf},
}

@incollection{tuker2024,
	address = {Cham},
	title = {Training {Spatial} {Skills} with {Virtual} {Reality} and {Augmented} {Reality}},
	isbn = {978-3-031-23161-2},
	url = {https://doi.org/10.1007/978-3-031-23161-2_173},
	language = {en},
	urldate = {2024-07-22},
	booktitle = {Encyclopedia of {Computer} {Graphics} and {Games}},
	publisher = {Springer International Publishing},
	author = {Tuker, Cetin},
	editor = {Lee, Newton},
	year = {2024},
	doi = {10.1007/978-3-031-23161-2_173},
	pages = {1904--1912},
}

@article{parra2021,
	title = {Combining {Virtual} {Reality} and {Organizational} {Neuroscience} for {Leadership} {Assessment}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/13/5956},
	doi = {10.3390/app11135956},
	abstract = {In this article, we introduce three-dimensional Serious Games (3DSGs) under an evidence-centered design (ECD) framework and use an organizational neuroscience-based eye-tracking measure to capture implicit behavioral signals associated with leadership skills. While ECD is a well-established framework used in the design and development of assessments, it has rarely been utilized in organizational research. The study proposes a novel 3DSG combined with organizational neuroscience methods as a promising tool to assess and recognize leadership-related behavioral patterns that manifest during complex and realistic social situations. We offer a research protocol for assessing task- and relationship-oriented leadership skills that uses ECD, eye-tracking measures, and machine learning. Seamlessly embedding biological measures into 3DSGs enables objective assessment methods that are based on machine learning techniques to achieve high ecological validity. We conclude by describing a future research agenda for the combined use of 3DSGs and organizational neuroscience methods for leadership and human resources.},
	language = {en},
	number = {13},
	urldate = {2024-07-22},
	journal = {Applied Sciences},
	author = {Parra, Elena and Chicchi Giglioli, Irene Alice and Philip, Jestine and Carrasco-Ribelles, Lucia Amalia and Marín-Morales, Javier and Alcañiz Raya, Mariano},
	month = jan,
	year = {2021},
	note = {Number: 13
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {visual attention, machine learning, assessment method, decision-making behaviors, evidence-centered design, leadership style, serious game},
	pages = {5956},
	file = {2021_Parra et al._Combining Virtual Reality and Organizational Neuroscience for Leadership Assessment.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2021_Parra et al._Combining Virtual Reality and Organizational Neuroscience for Leadership Assessment.pdf:application/pdf},
}

@article{parra2022,
	title = {Combining {Virtual} {Reality} and {Machine} {Learning} for {Leadership} {Styles} {Recognition}},
	volume = {13},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2022.864266/full},
	doi = {10.3389/fpsyg.2022.864266},
	abstract = {{\textless}p{\textgreater}The aim of this study was to evaluate the viability of a new selection procedure based on machine learning (ML) and virtual reality (VR). Specifically, decision-making behaviours and eye-gaze patterns were used to classify individuals based on their leadership styles while immersed in virtual environments that represented social workplace situations. The virtual environments were designed using an evidence-centred design approach. Interaction and gaze patterns were recorded in 83 subjects, who were classified as having either high or low leadership style, which was assessed using the Multifactor leadership questionnaire. A ML model that combined behaviour outputs and eye-gaze patterns was developed to predict subjects’ leadership styles (high vs low). The results indicated that the different styles could be differentiated by eye-gaze patterns and behaviours carried out during immersive VR. Eye-tracking measures contributed more significantly to this differentiation than behavioural metrics. Although the results should be taken with caution as the small sample does not allow generalization of the data, this study illustrates the potential for a future research roadmap that combines VR, implicit measures, and ML for personnel selection.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-07-22},
	journal = {Frontiers in Psychology},
	author = {Parra, Elena and García Delgado, Aitana and Carrasco-Ribelles, Lucía Amalia and Chicchi Giglioli, Irene Alice and Marín-Morales, Javier and Giglio, Cristina and Alcañiz Raya, Mariano},
	month = may,
	year = {2022},
	note = {Publisher: Frontiers},
	keywords = {virtual reality, Eye-tracking, machine learning, Leadership, leadership style recognition},
	file = {2022_Parra et al._Combining Virtual Reality and Machine Learning for Leadership Styles Recognition.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2022_Parra et al._Combining Virtual Reality and Machine Learning for Leadership Styles Recognition.pdf:application/pdf},
}

@article{chicchigiglioli2017,
	title = {A {Novel} {Integrating} {Virtual} {Reality} {Approach} for the {Assessment} of the {Attachment} {Behavioral} {System}},
	volume = {8},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2017.00959/full},
	doi = {10.3389/fpsyg.2017.00959},
	abstract = {{\textless}p{\textgreater}Virtual reality (VR) technology represents a novel and powerful tool for behavioral research in psychological assessment. VR provides simulated experiences able to create the sensation of undergoing real situations. Users become active participants in the virtual environment seeing, hearing, feeling, and actuating as if they were in the real world. Currently, the most psychological VR applications concern the treatment of various mental disorders but not the assessment, that it is mainly based on paper and pencil tests. The observation of behaviors is costly, labor-intensive, and it is hard to create social situations in laboratory settings, even if the observation of actual behaviors could be particularly informative. In this framework, social stressful experiences can activate various behaviors of attachment for a significant person that can help to control and soothe them to promote individual’s well-being. Social support seeking, physical proximity, and positive and negative behaviors represent the main attachment behaviors that people can carry out during experiences of distress. We proposed VR as a novel integrating approach to measure real attachment behaviors. The first studies on attachment behavioral system by VR showed the potentiality of this approach. To improve the assessment during the VR experience, we proposed virtual stealth assessment (VSA) as a new method. VSA could represent a valid and novel technique to measure various psychological attributes in real-time during the virtual experience. The possible use of this method in psychology could be to generate a more complete, exhaustive, and accurate individual’s psychological evaluation.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-07-22},
	journal = {Frontiers in Psychology},
	author = {Chicchi Giglioli, Irene Alice and Pravettoni, Gabriella and Sutil Martín, Dolores Lucia and Parra, Elena and Raya, Mariano A.},
	month = jun,
	year = {2017},
	note = {Publisher: Frontiers},
	keywords = {virtual reality, Attachment, Ecological Validity, evidence-centred design, presence, stealth assessment},
	file = {2017_Chicchi Giglioli et al._A Novel Integrating Virtual Reality Approach for the Assessment of the Attachment Behavioral System.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2017_Chicchi Giglioli et al._A Novel Integrating Virtual Reality Approach for the Assessment of the Attachment Behavioral System.pdf:application/pdf},
}

@article{daling2024,
	title = {Effects of {Augmented} {Reality}-, {Virtual} {Reality}-, and {Mixed} {Reality}–{Based} {Training} on {Objective} {Performance} {Measures} and {Subjective} {Evaluations} in {Manual} {Assembly} {Tasks}: {A} {Scoping} {Review}},
	volume = {66},
	issn = {0018-7208},
	shorttitle = {Effects of {Augmented} {Reality}-, {Virtual} {Reality}-, and {Mixed} {Reality}–{Based} {Training} on {Objective} {Performance} {Measures} and {Subjective} {Evaluations} in {Manual} {Assembly} {Tasks}},
	url = {https://doi.org/10.1177/00187208221105135},
	doi = {10.1177/00187208221105135},
	abstract = {ObjectiveThe present scoping review aims to transform the diverse field of research on the effects of mixed reality-based training on performance in manual assembly tasks into comprehensive statements about industrial needs for and effects of mixed reality-based training.BackgroundTechnologies such as augmented and virtual reality, referred to as mixed reality, are seen as promising media for training manual assembly tasks. Nevertheless, current literature shows partly contradictory results, which is due to the diversity of the hardware used, manual assembly tasks as well as methodological approaches to investigate the effects of mixed reality-based training.MethodFollowing the methodological approach of a scoping review, we selected 24 articles according to predefined criteria and analyzed them concerning five key aspects: (1) the needs in the industry for mixed reality-based training, (2) the actual use and classification of mixed reality technologies, (3) defined measures for evaluating the outcomes of mixed reality-based training, (4) findings on objectively measured performance and subjective evaluations, as well as (5) identified research gaps.ResultsRegarding the improvement of performance and effectiveness through mixed reality-based training, promising results were found particularly for augmented reality-based training, while virtual reality-based training is mostly—but not consistently—as good as traditional training.ApplicationMixed reality-based training is still not consistently better, but mostly at least as good as traditional training. However, depending on the use case and technology used, the training outcomes in terms of assembly performance and subjective evaluations show promising results of mixed reality-based training.},
	language = {en},
	number = {2},
	urldate = {2024-07-22},
	journal = {Human Factors},
	author = {Daling, Lea M. and Schlittmeier, Sabine J.},
	month = feb,
	year = {2024},
	note = {Publisher: SAGE Publications Inc},
	pages = {589--626},
	file = {2024_Daling and Schlittmeier_Effects of Augmented Reality-, Virtual Reality-, and Mixed Reality–Based Training on Objective Perfo.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2024_Daling and Schlittmeier_Effects of Augmented Reality-, Virtual Reality-, and Mixed Reality–Based Training on Objective Perfo.pdf:application/pdf},
}

@article{harris2023,
	title = {Assessing {Expertise} {Using} {Eye} {Tracking} in a {Virtual} {Reality} {Flight} {Simulation}},
	volume = {33},
	issn = {2472-1840},
	url = {https://doi.org/10.1080/24721840.2023.2195428},
	doi = {10.1080/24721840.2023.2195428},
	abstract = {The aim of this work was to examine the fidelity and validity of an aviation simulation using eye tracking. Commercial head-mounted virtual reality (VR) systems offer a convenient and cost-effective alternative to existing aviation simulation (e.g., for refresher exercises). We performed pre-implementation testing of a novel aviation simulation, designed for head-mounted VR, to determine its fidelity and validity as a training device. Eighteen airline pilots, with varying levels of flight experience, completed a sequence of training ‘flows.’ Self-reported measures of presence and workload and users’ perceptions of fidelity were taken. Pilots’ eye movements and performance were recorded to determine whether more experienced pilots showed distinct performance and eye gaze profiles in the simulation, as they would in the real-world. Real-world expertise correlated with eye gaze patterns characterized by fewer, but longer, fixations and a scan path that was more structured and less random. Multidimensional scaling analyses also indicated differential clustering of strategies in more versus less experienced pilots. Subjective ratings of performance, however, showed little relationship with real-world expertise or eye movements. We adopted an evidence-based approach to assessing the fidelity and validity of a VR flight training tool. Pilot reports indicated the simulation was realistic and potentially useful for training, while direct measurement of eye movements was useful for establishing construct validity and psychological fidelity of the simulation.},
	number = {3},
	urldate = {2024-07-22},
	journal = {The International Journal of Aerospace Psychology},
	author = {Harris, D. J. and Arthur, T. and de Burgh, T. and Duxbury, M. and Lockett-Kirk, R. and McBarnett, W. and Vine, S. J.},
	month = jul,
	year = {2023},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/24721840.2023.2195428},
	pages = {153--173},
	file = {2023_Harris et al._Assessing Expertise Using Eye Tracking in a Virtual Reality Flight Simulation.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2023_Harris et al._Assessing Expertise Using Eye Tracking in a Virtual Reality Flight Simulation.pdf:application/pdf},
}

@article{2023a,
	title = {Artificial intelligence in virtual reality simulation for interprofessional communication training: {Mixed} method study},
	volume = {122},
	issn = {0260-6917},
	shorttitle = {Artificial intelligence in virtual reality simulation for interprofessional communication training},
	url = {https://www.sciencedirect.com/science/article/pii/S0260691723000126},
	doi = {10.1016/j.nedt.2023.105718},
	abstract = {Virtual reality simulations are shown to be an effective approach for interprofessional nurse-physician communication training. However, its scalabili…},
	language = {en-US},
	urldate = {2024-07-22},
	journal = {Nurse Education Today},
	month = mar,
	year = {2023},
	note = {Publisher: Churchill Livingstone},
	pages = {105718},
	file = {2023_Artificial intelligence in virtual reality simulation for interprofessional communication training.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2023_Artificial intelligence in virtual reality simulation for interprofessional communication training.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\MSFRABTI\\S0260691723000126.html:text/html},
}

@phdthesis{degiorgi2022,
	type = {laurea},
	title = {Virtual reality body swapping to improve the training of soft skills and self-assessment},
	copyright = {cc\_by\_nc\_nd},
	url = {https://webthesis.biblio.polito.it/25575/},
	abstract = {Immersive Virtual Reality applications have been proven to be effective in training soft skills and in managing anxiety. Many studies on this technology have focused on public speaking and on the treatment of phobias. Apparently, there is an interesting domain represented by job interviews that has not been fully considered yet. In fact, most of the work done in this context leveraged desktop-based Virtual Reality, a configuration which does not enable to take full advantage of the potential of immersive technologies. Training before having a job interview, however, could be a precious asset for students who are about to finish their studies and need to apply for job positions, especially if they have never experienced one. This training could indeed be done with traditional methods; by means of Virtual Reality, though, an individual can see him or herself from the outside and better assess his or her performance. Compared with, e.g., practicing while watching in the mirror, there is so much more that can be done in immersive Virtual Reality. This technology offers the possibility to observe the whole scene where the job interview takes place, both from an external viewpoint as an observer without a direct involvement, as well as from another person’s perspective as if his or her virtual body was that of the individual being interviewed. Based on the above premises, this thesis work explores the two configurations above, aiming at disclosing which one could be more useful to self-assess an individual performance while answering questions in a job interview. Rather than actually developing a tool to support practicing in this scenario, the ultimate goal is to identify how such a tool could be most effective, i.e., whether observing the interview from an “objective” or “neutral” viewpoint or from the “subjective” viewpoint of the interviewer whose job is to carry out the evaluation.},
	language = {it},
	urldate = {2024-07-22},
	school = {Politecnico di Torino},
	author = {De Giorgi, Chiara},
	month = dec,
	year = {2022},
	file = {2022_De Giorgi_Virtual reality body swapping to improve the training of soft skills and self-assessment.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2022_De Giorgi_Virtual reality body swapping to improve the training of soft skills and self-assessment.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\7LQZAKPS\\25575.html:text/html},
}

@article{ruizmorales2017,
	title = {Evaluación de competencias genéricas en el ámbito universitario a través de entornos virtuales: {Una} revisión narrativa},
	volume = {23},
	issn = {1134-4032},
	shorttitle = {Evaluación de competencias genéricas en el ámbito universitario a través de entornos virtuales},
	url = {https://revistaseug.ugr.es/index.php/RELIEVE/article/view/17302},
	doi = {10.7203/relieve.23.1.7183},
	abstract = {The paper presents a narrative on the state of the question about the teaching and assessment of generic soft skills through Virtual Environments (VEs) in universities, based on consultation of scientific journals in electronic and printed format published between 2000 and 2014, as well as research projects focused on the development of generic skills through VEs. The paper summarises the theoretical and empirical contributions as a way of providing a greater insight into a line of research that began barely a decade ago. Soft skills related to student training, when combined with specific skills, enable better performance in personal, academic, social and organisational settings. This premise implies the need to consider the development of new methodologies for teaching, learning and assessment of soft skills. Results include the value of soft skills in training university students, as well as the development of innovative projects focused on the provision of teaching and assessment procedures that are feasible and effective for the achievement of soft skills assessment in VEs.},
	language = {en},
	number = {1},
	urldate = {2024-07-22},
	journal = {RELIEVE - Revista Electrónica de Investigación y Evaluación Educativa},
	author = {Ruiz Morales, Yovanni Alexander and Biencinto López, Chantal and Garcìa Garcìa, Mercedes and Carpintero, Elvira},
	month = jan,
	year = {2017},
	file = {2017_Ruiz Morales et al._Evaluación de competencias genéricas en el ámbito universitario a través de entornos virtuales Una.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2017_Ruiz Morales et al._Evaluación de competencias genéricas en el ámbito universitario a través de entornos virtuales Una.pdf:application/pdf},
}

@article{polakova2023,
	title = {Soft skills and their importance in the labour market under the conditions of {Industry} 5.0},
	volume = {9},
	issn = {2405-8440},
	url = {https://www.cell.com/heliyon/abstract/S2405-8440(23)05878-4},
	doi = {10.1016/j.heliyon.2023.e18670},
	language = {English},
	number = {8},
	urldate = {2024-07-22},
	journal = {Heliyon},
	author = {Poláková, Michaela and Suleimanová, Juliet Horváthová and Madzík, Peter and Copuš, Lukáš and Molnárová, Ivana and Polednová, Jana},
	month = aug,
	year = {2023},
	pmid = {37520995},
	note = {Publisher: Elsevier},
	keywords = {Industry 4.0, Digital skills, Digitalisation, Industry 5.0, Soft skills},
	file = {2023_Poláková et al._Soft skills and their importance in the labour market under the conditions of Industry 5.0.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2023_Poláková et al._Soft skills and their importance in the labour market under the conditions of Industry 5.0.pdf:application/pdf},
}

@article{2023b,
	title = {An {Immersive} {Virtual} {Reality} {Simulation} for {Cross}-{Cultural} {Communication} {Skills}: {Development} and {Feasibility}},
	volume = {77},
	issn = {1876-1399},
	shorttitle = {An {Immersive} {Virtual} {Reality} {Simulation} for {Cross}-{Cultural} {Communication} {Skills}},
	url = {https://www.sciencedirect.com/science/article/pii/S1876139923000051},
	doi = {10.1016/j.ecns.2023.01.005},
	abstract = {Educational approaches proven to produce cultural competence among nurses, consistently and cost-effectively, are not yet widely available. This study…},
	language = {en-US},
	urldate = {2024-07-22},
	journal = {Clinical Simulation in Nursing},
	month = apr,
	year = {2023},
	note = {Publisher: Elsevier},
	pages = {13--22},
	file = {Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\WZPDRCHN\\S1876139923000051.html:text/html},
}

@article{kourtesis2023,
	title = {Virtual {Reality} {Training} of {Social} {Skills} in {Adults} with {Autism} {Spectrum} {Disorder}: {An} {Examination} of {Acceptability}, {Usability}, {User} {Experience}, {Social} {Skills}, and {Executive} {Functions}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-328X},
	shorttitle = {Virtual {Reality} {Training} of {Social} {Skills} in {Adults} with {Autism} {Spectrum} {Disorder}},
	url = {https://www.mdpi.com/2076-328X/13/4/336},
	doi = {10.3390/bs13040336},
	abstract = {Poor social skills in autism spectrum disorder (ASD) are associated with reduced independence in daily life. Current interventions for improving the social skills of individuals with ASD fail to represent the complexity of real-life social settings and situations. Virtual reality (VR) may facilitate social skills training in social environments and situations similar to those in real life; however, more research is needed to elucidate aspects such as the acceptability, usability, and user experience of VR systems in ASD. Twenty-five participants with ASD attended a neuropsychological evaluation and three sessions of VR social skills training, which incorporated five social scenarios with three difficulty levels. Participants reported high acceptability, system usability, and user experience. Significant correlations were observed between performance in social scenarios, self-reports, and executive functions. Working memory and planning ability were significant predictors of the functionality level in ASD and the VR system’s perceived usability, respectively. Yet, performance in social scenarios was the best predictor of usability, acceptability, and functionality level. Planning ability substantially predicted performance in social scenarios, suggesting an implication in social skills. Immersive VR social skills training in individuals with ASD appears to be an appropriate service, but an errorless approach that is adaptive to the individual’s needs should be preferred.},
	language = {en},
	number = {4},
	urldate = {2024-07-22},
	journal = {Behavioral Sciences},
	author = {Kourtesis, Panagiotis and Kouklari, Evangelia-Chrysanthi and Roussos, Petros and Mantas, Vasileios and Papanikolaou, Katerina and Skaloumbakas, Christos and Pehlivanidis, Artemios},
	month = apr,
	year = {2023},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {virtual reality, training, autism, acceptability, executive functions, prompts, social cognition, social skills, usability, user experience},
	pages = {336},
	file = {2023_Kourtesis et al._Virtual Reality Training of Social Skills in Adults with Autism Spectrum Disorder An Examination of.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2023_Kourtesis et al._Virtual Reality Training of Social Skills in Adults with Autism Spectrum Disorder An Examination of.pdf:application/pdf},
}

@article{basdogan2024,
	title = {Perception of {Soft} {Objects} in {Virtual} {Environments} {Under} {Conflicting} {Visual} and {Haptic} {Cues}},
	volume = {17},
	issn = {2329-4051},
	url = {https://ieeexplore.ieee.org/document/10272716},
	doi = {10.1109/TOH.2023.3322189},
	abstract = {In virtual/augmented/mixed reality (VR/AR/MR) applications, rendering soft virtual objects using a hand-held haptic device is challenging due to the anatomical restrictions of the hand and the ungrounded nature of the design, which affect the selection of actuators and sensors and hence limit the resolution and range of forces displayed by the device. We developed a cable-driven haptic device for rendering the net forces involved in grasping and squeezing 3D virtual compliant (soft) objects being held between the index finger and thumb only. Using the proposed device, we investigate the perception of soft objects in virtual environments. We show that the range of object stiffness that can be effectively conveyed to a user in virtual environments (VEs) can be significantly expanded by controlling the relationship between the visual and haptic cues. We propose that a single variable, named Apparent Stiffness Difference, can predict the pattern of human stiffness perception under manipulated conflict, which can be used for rendering a range of soft objects in VEs larger than what is achievable by a haptic device alone due to its physical limits.},
	number = {2},
	urldate = {2024-07-22},
	journal = {IEEE Transactions on Haptics},
	author = {Basdogan, Cagatay and Ataseven, Berke and Srinivasan, Mandayam A.},
	month = apr,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Haptics},
	keywords = {Haptic interfaces, Actuators, Fingers, Force feedback, Hand-held haptic devices, haptic rendering of soft objects, multi-modal illusions, Psychophysics, sensory integration, stiffness perception, Thumb, virtual environments, Virtual environments, visual-haptic interactions},
	pages = {227--236},
	file = {2024_Basdogan et al._Perception of Soft Objects in Virtual Environments Under Conflicting Visual and Haptic Cues.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\haptic\\2024_Basdogan et al._Perception of Soft Objects in Virtual Environments Under Conflicting Visual and Haptic Cues.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\BW3ALJ6D\\10272716.html:text/html},
}

@inproceedings{yang2023,
	address = {New York, NY, USA},
	series = {{FME} '23},
	title = {Simple but {Effective} {In}-the-wild {Micro}-{Expression} {Spotting} {Based} on {Head} {Pose} {Segmentation}},
	isbn = {979-8-4007-0285-3},
	url = {https://doi.org/10.1145/3607829.3616445},
	doi = {10.1145/3607829.3616445},
	abstract = {Micro-expressions may occur in high-stake situations when people attempt to conceal or suppress their true feelings. Nowadays, intelligent micro-expression analysis has long been focused on videos captured under constrained laboratory conditions. This is due to the relatively small number of publicly available datasets. Moreover, micro-expression characteristics are subtle and brief, and thus very susceptible to interference from external factors and difficult to capture. In particular, head movement is unavoidable in unconstrained scenarios, making micro-expression spotting highly challenging. This paper proposes a simple yet effective method for avoiding the interference of head movement on micro-expression spotting in natural scenarios by considering three-dimensional space. In particular, based on the head pose, which can be mapped to two-dimensional vectors (translations and rotations) for representation, long and complex videos could be divided into short video segments that basically exclude head movement interference. Following that, segmented micro-expression spotting is realized based on an effective short-segment-based micro-expression spotting algorithm. Experimental results on in-the-wild databases demonstrate the effectiveness of our proposed method in avoiding head movement interference. Additionally, due to the simplicity of this method, it creates opportunities for spotting micro-expressions in real-world scenarios, possibly even in real-time. Furthermore, it helps alleviate the small sample size problem in micro-expression analysis by boosting the spotting performance in massive unlabeled videos.},
	urldate = {2024-07-22},
	booktitle = {Proceedings of the 3rd {Workshop} on {Facial} {Micro}-{Expression}: {Advanced} {Techniques} for {Multi}-{Modal} {Facial} {Expression} {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Xingpeng and Yang, Henian and Li, Jingting and Wang, Su-Jing},
	month = oct,
	year = {2023},
	pages = {9--16},
	file = {2023_Yang et al._Simple but Effective In-the-wild Micro-Expression Spotting Based on Head Pose Segmentation.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2023_Yang et al._Simple but Effective In-the-wild Micro-Expression Spotting Based on Head Pose Segmentation.pdf:application/pdf},
}

@book{moldoveanu2024,
	title = {Soft {Skills}: {How} to {See}, {Measure} and {Build} the {Skills} that {Make} {Us} {Uniquely} {Human}},
	isbn = {978-3-11-105552-7},
	shorttitle = {Soft {Skills}},
	abstract = {Although communicative and relational skills are currently in the greatest demand in organizations large and small, we are as educators, executives, and talent developers very far away from the kind of precision in identifying, measuring, selecting and developing these skills that we have achieved with cognitive and technical skills. At the same time, the relentless automation of swaths of human tasks has placed a sharp light on the ‘quintessentially human skills’ – those that cannot and in some cases should not be subject to algorithmic automation. This book aims to ‘change the soft skills game’ by introducing language for identifying and describing them, ways of measuring the degree to which a person possesses them and selecting those who possess them in the utmost from those less skilled, and ways of helping students and executives alike develop them, through a methodology that has been designed and practiced for the past ten years.     We need a ‘re-set’ in the way we think about human skill and in particular the ways we think about those human skills which cannot be sub-contracted to an algorithm running on silicon. This book aims to provide that re-set.},
	language = {en},
	publisher = {Walter de Gruyter GmbH \& Co KG},
	author = {Moldoveanu, Mihnea},
	month = apr,
	year = {2024},
	note = {Google-Books-ID: Io77EAAAQBAJ},
	keywords = {Business \& Economics / General, Business \& Economics / Human Resources \& Personnel Management, Business \& Economics / Leadership, Self-Help / Personal Growth / Success},
}

@article{wahid2023,
	title = {Human {Micro}-{Expression}: {A} {Novel} {Social} {Behavioral} {Biometric} for {Person} {Identification}},
	volume = {11},
	issn = {2169-3536},
	shorttitle = {Human {Micro}-{Expression}},
	url = {https://ieeexplore.ieee.org/abstract/document/10145790},
	doi = {10.1109/ACCESS.2023.3283932},
	abstract = {The reliance on Online Social Networks (OSN) for both formal and informal social interactions has dramatically changed the way people communicate. In this paper, a novel Social Behavioral Biometric (SBB), human micro-expression, is introduced for person identification. An emotion detection model is developed to extract emotion probability scores from person’s writing samples posted on Twitter. The corresponding emotion-progression features are extracted using an original technique that turns users’ microblogs into emotion-progression signals. Finally, a novel social behavioral biometric system that leverages rank-level weighted majority voting to achieve an accurate person identification is implemented. The proposed system is validated on a proprietary benchmark dataset consisting of 250 Twitter users. The experimental results convincingly demonstrate that the proposed social behavioral biometric, human micro-expression, possesses a strong distinguishable ability and can be used for person identification. The study further reveals that the proposed social behavioral biometric outperforms all the original SBB traits.},
	urldate = {2024-07-22},
	journal = {IEEE Access},
	author = {Wahid, Zaman and Bari, Asm Hossain and Anzum, Fahim and Gavrilova, Marina L.},
	year = {2023},
	note = {Conference Name: IEEE Access},
	keywords = {Emotion recognition, Biometrics (access control), Behavioral sciences, Biometric security, Blogs, emotion extraction, emotion progression signals, Feature extraction, human micro-expression, Identification of persons, natural language processing, person identification, social behavioral biometrics, Social networking (online)},
	pages = {57481--57493},
	file = {2023_Wahid et al._Human Micro-Expression A Novel Social Behavioral Biometric for Person Identification.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2023_Wahid et al._Human Micro-Expression A Novel Social Behavioral Biometric for Person Identification.pdf:application/pdf},
}

@article{wahid2023a,
	title = {Human {Micro}-{Expressions} in {Multimodal} {Social} {Behavioral} {Biometrics}},
	volume = {23},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/23/19/8197},
	doi = {10.3390/s23198197},
	abstract = {The advent of Social Behavioral Biometrics (SBB) in the realm of person identification has underscored the importance of understanding unique patterns of social interactions and communication. This paper introduces a novel multimodal SBB system that integrates human micro-expressions from text, an emerging biometric trait, with other established SBB traits in order to enhance online user identification performance. Including human micro-expression, the proposed method extracts five other original SBB traits for a comprehensive representation of the social behavioral characteristics of an individual. Upon finding the independent person identification score by every SBB trait, a rank-level fusion that leverages the weighted Borda count is employed to fuse the scores from all the traits, obtaining the final identification score. The proposed method is evaluated on a benchmark dataset of 250 Twitter users, and the results indicate that the incorporation of human micro-expression with existing SBB traits can substantially boost the overall online user identification performance, with an accuracy of 73.87\% and a recall score of 74\%. Furthermore, the proposed method outperforms the state-of-the-art SBB systems.},
	language = {en},
	number = {19},
	urldate = {2024-07-22},
	journal = {Sensors},
	author = {Wahid, Zaman and Bari, A. S. M. Hossain and Gavrilova, Marina},
	month = jan,
	year = {2023},
	note = {Number: 19
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {human micro-expression, natural language processing, person identification, social behavioral biometrics, biometric identification},
	pages = {8197},
	file = {2023_Wahid et al._Human Micro-Expressions in Multimodal Social Behavioral Biometrics.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2023_Wahid et al._Human Micro-Expressions in Multimodal Social Behavioral Biometrics.pdf:application/pdf},
}

@inproceedings{hickson2019,
	title = {Eyemotion: {Classifying} {Facial} {Expressions} in {VR} {Using} {Eye}-{Tracking} {Cameras}},
	shorttitle = {Eyemotion},
	url = {https://ieeexplore.ieee.org/document/8658392},
	doi = {10.1109/WACV.2019.00178},
	abstract = {One of the main challenges of social interaction in virtual reality settings is that head-mounted displays occlude a large portion of the face, blocking facial expressions and thereby restricting social engagement cues among users. We present an algorithm to automatically infer expressions by analyzing only a partially occluded face while the user is engaged in a virtual reality experience. Specifically, we show that images of the user's eyes captured from an IR gaze-tracking camera within a VR headset are sufficient to infer a subset of facial expressions without the use of any fixed external camera. Using these inferences, we can generate dynamic avatars in real-time which function as an expressive surrogate for the user. We propose a novel data collection pipeline as well as a novel approach for increasing CNN accuracy via personalization. Our results show a mean accuracy of 74\% (F1 of 0.73) among 5 'emotive' expressions and a mean accuracy of 70\% (F1 of 0.68) among 10 distinct facial action units, outperforming human raters.},
	urldate = {2024-07-22},
	booktitle = {2019 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Hickson, Steven and Dufour, Nick and Sud, Avneesh and Kwatra, Vivek and Essa, Irfan},
	month = jan,
	year = {2019},
	note = {ISSN: 1550-5790},
	keywords = {Deep learning, Cameras, Headphones, Visualization, Avatars, Face, Resists},
	pages = {1626--1635},
	file = {2019_Hickson et al._Eyemotion Classifying Facial Expressions in VR Using Eye-Tracking Cameras.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2019_Hickson et al._Eyemotion Classifying Facial Expressions in VR Using Eye-Tracking Cameras.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\UVN9NKRN\\8658392.html:text/html},
}

@inproceedings{sasikumar2024,
	title = {A {User} {Study} on {Sharing} {Physiological} {Cues} in {VR} {Assembly} {Tasks}},
	url = {https://ieeexplore.ieee.org/document/10494102},
	doi = {10.1109/VR58804.2024.00096},
	abstract = {In collaborative settings where multiple individuals are tasked with completing a shared goal, understanding one’s partner’s emotional state could be crucial for achieving a successful outcome. This is particularly relevant in remote collaboration contexts, where physical distance can impede understanding, empathy, and mutual comprehension between partners. In this paper, we demonstrate representing emotional patterns from physiological data in a shared Virtual Reality (VR) environment, and explore how it impacted communication styles. A user study investigated the potential effects of this emotional representation in fostering empathetic communication during remote collaboration. The study’s findings revealed that although there was minimal variance in the workload associated with observing physiological cues, participants generally preferred monitoring their partner’s attentional state. However, with the assembly task chosen, most participants only directed a minimal proportion of their attention toward the physiological cues displayed by their partner, and were frequently uncertain of how to interpret and use the information obtained. We also discuss limitations of the research and opportunities for future work.},
	urldate = {2024-07-22},
	booktitle = {2024 {IEEE} {Conference} {Virtual} {Reality} and {3D} {User} {Interfaces} ({VR})},
	author = {Sasikumar, Prasanth and Hajika, Ryo and Gupta, Kunal and Gunasekaran, Tamil Selvan and Pai, Yun Suen and Bai, Huidong and Nanayakkara, Suranga and Billinghurst, Mark},
	month = mar,
	year = {2024},
	note = {ISSN: 2642-5254},
	keywords = {Virtual reality, Three-dimensional displays, User interfaces, Collaboration, Adaptive VR, AR Assembly, Biomedical monitoring, Collaborative VR, Emotion Adaptive VR, Measurement, Physiological sensing, Physiology},
	pages = {765--773},
	file = {2024_Sasikumar et al._A User Study on Sharing Physiological Cues in VR Assembly Tasks.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2024_Sasikumar et al._A User Study on Sharing Physiological Cues in VR Assembly Tasks.pdf:application/pdf},
}

@inproceedings{liu2023,
	title = {Identifying {Emotions} for {Virtual} {Reality} {Based} on {Eye} {Tracking} and {Cloud} {Computing}},
	url = {https://ieeexplore.ieee.org/document/10234036},
	doi = {10.1109/APWCS60142.2023.10234036},
	abstract = {With the rapid development of virtual reality technology, immersive experiences have become the new generation’s sought-after entertainment feast. Nowadays, various industries have also started utilizing virtual reality for vocational training in fields such as education, healthcare, entertainment, and military. In virtual environments, users can interact socially with others through virtual avatars as their representatives. In this study, we aim to integrate virtual reality devices, eye-tracking devices, and public cloud computing services to infer emotions based on changes in eye data. We evaluate the average accuracy of the proposed model and hope to enhance the facial emotions of avatars to better align with users’ intentions. This study will contribute to improving the authenticity of social interaction and providing users with a better social experience in virtual reality environments.},
	urldate = {2024-07-22},
	booktitle = {2023 {VTS} {Asia} {Pacific} {Wireless} {Communications} {Symposium} ({APWCS})},
	author = {Liu, Yi-Chun and Huang, Huai-Sheng},
	month = aug,
	year = {2023},
	keywords = {Solid modeling, Virtual Reality, Virtual environments, Avatars, Cloud computing, Cloud Computing, Computational modeling, Emotion, Eye Tracking, Military computing, Social Interaction, Wireless communication},
	pages = {1--2},
	file = {2023_Liu and Huang_Identifying Emotions for Virtual Reality Based on Eye Tracking and Cloud Computing.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2023_Liu and Huang_Identifying Emotions for Virtual Reality Based on Eye Tracking and Cloud Computing.pdf:application/pdf},
}

@inproceedings{dehghani2023,
	title = {Facial {Emotion} {Recognition} in {VR} {Games}},
	url = {https://ieeexplore.ieee.org/document/10333160},
	doi = {10.1109/CoG57401.2023.10333160},
	abstract = {Emotion detection is a crucial component of Games User Research (GUR), as it allows game developers to gain insights into players’ emotional experiences and tailor their games accordingly. However, detecting emotions in Virtual Reality (VR) games is challenging due to the Head-Mounted Display (HMD) that covers the top part of the player’s face, namely, their eyes and eyebrows, which provide crucial information for recognizing the impression. To tackle this we used a Convolutional Neural Network (CNN) to train a model to predict emotions in full-face images where the eyes and eyebrows are covered. We used the FER2013 dataset, which we modified to cover eyes and eyebrows in images. The model in these images can accurately recognize seven different emotions which are anger, happiness, disgust, fear, impartiality, sadness and surprise.We assessed the model’s performance by testing it on two VR games and using it to detect players’ emotions. We collected self-reported emotion data from the players after the gameplay sessions. We analyzed the data collected from our experiment to understand which emotions players experience during the gameplay. We found that our approach has the potential to enhance gameplay analysis by enabling the detection of players’ emotions in VR games, which can help game developers create more engaging and immersive game experiences.},
	urldate = {2024-07-22},
	booktitle = {2023 {IEEE} {Conference} on {Games} ({CoG})},
	author = {Dehghani, Fatemeh and Zaman, Loutfouz},
	month = aug,
	year = {2023},
	note = {ISSN: 2325-4289},
	keywords = {Virtual reality, Face recognition, Emotion recognition, Solid modeling, Virtual Reality, Resists, Emotions, Facial Expressions, Games, Players, Predictive models},
	pages = {1--4},
	file = {2023_Dehghani and Zaman_Facial Emotion Recognition in VR Games.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2023_Dehghani and Zaman_Facial Emotion Recognition in VR Games.pdf:application/pdf},
}

@article{kim2023,
	title = {Facial {Expression} {Recognition} in the {Wild} {Using} {Face} {Graph} and {Attention}},
	volume = {11},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10153584},
	doi = {10.1109/ACCESS.2023.3286547},
	abstract = {Facial expression recognition (FER) in the wild from various viewpoints, lighting conditions, face poses, scales, and occlusions is an extremely challenging field of research. In this study, we construct a face graph by selecting action units that play an important role in changing facial expressions, and we propose an algorithm for recognizing facial expressions using a graph convolutional network (GCN). We first generated an attention map that can highlight action units to extract important facial expression features from faces in the wild. After feature extraction, a face graph is constructed by combining the attention map with face patches, and changes in expression in the wild are recognized using a GCN. Through comparative experiments conducted using both lab-controlled and wild datasets, we prove that the proposed method is the most suitable FER approach for use with image datasets captured in the wild and those under well-controlled indoor conditions.},
	urldate = {2024-07-22},
	journal = {IEEE Access},
	author = {Kim, Hyeongjin and Lee, Jong-Ha and Ko, Byoung Chul},
	year = {2023},
	note = {Conference Name: IEEE Access},
	keywords = {Face recognition, Lighting, Three-dimensional displays, action unit, Feature extraction, attention map, Convolutional neural networks, face graph, Facial expression recognition, graph convolutional network, Image recognition, Image reconstruction},
	pages = {59774--59787},
	file = {2023_Kim et al._Facial Expression Recognition in the Wild Using Face Graph and Attention.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2023_Kim et al._Facial Expression Recognition in the Wild Using Face Graph and Attention.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\QCY2K74U\\10153584.html:text/html},
}

@inproceedings{nordinforsberg2023,
	address = {Cham},
	title = {{VR} for {HR} – {A} {Case} {Study} of {Human} {Resource} {Development} {Professionals} {Using} {Virtual} {Reality} for {Social} {Skills} {Training} in the {Workplace}},
	isbn = {978-3-031-42293-5},
	doi = {10.1007/978-3-031-42293-5_17},
	abstract = {The Human Resource (HR) area has made little use of innovative technologies to develop its processes, routines and education. However, we believe that digital tools such as Virtual Reality (VR) can play an important role in developing social aspects of work. We have investigated Human Resource Development Professionals’ (HRD-Ps’) perception of using a VR-prototype for training of social skills in the workplace. A digital three-dimensional world was designed for the study participants, in which they interacted with agents to train social skills in the workplace. Study participants explored a VR-prototype through the usage of head-mounted devices (HMD). We collected the designer’s description of the intended design element of the VR prototype and pre- and post-intervention questionnaire from the study participants and conducted a top-down thematic analysis. The three intended design elements 1) focus on the training experience, 2) learning-depth through emotional response for engagement and motivation, and 3) perspective-taking enabled by game design, were confirmed and reflected upon by the HRD-Ps’. Additionally, using VR for social skills training in the workplace was recognized as innovative, and could have the capacity to position an organization as being in the forefront of digitalization. The conclusion is that VR has a potential to create engagement and provide insights in HR matters, but further studies are needed to show the full power and potential in using VR for HR matters.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2023},
	publisher = {Springer Nature Switzerland},
	author = {Nordin Forsberg, Britta and Lundström, Anders and Gulliksen, Jan},
	editor = {Abdelnour Nocera, José and Kristín Lárusdóttir, Marta and Petrie, Helen and Piccinno, Antonio and Winckler, Marco},
	year = {2023},
	pages = {231--251},
	file = {2023_Nordin Forsberg et al._VR for HR – A Case Study of Human Resource Development Professionals Using Virtual Reality for Socia.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2023_Nordin Forsberg et al._VR for HR – A Case Study of Human Resource Development Professionals Using Virtual Reality for Socia.pdf:application/pdf},
}

@article{li2018,
	title = {Towards {Reading} {Hidden} {Emotions}: {A} {Comparative} {Study} of {Spontaneous} {Micro}-{Expression} {Spotting} and {Recognition} {Methods}},
	volume = {9},
	issn = {1949-3045},
	shorttitle = {Towards {Reading} {Hidden} {Emotions}},
	url = {https://ieeexplore.ieee.org/document/7851001},
	doi = {10.1109/TAFFC.2017.2667642},
	abstract = {Micro-expressions (MEs) are rapid, involuntary facial expressions which reveal emotions that people do not intend to show. Studying MEs is valuable as recognizing them has many important applications, particularly in forensic science and psychotherapy. However, analyzing spontaneous MEs is very challenging due to their short duration and low intensity. Automatic ME analysis includes two tasks: ME spotting and ME recognition. For ME spotting, previous studies have focused on posed rather than spontaneous videos. For ME recognition, the performance of previous studies is low. To address these challenges, we make the following contributions: (i) We propose the first method for spotting spontaneous MEs in long videos (by exploiting feature difference contrast). This method is training free and works on arbitrary unseen videos. (ii) We present an advanced ME recognition framework, which outperforms previous work by a large margin on two challenging spontaneous ME databases (SMIC and CASMEII). (iii) We propose the first automatic ME analysis system (MESR), which can spot and recognize MEs from spontaneous video data. Finally, we show our method outperforms humans in the ME recognition task by a large margin, and achieves comparable performance to humans at the very challenging task of spotting and then recognizing spontaneous MEs.},
	number = {4},
	urldate = {2024-07-23},
	journal = {IEEE Transactions on Affective Computing},
	author = {Li, Xiaobai and Hong, Xiaopeng and Moilanen, Antti and Huang, Xiaohua and Pfister, Tomas and Zhao, Guoying and Pietikäinen, Matti},
	month = oct,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {Face recognition, Cameras, Emotion recognition, Training, Videos, affective computing, facial expression recognition, HOG, LBP, Micro-expression},
	pages = {563--577},
	file = {2018_Li et al._Towards Reading Hidden Emotions A Comparative Study of Spontaneous Micro-Expression Spotting and Re.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2018_Li et al._Towards Reading Hidden Emotions A Comparative Study of Spontaneous Micro-Expression Spotting and Re.pdf:application/pdf},
}

@article{lakshminarayanan2023,
	title = {Health {Care} {Equity} {Through} {Intelligent} {Edge} {Computing} and {Augmented} {Reality}/{Virtual} {Reality}: {A} {Systematic} {Review}},
	volume = {16},
	issn = {null},
	shorttitle = {Health {Care} {Equity} {Through} {Intelligent} {Edge} {Computing} and {Augmented} {Reality}/{Virtual} {Reality}},
	url = {https://www.tandfonline.com/doi/abs/10.2147/JMDH.S419923},
	doi = {10.2147/JMDH.S419923},
	abstract = {Intellectual capital is a scarce resource in the healthcare industry. Making the most of this resource is the first step toward achieving a completely intelligent healthcare system. However, most existing centralized and deep learning-based systems are unable to adapt to the growing volume of global health records and face application issues. To balance the scarcity of healthcare resources, the emerging trend of IoMT (Internet of Medical Things) and edge computing will be very practical and cost-effective. A full examination of the transformational role of intelligent edge computing in the IoMT era to attain health care equity is offered in this research. Intelligent edge computing-aided distribution and collaborative information management is a possible approach for a long-term digital healthcare system. Furthermore, IEC (Intelligent Edge Computing) encourages digital health data to be processed only at the edge, minimizing the amount of information exchanged with central servers/the internet. This significantly increases the privacy of digital health data. Another critical component of a sustainable healthcare system is affordability in digital healthcare. Affordability in digital healthcare is another key component of a sustainable healthcare system. Despite its importance, it has received little attention due to its complexity. In isolated and rural areas where expensive equipment is unavailable, IEC with AR / VR, also known as edge device shadow, can play a significant role in the inexpensive data collection process. Healthcare equity becomes a reality by combining intelligent edge device shadows and edge computing.},
	urldate = {2024-07-24},
	journal = {Journal of Multidisciplinary Healthcare},
	author = {Lakshminarayanan, Vishal and Ravikumar, Aswathy and Sriraman, Harini and Alla, Sujatha and Chattu, Vijay Kumar},
	month = dec,
	year = {2023},
	pmid = {37753339},
	note = {Publisher: Dove Medical Press
\_eprint: https://www.tandfonline.com/doi/pdf/10.2147/JMDH.S419923},
	keywords = {machine learning, digital health, distributed computing, equitable healthcare, internet of things},
	pages = {2839--2859},
	file = {2023_Lakshminarayanan et al._Health Care Equity Through Intelligent Edge Computing and Augmented RealityVirtual Reality A Syste.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2023_Lakshminarayanan et al._Health Care Equity Through Intelligent Edge Computing and Augmented RealityVirtual Reality A Syste.pdf:application/pdf},
}

@article{yondjo2024,
	title = {“{VR} is the future”: perspectives of healthcare professionals on virtual reality as a diagnostic tool for dementia status in primary care},
	volume = {24},
	issn = {1472-6947},
	shorttitle = {“{VR} is the future”},
	url = {https://doi.org/10.1186/s12911-023-02413-y},
	doi = {10.1186/s12911-023-02413-y},
	abstract = {Healthcare professionals (HPs) hold critical perspectives on the barriers and facilitating factors for the implementation of virtual reality (VR) dementia diagnosis tools in the clinical setting. This study aims to explore HP perspectives regarding the clinical implementation of dementia diagnosis tools using VR platforms.},
	language = {en},
	number = {1},
	urldate = {2024-07-24},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Yondjo, Joshua and Siette, Joyce},
	month = jan,
	year = {2024},
	keywords = {Virtual reality, Dementia, Healthcare professionals, Primary care},
	pages = {9},
	file = {2024_Yondjo and Siette_“VR is the future” perspectives of healthcare professionals on virtual reality as a diagnostic tool.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2024_Yondjo and Siette_“VR is the future” perspectives of healthcare professionals on virtual reality as a diagnostic tool.pdf:application/pdf},
}

@article{2022,
	title = {Virtual reality in the diagnostic and therapy for mental disorders: {A} systematic review},
	volume = {98},
	issn = {0272-7358},
	shorttitle = {Virtual reality in the diagnostic and therapy for mental disorders},
	url = {https://www.sciencedirect.com/science/article/pii/S0272735822000988},
	doi = {10.1016/j.cpr.2022.102213},
	abstract = {Virtual reality (VR) technologies are playing an increasingly important role in the diagnostics and treatment of mental disorders.To systematically re…},
	language = {en-US},
	urldate = {2024-07-24},
	journal = {Clinical Psychology Review},
	month = dec,
	year = {2022},
	note = {Publisher: Pergamon},
	pages = {102213},
	file = {2022_Virtual reality in the diagnostic and therapy for mental disorders A systematic review.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2022_Virtual reality in the diagnostic and therapy for mental disorders A systematic review.pdf:application/pdf},
}

@article{tabbaa2022,
	title = {{VREED}: {Virtual} {Reality} {Emotion} {Recognition} {Dataset} {Using} {Eye} {Tracking} \& {Physiological} {Measures}},
	volume = {5},
	shorttitle = {{VREED}},
	url = {https://doi.org/10.1145/3495002},
	doi = {10.1145/3495002},
	abstract = {The paper introduces a multimodal affective dataset named VREED (VR Eyes: Emotions Dataset) in which emotions were triggered using immersive 360° Video-Based Virtual Environments (360-VEs) delivered via Virtual Reality (VR) headset. Behavioural (eye tracking) and physiological signals (Electrocardiogram (ECG) and Galvanic Skin Response (GSR)) were captured, together with self-reported responses, from healthy participants (n=34) experiencing 360-VEs (n=12, 1--3 min each) selected through focus groups and a pilot trial. Statistical analysis confirmed the validity of the selected 360-VEs in eliciting the desired emotions. Preliminary machine learning analysis was carried out, demonstrating state-of-the-art performance reported in affective computing literature using non-immersive modalities. VREED is among the first multimodal VR datasets in emotion recognition using behavioural and physiological signals. VREED is made publicly available on Kaggle1. We hope that this contribution encourages other researchers to utilise VREED further to understand emotional responses in VR and ultimately enhance VR experiences design in applications where emotional elicitation plays a key role, i.e. healthcare, gaming, education, etc.},
	number = {4},
	urldate = {2024-07-24},
	journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	author = {Tabbaa, Luma and Searle, Ryan and Bafti, Saber Mirzaee and Hossain, Md Moinul and Intarasisrisawat, Jittrapol and Glancy, Maxine and Ang, Chee Siang},
	month = dec,
	year = {2022},
	pages = {178:1--178:20},
	file = {PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2022_Tabbaa et al._VREED Virtual Reality Emotion Recognition Dataset Using Eye Tracking & Physiological Measures.pdf:application/pdf},
}

@book{ekman2003,
	address = {Cambridge, Mass.},
	title = {Unmasking the {Face}: {A} {Guide} to {Recognizing} {Emotions} {From} {Facial} {Expressions}},
	isbn = {978-1-883536-36-7},
	shorttitle = {Unmasking the {Face}},
	abstract = {Ekman and Friesen’s breakthrough research on the facial expression of emotion is richly illustrated with photographs depicting surprise, fear, disgust, anger, happiness and sadness. The authors explain how to identify these basic emotions correctly and how to tell when people try to mask, simulate or neutralize them. The book features several practical exercises that can help actors, teachers, salesmen, counselors, nurses, law-enforcement personnel and physicians – and everyone else who deals with people – become adept, perceptive readers of the facial expressions of emotions.},
	language = {English},
	publisher = {Malor Books},
	author = {Ekman, Paul and Friesen, Wallace V.},
	month = dec,
	year = {2003},
}

@article{kang2024,
	title = {Analysis by {Synthesis} {Assessment} of {Speech} {Emotion} {Perception} in  {Different} {Languages}},
	language = {en},
	author = {Kang, Jueun and Sani, Paolo},
	year = {2024},
	file = {PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2024_Kang and Sani_Analysis by Synthesis Assessment of Speech Emotion Perception in  Different Languages.pdf:application/pdf},
}

@misc{zotero-1798,
	title = {Les nouvelles technologies au service de la formation - {PwC} {Store} {France}},
	url = {https://store.pwc.fr/fr/nouvelles-technologies-pour-formation},
	abstract = {Avec la montée en puissance des technologies, c'est le bon moment pour de nombreuses entreprises de commencer ou d'accélérer l’usage de la VR. De nouvelles opportunités se créent en matière de perfectionnement et de collaboration basées sur la réalité virtuelle.},
	language = {fr},
	urldate = {2024-07-24},
}

@article{nardelli2015,
	title = {Recognizing {Emotions} {Induced} by {Affective} {Sounds} through {Heart} {Rate} {Variability}},
	volume = {6},
	issn = {1949-3045},
	url = {https://ieeexplore.ieee.org/document/7106477},
	doi = {10.1109/TAFFC.2015.2432810},
	abstract = {This paper reports on how emotional states elicited by affective sounds can be effectively recognized by means of estimates of Autonomic Nervous System (ANS) dynamics. Specifically, emotional states are modeled as a combination of arousal and valence dimensions according to the well-known circumplex model of affect, whereas the ANS dynamics is estimated through standard and nonlinear analysis of Heart rate variability (HRV) exclusively, which is derived from the electrocardiogram (ECG). In addition, Lagged Poincaré Plots of the HRV series were also taken into account. The affective sounds were gathered from the International Affective Digitized Sound System and grouped into four different levels of arousal (intensity) and two levels of valence (unpleasant and pleasant). A group of 27 healthy volunteers were administered with these standardized stimuli while ECG signals were continuously recorded. Then, those HRV features showing significant changes (p {\textless}; 0.05 from statistical tests) between the arousal and valence dimensions were used as input of an automatic classification system for the recognition of the four classes of arousal and two classes of valence. Experimental results demonstrated that a quadratic discriminant classifier, tested through Leave-One-Subject-Out procedure, was able to achieve a recognition accuracy of 84.72 percent on the valence dimension, and 84.26 percent on the arousal dimension.},
	number = {4},
	urldate = {2024-07-24},
	journal = {IEEE Transactions on Affective Computing},
	author = {Nardelli, Mimma and Valenza, Gaetano and Greco, Alberto and Lanata, Antonio and Scilingo, Enzo Pasquale},
	month = oct,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {Emotion recognition, Heart rate variability, Standards, Feature extraction, Acoustics, affective digitized sound system (IADS), Affective Digitized Sound System (IADS), autonomic nervous system, Autonomic Nervous System, Emotion Recognition, Heart rate measurement, heart rate variability, nonlinear analysis, Nonlinear Analysis, Poincare plot, poincaré plot, quadratic discriminant classifier, Quadratic Discriminant Classifier, Statistical analysis},
	pages = {385--394},
	file = {PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2015_Nardelli et al._Recognizing Emotions Induced by Affective Sounds through Heart Rate Variability.pdf:application/pdf},
}

@article{zhang2015,
	title = {Classification of {Evoked} {Emotions} {Using} an {Artificial} {Neural} {Network} {Based} on {Single}, {Short}-{Term} {Physiological} {Signals}},
	volume = {19},
	url = {https://www.fujipress.jp/jaciii/jc/jacii001900010118/},
	doi = {10.20965/jaciii.2015.p0118},
	abstract = {Title: Classification of Evoked Emotions Using an Artificial Neural Network Based on Single, Short-Term Physiological Signals {\textbar} Keywords: emotional recognition, ANN, automatic recognition, ECG, GSR {\textbar} Author: Shanbin Zhang, Guangyuan Liu, and Xiangwei Lai},
	number = {1},
	urldate = {2024-07-24},
	journal = {Journal of Advanced Computational Intelligence and Intelligent Informatics},
	author = {Zhang, Shanbin and Liu, Guangyuan and Lai, Xiangwei},
	month = jan,
	year = {2015},
	note = {Publisher: Fuji Technology Press Ltd.},
	pages = {118--126},
	file = {Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\93H5U2GR\\jacii001900010118.html:text/html},
}

@article{macedonio2007,
	title = {Immersiveness and {Physiological} {Arousal} within {Panoramic} {Video}-{Based} {Virtual} {Reality}},
	volume = {10},
	issn = {1094-9313},
	url = {https://www.liebertpub.com/doi/10.1089/cpb.2007.9997},
	doi = {10.1089/cpb.2007.9997},
	abstract = {In this paper, we discuss findings from a study that used panoramic video-based virtual environments (PVVEs) to induce self-reported anger. The study assessed “immersiveness” and physiological correlates of anger arousal (i.e., heart rate, blood pressure, galvanic skin response [GSR], respiration, and skin temperature). Results indicate that over time, panoramic video-based virtual scenarios can be, at the very least, physiologically arousing. Further, it can be affirmed from the results that hypnotizability, as defined by the applied measures, interacts with group on physiological arousal measures. Hence, physiological arousal appeared to be moderated by participant hypnotizability and absorption levels.},
	number = {4},
	urldate = {2024-07-24},
	journal = {CyberPsychology \& Behavior},
	author = {Macedonio, Mary F. and Parsons, Thomas D. and Digiuseppe, Raymond A. and Weiderhold, Brenda A. and Rizzo, Albert A.},
	month = aug,
	year = {2007},
	note = {Publisher: Mary Ann Liebert, Inc., publishers},
	pages = {508--515},
	file = {PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2007_Macedonio et al._Immersiveness and Physiological Arousal within Panoramic Video-Based Virtual Reality.pdf:application/pdf},
}

@article{valenza2012,
	title = {The {Role} of {Nonlinear} {Dynamics} in {Affective} {Valence} and {Arousal} {Recognition}},
	volume = {3},
	issn = {1949-3045},
	url = {https://ieeexplore.ieee.org/document/6007125},
	doi = {10.1109/T-AFFC.2011.30},
	abstract = {This paper reports on a new methodology for the automatic assessment of emotional responses. More specifically, emotions are elicited in agreement with a bidimensional spatial localization of affective states, that is, arousal and valence dimensions. A dedicated experimental protocol was designed and realized where specific affective states are suitably induced while three peripheral physiological signals, i.e., ElectroCardioGram (ECG), ElectroDermal Response (EDR), and ReSPiration activity (RSP), are simultaneously acquired. A group of 35 volunteers was presented with sets of images gathered from the International Affective Picture System (IAPS) having five levels of arousal and five levels of valence, including a neutral reference level in both. Standard methods as well as nonlinear dynamic techniques were used to extract sets of features from the collected signals. The goal of this paper is to implement an automatic multiclass arousal/valence classifier comparing performance when extracted features from nonlinear methods are used as an alternative to standard features. Results show that, when nonlinearly extracted features are used, the percentages of successful recognition dramatically increase. A good recognition accuracy ({\textgreater};90 percent) after 40-fold cross-validation steps for both arousal and valence classes was achieved by using the Quadratic Discriminant Classifier (QDC).},
	number = {2},
	urldate = {2024-07-24},
	journal = {IEEE Transactions on Affective Computing},
	author = {Valenza, Gaetano and Lanata, Antonio and Scilingo, Enzo Pasquale},
	month = apr,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {Emotion recognition, Feature extraction, affective computing, nonlinear analysis, Appraisal, Electrocardiography, Electromyography, feature extraction., Protocols, Support vector machines},
	pages = {237--249},
	file = {PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2012_Valenza et al._The Role of Nonlinear Dynamics in Affective Valence and Arousal Recognition.pdf:application/pdf},
}

@inproceedings{jalilifard2016,
	title = {Emotion classification using single-channel scalp-{EEG} recording},
	url = {https://ieeexplore.ieee.org/document/7590833},
	doi = {10.1109/EMBC.2016.7590833},
	abstract = {Several studies have found evidence for corticolimbic Theta electroencephalographic (EEG) oscillation in the neural processing of visual stimuli perceived as fear or threatening scene. Recent studies showed that neural oscillations' patterns in Theta, Alpha, Beta and Gamma sub-bands play a main role in brain's emotional processing. The main goal of this study is to classify two different emotional states by means of EEG data recorded through a single-electrode EEG headset. Nineteen young subjects participated in an EEG experiment while watching a video clip that evoked three emotional states: neutral, relaxation and scary. Following each video clip, participants were asked to report on their subjective affect by giving a score between 0 to 10. First, recorded EEG data were preprocessed by stationary wavelet transform (SWT) based denoising to remove artifacts. Afterward, the distribution of power in time-frequency space was obtained using short-time Fourier transform (STFT) and then, the mean value of energy was calculated for each EEG sub-band. Finally, 46 features, as the mean energy of frequency bands between 4 and 50 Hz, containing 689 instances - for each subject -were collected in order to classify the emotional states. Our experimental results show that EEG dynamics induced by horror and relaxing movies can be classified with average classification rate of 92\% using support vector machine (SVM) classifier. We also compared the performance of SVM to K-nearest neighbors (K-NN). The results show that K-NN achieves a better classification rate by 94\% accuracy. The findings of this work are expected to pave the way to a new horizon in neuroscience by proving the point that only single-channel EEG data carry enough information for emotion classification.},
	urldate = {2024-07-24},
	booktitle = {2016 38th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Jalilifard, Amir and Pizzolato, Ednaldo Brigante and Islam, Md Kafiul},
	month = aug,
	year = {2016},
	note = {ISSN: 1558-4615},
	keywords = {Electroencephalography, Noise reduction, Feature extraction, Support vector machines, Motion pictures, Wavelet transforms},
	pages = {845--849},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\S8ZG5AGW\\7590833.html:text/html;PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2016_Jalilifard et al._Emotion classification using single-channel scalp-EEG recording.pdf:application/pdf},
}

@inproceedings{kosunen2016,
	address = {New York, NY, USA},
	series = {{IUI} '16},
	title = {{RelaWorld}: {Neuroadaptive} and {Immersive} {Virtual} {Reality} {Meditation} {System}},
	isbn = {978-1-4503-4137-0},
	shorttitle = {{RelaWorld}},
	url = {https://doi.org/10.1145/2856767.2856796},
	doi = {10.1145/2856767.2856796},
	abstract = {Meditation in general and mindfulness in particular have been shown to be useful techniques in the treatment of a plethora of ailments, yet they can be challenging for novices. We present RelaWorld: a neuroadaptive virtual reality meditation system that combines virtual reality with neurofeedback to provide a tool that is easy for novices to use yet provides added value even for experienced meditators. Using a head-mounted display, users can levitate in a virtual world by doing meditation exercises. The system measures users' brain activity in real time via EEG and calculates estimates for the level of conCentration and relaxation. These values are then mapped into the virtual reality. In a user study of 43 subjects, we were able to show that the RelaWorld system elicits deeper relaxation, feeling of presence and a deeper level of meditation when compared to a similar setup without head-mounted display or neurofeedback.},
	urldate = {2024-07-24},
	booktitle = {Proceedings of the 21st {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Kosunen, Ilkka and Salminen, Mikko and Järvelä, Simo and Ruonala, Antti and Ravaja, Niklas and Jacucci, Giulio},
	month = mar,
	year = {2016},
	pages = {208--217},
}

@inproceedings{salvucci2000,
	address = {New York, NY, USA},
	series = {{ETRA} '00},
	title = {Identifying fixations and saccades in eye-tracking protocols},
	isbn = {978-1-58113-280-9},
	url = {https://doi.org/10.1145/355017.355028},
	doi = {10.1145/355017.355028},
	abstract = {The process of fixation identification—separating and labeling fixations and saccades in eye-tracking protocols—is an essential part of eye-movement data analysis and can have a dramatic impact on higher-level analyses. However, algorithms for performing fixation identification are often described informally and rarely compared in a meaningful way. In this paper we propose a taxonomy of fixation identification algorithms that classifies algorithms in terms of how they utilize spatial and temporal information in eye-tracking protocols. Using this taxonomy, we describe five algorithms that are representative of different classes in the taxonomy and are based on commonly employed techniques. We then evaluate and compare these algorithms with respect to a number of qualitative characteristics. The results of these comparisons offer interesting implications for the use of the various algorithms in future work.},
	urldate = {2024-07-24},
	booktitle = {Proceedings of the 2000 symposium on {Eye} tracking research \& applications},
	publisher = {Association for Computing Machinery},
	author = {Salvucci, Dario D. and Goldberg, Joseph H.},
	month = nov,
	year = {2000},
	pages = {71--78},
}

@article{soleymani2012,
	title = {A {Multimodal} {Database} for {Affect} {Recognition} and {Implicit} {Tagging}},
	volume = {3},
	issn = {1949-3045},
	url = {https://ieeexplore.ieee.org/document/5975141},
	doi = {10.1109/T-AFFC.2011.25},
	abstract = {MAHNOB-HCI is a multimodal database recorded in response to affective stimuli with the goal of emotion recognition and implicit tagging research. A multimodal setup was arranged for synchronized recording of face videos, audio signals, eye gaze data, and peripheral/central nervous system physiological signals. Twenty-seven participants from both genders and different cultural backgrounds participated in two experiments. In the first experiment, they watched 20 emotional videos and self-reported their felt emotions using arousal, valence, dominance, and predictability as well as emotional keywords. In the second experiment, short videos and images were shown once without any tag and then with correct or incorrect tags. Agreement or disagreement with the displayed tags was assessed by the participants. The recorded videos and bodily responses were segmented and stored in a database. The database is made available to the academic community via a web-based system. The collected data were analyzed and single modality and modality fusion results for both emotion recognition and implicit tagging experiments are reported. These results show the potential uses of the recorded modalities and the significance of the emotion elicitation protocol.},
	number = {1},
	urldate = {2024-07-24},
	journal = {IEEE Transactions on Affective Computing},
	author = {Soleymani, Mohammad and Lichtenauer, Jeroen and Pun, Thierry and Pantic, Maja},
	month = jan,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {Cameras, Databases, Emotion recognition, Humans, Videos, Physiology, affective computing., EEG, eye gaze, facial expressions, implicit tagging, pattern classification, physiological signals, Tagging},
	pages = {42--55},
	file = {PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2012_Soleymani et al._A Multimodal Database for Affect Recognition and Implicit Tagging.pdf:application/pdf},
}

@article{abadi2015,
	title = {{DECAF}: {MEG}-{Based} {Multimodal} {Database} for {Decoding} {Affective} {Physiological} {Responses}},
	volume = {6},
	issn = {1949-3045},
	shorttitle = {{DECAF}},
	url = {https://ieeexplore.ieee.org/document/7010926},
	doi = {10.1109/TAFFC.2015.2392932},
	abstract = {In this work, we present DECAF-a multimodal data set for decoding user physiological responses to affective multimedia content. Different from data sets such as DEAP [15] and MAHNOB-HCI [31], DECAF contains (1) brain signals acquired using the Magnetoencephalogram (MEG) sensor, which requires little physical contact with the user's scalp and consequently facilitates naturalistic affective response, and (2) explicit and implicit emotional responses of 30 participants to 40 one-minute music video segments used in [15] and 36 movie clips, thereby enabling comparisons between the EEG versus MEG modalities as well as movie versus music stimuli for affect recognition. In addition to MEG data, DECAF comprises synchronously recorded near-infra-red (NIR) facial videos, horizontal Electrooculogram (hEOG), Electrocardiogram (ECG), and trapezius-Electromyogram (tEMG) peripheral physiological responses. To demonstrate DECAF's utility, we present (i) a detailed analysis of the correlations between participants' self-assessments and their physiological responses and (ii) single-trial classification results for valence, arousal and dominance, with performance evaluation against existing data sets. DECAF also contains time-continuous emotion annotations for movie clips from seven users, which we use to demonstrate dynamic emotion prediction.},
	number = {3},
	urldate = {2024-07-24},
	journal = {IEEE Transactions on Affective Computing},
	author = {Abadi, Mojtaba Khomami and Subramanian, Ramanathan and Kia, Seyed Mostafa and Avesani, Paolo and Patras, Ioannis and Sebe, Nicu},
	month = jul,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {Electroencephalography, Databases, Emotion recognition, Physiology, affective computing, Electrocardiography, Motion pictures, Affective computing, Educational institutions, MEG, single-trial classification, Single-trial classification, user physiological responses, User physiological responses},
	pages = {209--222},
	file = {PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2015_Abadi et al._DECAF MEG-Based Multimodal Database for Decoding Affective Physiological Responses.pdf:application/pdf},
}

@article{koelstra2012,
	title = {{DEAP}: {A} {Database} for {Emotion} {Analysis} ;{Using} {Physiological} {Signals}},
	volume = {3},
	issn = {1949-3045},
	shorttitle = {{DEAP}},
	url = {https://ieeexplore.ieee.org/document/5871728},
	doi = {10.1109/T-AFFC.2011.15},
	abstract = {We present a multimodal data set for the analysis of human affective states. The electroencephalogram (EEG) and peripheral physiological signals of 32 participants were recorded as each watched 40 one-minute long excerpts of music videos. Participants rated each video in terms of the levels of arousal, valence, like/dislike, dominance, and familiarity. For 22 of the 32 participants, frontal face video was also recorded. A novel method for stimuli selection is proposed using retrieval by affective tags from the last.fm website, video highlight detection, and an online assessment tool. An extensive analysis of the participants' ratings during the experiment is presented. Correlates between the EEG signal frequencies and the participants' ratings are investigated. Methods and results are presented for single-trial classification of arousal, valence, and like/dislike ratings using the modalities of EEG, peripheral physiological signals, and multimedia content analysis. Finally, decision fusion of the classification results from different modalities is performed. The data set is made publicly available and we encourage other researchers to use it for testing their own affective state estimation methods.},
	number = {1},
	urldate = {2024-07-24},
	journal = {IEEE Transactions on Affective Computing},
	author = {Koelstra, Sander and Muhl, Christian and Soleymani, Mohammad and Lee, Jong-Seok and Yazdani, Ashkan and Ebrahimi, Touradj and Pun, Thierry and Nijholt, Anton and Patras, Ioannis},
	month = jan,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {Electroencephalography, Databases, Visualization, Videos, Face, Motion pictures, affective computing., EEG, pattern classification, physiological signals, Emotion classification, Multimedia communication, signal processing},
	pages = {18--31},
	file = {PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2012_Koelstra et al._DEAP A Database for Emotion Analysis \;Using Physiological Signals.pdf:application/pdf},
}

@article{katsigiannis2018,
	title = {{DREAMER}: {A} {Database} for {Emotion} {Recognition} {Through} {EEG} and {ECG} {Signals} {From} {Wireless} {Low}-cost {Off}-the-{Shelf} {Devices}},
	volume = {22},
	issn = {2168-2208},
	shorttitle = {{DREAMER}},
	url = {https://ieeexplore.ieee.org/document/7887697},
	doi = {10.1109/JBHI.2017.2688239},
	abstract = {In this paper, we present DREAMER, a multimodal database consisting of electroencephalogram (EEG) and electrocardiogram (ECG) signals recorded during affect elicitation by means of audio-visual stimuli. Signals from 23 participants were recorded along with the participants self-assessment of their affective state after each stimuli, in terms of valence, arousal, and dominance. All the signals were captured using portable, wearable, wireless, low-cost, and off-the-shelf equipment that has the potential to allow the use of affective computing methods in everyday applications. A baseline for participant-wise affect recognition using EEG and ECG-based features, as well as their fusion, was established through supervised classification experiments using support vector machines (SVMs). The self-assessment of the participants was evaluated through comparison with the self-assessments from another study using the same audio-visual stimuli. Classification results for valence, arousal, and dominance of the proposed database are comparable to the ones achieved for other databases that use nonportable, expensive, medical grade devices. These results indicate the prospects of using low-cost devices for affect recognition applications. The proposed database will be made publicly available in order to allow researchers to achieve a more thorough evaluation of the suitability of these capturing devices for affect recognition applications.},
	number = {1},
	urldate = {2024-07-24},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Katsigiannis, Stamos and Ramzan, Naeem},
	month = jan,
	year = {2018},
	note = {Conference Name: IEEE Journal of Biomedical and Health Informatics},
	keywords = {Electroencephalography, Databases, emotion, Emotion recognition, Physiology, Wireless communication, Electrocardiography, EEG, physiological signals, Multimedia communication, affect, affect recognition, ECG, wireless devices},
	pages = {98--107},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\YY945MMQ\\7887697.html:text/html;PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2018_Katsigiannis and Ramzan_DREAMER A Database for Emotion Recognition Through EEG and ECG Signals From Wireless Low-cost Off-t.pdf:application/pdf},
}

@inproceedings{schmidt2018,
	address = {New York, NY, USA},
	series = {{ICMI} '18},
	title = {Introducing {WESAD}, a {Multimodal} {Dataset} for {Wearable} {Stress} and {Affect} {Detection}},
	isbn = {978-1-4503-5692-3},
	url = {https://doi.org/10.1145/3242969.3242985},
	doi = {10.1145/3242969.3242985},
	abstract = {Affect recognition aims to detect a person's affective state based on observables, with the goal to e.g. improve human-computer interaction. Long-term stress is known to have severe implications on wellbeing, which call for continuous and automated stress monitoring systems. However, the affective computing community lacks commonly used standard datasets for wearable stress detection which a) provide multimodal high-quality data, and b) include multiple affective states. Therefore, we introduce WESAD, a new publicly available dataset for wearable stress and affect detection. This multimodal dataset features physiological and motion data, recorded from both a wrist- and a chest-worn device, of 15 subjects during a lab study. The following sensor modalities are included: blood volume pulse, electrocardiogram, electrodermal activity, electromyogram, respiration, body temperature, and three-axis acceleration. Moreover, the dataset bridges the gap between previous lab studies on stress and emotions, by containing three different affective states (neutral, stress, amusement). In addition, self-reports of the subjects, which were obtained using several established questionnaires, are contained in the dataset. Furthermore, a benchmark is created on the dataset, using well-known features and standard machine learning methods. Considering the three-class classification problem ( baseline vs. stress vs. amusement ), we achieved classification accuracies of up to 80\%,. In the binary case ( stress vs. non-stress ), accuracies of up to 93\%, were reached. Finally, we provide a detailed analysis and comparison of the two device locations ( chest vs. wrist ) as well as the different sensor modalities.},
	urldate = {2024-07-24},
	booktitle = {Proceedings of the 20th {ACM} {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Schmidt, Philip and Reiss, Attila and Duerichen, Robert and Marberger, Claus and Van Laerhoven, Kristof},
	month = oct,
	year = {2018},
	pages = {400--408},
}

@article{howard2021,
	title = {A meta-analysis of virtual reality training programs},
	volume = {121},
	issn = {0747-5632},
	url = {https://doi.org/10.1016/j.chb.2021.106808},
	doi = {10.1016/j.chb.2021.106808},
	number = {C},
	urldate = {2024-07-24},
	journal = {Comput. Hum. Behav.},
	author = {Howard, Matt C. and Gutworth, Melissa B. and Jacobs, Rick R.},
	month = aug,
	year = {2021},
}

@article{ekman1986,
	title = {A new pan-cultural facial expression of emotion},
	volume = {10},
	copyright = {http://www.springer.com/tdm},
	issn = {0146-7239, 1573-6644},
	url = {http://link.springer.com/10.1007/BF00992253},
	doi = {10.1007/BF00992253},
	language = {en},
	number = {2},
	urldate = {2024-07-24},
	journal = {Motivation and Emotion},
	author = {Ekman, Paul and Friesen, Wallace V.},
	month = jun,
	year = {1986},
	pages = {159--168},
	file = {PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\1986_Ekman and Friesen_A new pan-cultural facial expression of emotion.pdf:application/pdf},
}

@article{lim2016,
	title = {Cultural differences in emotion: differences in emotional arousal level between the {East} and the {West}},
	volume = {5},
	issn = {2213-4220},
	shorttitle = {Cultural differences in emotion},
	doi = {10.1016/j.imr.2016.03.004},
	abstract = {Whether emotion is universal or social is a recurrent issue in the history of emotion study among psychologists. Some researchers view emotion as a universal construct, and that a large part of emotional experience is biologically based. However, emotion is not only biologically determined, but is also influenced by the environment. Therefore, cultural differences exist in some aspects of emotions, one such important aspect of emotion being emotional arousal level. All affective states are systematically represented as two bipolar dimensions, valence and arousal. Arousal level of actual and ideal emotions has consistently been found to have cross-cultural differences. In Western or individualist culture, high arousal emotions are valued and promoted more than low arousal emotions. Moreover, Westerners experience high arousal emotions more than low arousal emotions. By contrast, in Eastern or collectivist culture, low arousal emotions are valued more than high arousal emotions. Moreover, people in the East actually experience and prefer to experience low arousal emotions more than high arousal emotions. Mechanism of these cross-cultural differences and implications are also discussed.},
	language = {eng},
	number = {2},
	journal = {Integrative Medicine Research},
	author = {Lim, Nangyeon},
	month = jun,
	year = {2016},
	pmid = {28462104},
	pmcid = {PMC5381435},
	keywords = {collectivist culture, cultural difference, emotional arousal level, individualist culture},
	pages = {105--109},
	file = {PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2016_Lim_Cultural differences in emotion differences in emotional arousal level between the East and the Wes.pdf:application/pdf},
}

@article{tsai2007,
	title = {Ideal affect: {Cultural} causes and behavioral consequences},
	volume = {2},
	issn = {1745-6924},
	shorttitle = {Ideal affect},
	doi = {10.1111/j.1745-6916.2007.00043.x},
	abstract = {Most research focuses on actual affect, or the affective states that people actually feel. In this article, I demonstrate the importance and utility of studying ideal affect, or the affective states that people ideally want to feel. First, I define ideal affect and describe the cultural causes and behavioral consequences of ideal affect. To illustrate these points, I compare American and East Asian cultures, which differ in their valuation of high-arousal positive affective states (e.g., excitement, enthusiasm) and low-arousal positive affective states (e.g., calm, peacefulness). I then introduce affect valuation theory, which integrates ideal affect with current models of affect and emotion and, in doing so, provides a new framework for understanding how cultural and temperamental factors may shape affect and behavior. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	journal = {Perspectives on Psychological Science},
	author = {Tsai, Jeanne L.},
	year = {2007},
	note = {Place: United Kingdom
Publisher: Blackwell Publishing},
	keywords = {Asians, Behavior, Causality, Consequence, Cross Cultural Differences, Emotional States, Idealism, Physiological Arousal, Sociocultural Factors, Values},
	pages = {242--259},
	file = {Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\9X9HMMBK\\2007-13241-002.html:text/html},
}

@article{ekman1999,
	title = {A few can catch a liar},
	volume = {10},
	issn = {1467-9280},
	doi = {10.1111/1467-9280.00147},
	abstract = {Research suggests that most people cannot tell from demeanor when others are lying. Such poor performance is typical not only of lay people but also of most professionals concerned with lying. In this study, three professional groups with special interest or skill in deception, two law-enforcement groups (made up of 23 federal officers [mean age 40.8 yrs], 43 sheriffs [mean age 40.7 yrs], 84 federal judges [mean age 52.4 yrs], and 36 mixed law-enforcement officers [mean age 34.9 yrs]) and a select group of clinical psychologists (made up of 107 deception- interested clinical psychologists [mean age 49.6 yrs], 209 regular clinical psychologists [mean age 49.1 yrs], and 125 academic psychologists [mean age 39.2 yrs]), obtained high accuracy in judging videotapes of people who were lying or telling the truth about their opinions. These findings strengthen earlier evidence that some professional lie catchers are highly accurate, and that behavioral clues to lying are detectable in real time. This study also provides the first evidence that some psychologists can achieve high accuracy in catching lies. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	journal = {Psychological Science},
	author = {Ekman, Paul and O'Sullivan, Maureen and Frank, Mark G.},
	year = {1999},
	note = {Place: United Kingdom
Publisher: Blackwell Publishing},
	keywords = {Clinical Psychologists, Deception, Dishonesty, Police Personnel},
	pages = {263--266},
	file = {Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\Y66E5Y5N\\1999-05631-018.html:text/html},
}

@article{jordan2019,
	title = {A test of the micro‐expressions training tool: {Does} it improve lie detection?},
	volume = {16},
	issn = {1544-4767},
	shorttitle = {A test of the micro‐expressions training tool},
	doi = {10.1002/jip.1532},
	abstract = {The purpose of the study was to examine the effectiveness of the micro‐expressions training tool (METT) in identifying and using micro‐expressions to improve lie detection. Participants (n = 90) were randomly assigned to receive training in micro‐expressions recognition, a bogus control training, or no training. All participants made veracity judgements of five randomly selected videos of targets providing deceptive or truthful statements. With the use of the Bayesian analyses, we found that the METT group did not outperform those in the bogus training and no training groups. Further, overall accuracy was slightly below chance. Implications of these results are discussed. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
	number = {3},
	journal = {Journal of Investigative Psychology and Offender Profiling},
	author = {Jordan, Sarah and Brimbal, Laure and Wallace, D. Brian and Kassin, Saul M. and Hartwig, Maria and Street, Chris N. H.},
	year = {2019},
	note = {Place: US
Publisher: John Wiley \& Sons},
	keywords = {Training, Facial Expressions, Deception},
	pages = {222--235},
	file = {PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2019_Jordan et al._A test of the micro‐expressions training tool Does it improve lie detection.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\WF7KA3EU\\2019-49472-001.html:text/html},
}

@article{zeng2009,
	title = {A survey of affect recognition methods: audio, visual, and spontaneous expressions},
	volume = {31},
	issn = {0162-8828},
	shorttitle = {A survey of affect recognition methods},
	doi = {10.1109/TPAMI.2008.52},
	abstract = {Automated analysis of human affective behavior has attracted increasing attention from researchers in psychology, computer science, linguistics, neuroscience, and related disciplines. However, the existing methods typically handle only deliberately displayed and exaggerated expressions of prototypical emotions despite the fact that deliberate behaviour differs in visual appearance, audio profile, and timing from spontaneously occurring behaviour. To address this problem, efforts to develop algorithms that can process naturally occurring human affective behaviour have recently emerged. Moreover, an increasing number of efforts are reported toward multimodal fusion for human affect analysis including audiovisual fusion, linguistic and paralinguistic fusion, and multi-cue visual fusion based on facial expressions, head movements, and body gestures. This paper introduces and surveys these recent advances. We first discuss human emotion perception from a psychological perspective. Next we examine available approaches to solving the problem of machine understanding of human affective behavior, and discuss important issues like the collection and availability of training and test data. We finally outline some of the scientific and engineering challenges to advancing human affect sensing technology.},
	language = {eng},
	number = {1},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Zeng, Zhihong and Pantic, Maja and Roisman, Glenn I. and Huang, Thomas S.},
	month = jan,
	year = {2009},
	pmid = {19029545},
	keywords = {Algorithms, Artificial Intelligence, Emotions, Affect, Facial Expression, Monitoring, Physiologic, Pattern Recognition, Automated, Sound Spectrography},
	pages = {39--58},
}

@book{picard1997,
	title = {Affective {Computing}},
	isbn = {978-0-262-28158-4},
	url = {https://direct.mit.edu/books/book/4296/Affective-Computing},
	abstract = {According to Rosalind Picard, if we want computers to be genuinely intelligent and to interact naturally with us, we must give computers the ability to recognize, understand, even to have and express emotions.
            The latest scientific findings indicate that emotions play an essential role in decision making, perception, learning, and more—that is, they influence the very mechanisms of rational thinking. Not only too much, but too little emotion can impair decision making. According to Rosalind Picard, if we want computers to be genuinely intelligent and to interact naturally with us, we must give computers the ability to recognize, understand, even to have and express emotions.
            Part 1 of this book provides the intellectual framework for affective computing. It includes background on human emotions, requirements for emotionally intelligent computers, applications of affective computing, and moral and social questions raised by the technology. Part 2 discusses the design and construction of affective computers. Although this material is more technical than that in Part 1, the author has kept it less technical than typical scientific publications in order to make it accessible to newcomers. Topics in Part 2 include signal-based representations of emotions, human affect recognition as a pattern recognition and learning problem, recent and ongoing efforts to build models of emotion for synthesizing emotions in computers, and the new application area of affective wearable computers.},
	language = {en},
	urldate = {2024-07-24},
	publisher = {The MIT Press},
	author = {Picard, Rosalind W.},
	month = sep,
	year = {1997},
	doi = {10.7551/mitpress/1140.001.0001},
}

@article{becattini2022a,
	title = {Understanding {Human} {Reactions} {Looking} at {Facial} {Microexpressions} {With} an {Event} {Camera}},
	volume = {18},
	issn = {1941-0050},
	url = {https://ieeexplore.ieee.org/abstract/document/9844855},
	doi = {10.1109/TII.2022.3195063},
	abstract = {With the establishment of Industry 4.0, machines are now required to interact with workers. By observing biometrics they can assess if humans are authorized, or mentally and physically fit to work. Understanding body language, makes human–machine interaction more natural, secure, and effective. Nonetheless, traditional cameras have limitations; low frame rate and dynamic range hinder a comprehensive human understanding. This poses a challenge, since faces undergo frequent instantaneous microexpressions. In addition, this is privacy-sensitive information that must be protected. We propose to model expressions with event cameras, bio-inspired vision sensors that have found application within the Industry 4.0 scope. They capture motion at millisecond rates and work under challenging conditions like low illumination and highly dynamic scenes. Such cameras are also privacy-preserving, making them extremely interesting for industry. We show that using event cameras, we can understand human reactions by only observing facial expressions. Comparison with red-green-blue (RGB)-based modeling demonstrates improved effectiveness and robustness.},
	number = {12},
	urldate = {2024-07-24},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Becattini, Federico and Palai, Federico and Bimbo, Alberto Del},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Industrial Informatics},
	keywords = {Computer vision, Cameras, Biometrics, emotion recognition, event camera, Industry 4.0, microexpressions, neuromorphic sensor, privacy-preserving, Neuromorphics, Biometrics (access control), Faces, human–machine interfaces, Robot vision systems, Sensors},
	pages = {9112--9121},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\V4H63XAE\\9844855.html:text/html;PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2022_Becattini et al._Understanding Human Reactions Looking at Facial Microexpressions With an Event Camera.pdf:application/pdf},
}

@article{cronin2024,
	title = {Feasibility of {OpenPose} markerless motion analysis in a real athletics competition},
	volume = {5},
	issn = {2624-9367},
	url = {https://www.frontiersin.org/journals/sports-and-active-living/articles/10.3389/fspor.2023.1298003/full},
	doi = {10.3389/fspor.2023.1298003},
	abstract = {{\textless}p{\textgreater}This study tested the performance of OpenPose on footage collected by two cameras at 200 Hz from a real-life competitive setting by comparing it with manually analyzed data in SIMI motion. The same take-off recording from the men's Long Jump finals at the 2017 World Athletics Championships was used for both approaches (markerless and manual) to reconstruct the 3D coordinates from each of the camera's 2D coordinates. Joint angle and Centre of Mass (COM) variables during the final step and take-off phase of the jump were determined. Coefficients of Multiple Determinations (CMD) for joint angle waveforms showed large variation between athletes with the knee angle values typically being higher (take-off leg: 0.727 ± 0.242; swing leg: 0.729 ± 0.190) than those for hip (take-off leg: 0.388 ± 0.193; swing leg: 0.370 ± 0.227) and ankle angle (take-off leg: 0.247 ± 0.172; swing leg: 0.155 ± 0.228). COM data also showed considerable variation between athletes and parameters, with position (0.600 ± 0.322) and projection angle (0.658 ± 0.273) waveforms generally showing better agreement than COM velocity (0.217 ± 0.241). Agreement for discrete data was generally poor with high random error for joint kinematics and COM parameters at take-off and an average ICC across variables of 0.17. The poor agreement statistics and a range of unrealistic values returned by the pose estimation underline that OpenPose is not suitable for in-competition performance analysis in events such as the long jump, something that manual analysis still achieves with high levels of accuracy and reliability.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-07-30},
	journal = {Frontiers in Sports and Active Living},
	author = {Cronin, Neil J. and Walker, Josh and Tucker, Catherine B. and Nicholson, Gareth and Cooke, Mark and Merlino, Stéphane and Bissas, Athanassios},
	month = jan,
	year = {2024},
	note = {Publisher: Frontiers},
	keywords = {Artificial Intelligence 3, kinematics 4, markerless tracking 2, motion capture 1, sprinting 5},
	file = {PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Body-Tracking\\2024_Cronin et al._Feasibility of OpenPose markerless motion analysis in a real athletics competition.pdf:application/pdf},
}

@article{yan2014,
	title = {{CASME} {II}: {An} {Improved} {Spontaneous} {Micro}-{Expression} {Database} and the {Baseline} {Evaluation}},
	volume = {9},
	shorttitle = {{CASME} {II}},
	doi = {10.1371/journal.pone.0086041},
	abstract = {A robust automatic micro-expression recognition system would have broad applications in national safety, police interrogation, and clinical diagnosis. Developing such a system requires high quality databases with sufficient training samples which are currently not available. We reviewed the previously developed micro-expression databases and built an improved one (CASME II), with higher temporal resolution (200 fps) and spatial resolution (about 280×340 pixels on facial area). We elicited participants' facial expressions in a well-controlled laboratory environment and proper illumination (such as removing light flickering). Among nearly 3000 facial movements, 247 micro-expressions were selected for the database with action units (AUs) and emotions labeled. For baseline evaluation, LBP-TOP and SVM were employed respectively for feature extraction and classifier with the leave-one-subject-out cross-validation method. The best performance is 63.41\% for 5-class classification.},
	journal = {PloS one},
	author = {Yan, Wen-Jing and Li, Xiaobai and Wang, Su-Jing and Zhao, Guoying and Liu, Yong-Jin and Chen, Yu-Hsin and Fu, Xiaolan},
	month = jan,
	year = {2014},
	pages = {e86041},
	file = {PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2014_Yan et al._CASME II An Improved Spontaneous Micro-Expression Database and the Baseline Evaluation.pdf:application/pdf},
}

@article{bell2024,
	title = {Advances in the use of virtual reality to treat mental health conditions},
	volume = {3},
	copyright = {2024 Springer Nature America, Inc.},
	issn = {2731-0574},
	url = {https://www.nature.com/articles/s44159-024-00334-9},
	doi = {10.1038/s44159-024-00334-9},
	abstract = {Virtual reality (VR) has emerged as a promising tool in the field of mental health. Central to this technology are immersive environments, which enable exposure to highly controlled virtual experiences that feel real. In this Review, we elaborate on the active elements of immersive experiences and how VR-based treatments work. We provide an overview of developments in the use of VR to treat mental health conditions (anxiety, psychotic symptoms, post-traumatic stress, eating disorders, depression and stress management) with a focus on the core mechanisms that drive effective interventions. Artificial intelligence, biofeedback and gamification are emerging areas of development, and we discuss how they might enhance the accessibility, engagement and effectiveness of psychological treatments. Conducting rigorous studies with user-centred designs in diverse populations is a key research priority. As the use of VR in mental health continues to evolve, addressing ethical and implementation considerations is critical for ensuring ongoing treatment improvements.},
	language = {en},
	number = {8},
	urldate = {2024-08-19},
	journal = {Nature Reviews Psychology},
	author = {Bell, Imogen H. and Pot-Kolder, Roos and Rizzo, Albert and Rus-Calafell, Mar and Cardi, Valentina and Cella, Matteo and Ward, Thomas and Riches, Simon and Reinoso, Martin and Thompson, Andrew and Alvarez-Jimenez, Mario and Valmaggia, Lucia},
	month = aug,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Technology, Psychiatric disorders, Psychology},
	pages = {552--567},
	file = {2024_Bell et al._Advances in the use of virtual reality to treat mental health conditions:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2024_Bell et al._Advances in the use of virtual reality to treat mental health conditions.pdf:application/pdf},
}

@article{2022a,
	title = {Probing an intelligent predictive maintenance approach with deep learning and augmented reality for machine tools in {IoT}-enabled manufacturing},
	volume = {77},
	issn = {0736-5845},
	url = {https://www.sciencedirect.com/science/article/pii/S073658452200045X},
	doi = {10.1016/j.rcim.2022.102357},
	abstract = {In the Industry 4.0 era, the number and complexity of machine tools are both increased, which is prone to cause malfunctions and downtime in the manuf…},
	language = {en-US},
	urldate = {2024-08-21},
	journal = {Robotics and Computer-Integrated Manufacturing},
	month = oct,
	year = {2022},
	note = {Publisher: Pergamon},
	pages = {102357},
}

@phdthesis{armengolurpi2023,
	address = {United States -- Massachusetts},
	type = {Ph.{D}.},
	title = {Capturing {Tacit} {Knowledge} of {Experts} {Through} the {Study} of {Visual} {Attention}: {Applications} for {Human} {Expertise} and {AI}},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	shorttitle = {Capturing {Tacit} {Knowledge} of {Experts} {Through} the {Study} of {Visual} {Attention}},
	url = {https://www.proquest.com/docview/3031006792/abstract/137F0EAC92FA48A9PQ/1},
	abstract = {Tacit or implicit knowledge is know-how that humans cannot convey explicitly; it is difficult to verbalize, and hence, it is challenging to transfer to others in words. Tacit knowledge is usually gained from experience and internalized unconsciously through implicit learning. Since tacit knowledge is not consciously accessible, it is commonly seen as a “mysterious" part of expertise that can only be transferred from one person to another through close interaction, coaching, mentoring, and observation of expert behavior. Examples of daily activities based on tacit knowledge include riding a bike, recognizing a face, writing a persuasive thesis or speaking a native language. This thesis explores new methods for tacit knowledge extraction using visual attention-based human-computer interfaces.
Earlier studies suggest that eye gaze is particularly suited for studying the unconscious component of expertise. For this reason, this research focuses on developing new interfaces that track the visual attention of experts while performing tasks in which they excel. This thesis is divided into two main sections. In the first part, we develop novel human-computer interfaces suited to track visual attention and that enhance existing interfaces based on gaze tracking alone. We do this by exploiting brain activity in addition to eye gaze. First, we leverage neural mechanisms of visual attention to improve the accuracy of a commercial eye tracker through the analysis of electroencephalography (EEG) waves. Our hybrid system combines EEG and eye-tracking modalities to overcome the accuracy limitations of the gaze tracker alone. We integrate EEG and gaze data to efficiently exploit their complementary strengths by driving a Bayesian probabilistic decoder that estimates the region in the visual field gazed at by the user. This demonstrates that the intrinsic accuracy limitations of camera-based eye trackers can be corrected with the integration of EEG data. Then, we show why visual attention and gaze can be decoupled by developing an interface that tracks peripheral attention using EEG waves. Our novel approach can detect peripheral (or covert) spatial attention by using single-frequency phase-coded stimuli that elicit the corresponding Steady-State Visually Evoked Potentials (SSVEPs). This opens opportunities for attention-tracking applications with largely increased number of targets in the visual field.
In the second part of this thesis, we exploit our previously developed interfaces to track the visual attention of experts performing image classification tasks. First, we create images with a hidden asymmetry that is not consciously (or explicitly) recognized by the experts. However, their visual attention patterns reveal that the asymmetry is unconsciously internalized because the attention metrics are skewed towards the image regions most relevant for categorization. This demonstrates that we can capture insights about experts' tacit knowledge by tracking their visual attention. We then show that the expertise of subjects who receive feedback extracted from their own attention patterns is significantly enhanced compared to subjects who did not. We refer to this as cognitive reinforcement. This research opens the door to new ways in which human expertise can be enhanced, exploited, and transferred. Finally, we utilize human attention maps captured during image exploration and labeling to feed a CNN-based image classification model. We demonstrate that when few images are available for training, the model fed with human attention maps, in addition to images and labels, significantly outperforms the baseline model. These results illustrate that experts' tacit knowledge can be exploited to enhance the performance of human experts as well as AI systems.},
	urldate = {2024-08-21},
	school = {Massachusetts Institute of Technology},
	author = {Armengol Urpí, Àlex},
	year = {2023},
	note = {ISBN: 9798381955170},
	keywords = {Electroencephalography, Artificial intelligence, Cognitive reinforcement, Implicit learning, Mechanical engineering},
	file = {2023_Armengol Urpí_Capturing Tacit Knowledge of Experts Through the Study of Visual Attention Applications for Human E:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2023_Armengol Urpí_Capturing Tacit Knowledge of Experts Through the Study of Visual Attention Applications for Human E.pdf:application/pdf},
}

@article{gjoreski2022,
	title = {Facial {EMG} sensing for monitoring affect using a wearable device},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-21456-1},
	doi = {10.1038/s41598-022-21456-1},
	abstract = {Using a novel wearable surface electromyography (sEMG), we investigated induced affective states by measuring the activation of facial muscles traditionally associated with positive (left/right orbicularis and left/right zygomaticus) and negative expressions (the corrugator muscle). In a sample of 38 participants that watched 25 affective videos in a virtual reality environment, we found that each of the three variables examined—subjective valence, subjective arousal, and objective valence measured via the validated video types (positive, neutral, and negative)—sEMG amplitude varied significantly depending on video content. sEMG aptitude from “positive muscles” increased when participants were exposed to positively valenced stimuli compared with stimuli that was negatively valenced. In contrast, activation of “negative muscles” was elevated following exposure to negatively valenced stimuli compared with positively valenced stimuli. High arousal videos increased muscle activations compared to low arousal videos in all the measured muscles except the corrugator muscle. In line with previous research, the relationship between sEMG amplitude as a function of subjective valence was V-shaped.},
	language = {en},
	number = {1},
	urldate = {2024-08-21},
	journal = {Scientific Reports},
	author = {Gjoreski, Martin and Kiprijanovska, Ivana and Stankoski, Simon and Mavridou, Ifigeneia and Broulidakis, M. John and Gjoreski, Hristijan and Nduka, Charles},
	month = oct,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Information technology},
	pages = {16876},
	file = {2022_Gjoreski et al._Facial EMG sensing for monitoring affect using a wearable device.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\moocooc\\2022_Gjoreski et al._Facial EMG sensing for monitoring affect using a wearable device.pdf:application/pdf},
}

@article{serradilla2022,
	title = {Deep learning models for predictive maintenance: a survey, comparison, challenges and prospects},
	volume = {52},
	issn = {1573-7497},
	shorttitle = {Deep learning models for predictive maintenance},
	url = {https://doi.org/10.1007/s10489-021-03004-y},
	doi = {10.1007/s10489-021-03004-y},
	abstract = {Given the growing amount of industrial data in the 4th industrial revolution, deep learning solutions have become popular for predictive maintenance (PdM) tasks, which involve monitoring assets to anticipate their requirements and optimise maintenance tasks. However, given the large variety of such tasks in the literature, choosing the most suitable architecture for each use case is difficult. This work aims to facilitate this task by reviewing various state-of-the-art deep learning (DL) architectures and analysing how well they integrate with predictive maintenance stages to meet industrial companies’ requirements from a PdM perspective. This review includes a self-organising map (SOM), one-class neural network (OC-NN) and generative techniques. This article explains how to adapt DL architectures to facilitate data variability handling, model adaptability and ensemble learning, all of which are characteristics relevant to industrial requirements. In addition, this review compares the results of state-of-the-art DL architectures on a publicly available dataset to facilitate reproducibility and replicability, enabling comparisons. Furthermore, this work covers the mitigation step with deep learning models, the final PdM stage that is essential for implementing PdM systems. Moreover, state-of-the-art deep learning architectures are categorised, analysed and compared; their industrial applications are presented; and an explanation of how to combine different architectures in a solution is presented that addresses their gaps. Finally, open challenges and possible future research paths are presented and supported in this review, and current research trends are identified.},
	language = {en},
	number = {10},
	urldate = {2024-08-27},
	journal = {Applied Intelligence},
	author = {Serradilla, Oscar and Zugasti, Ekhi and Rodriguez, Jon and Zurutuza, Urko},
	month = aug,
	year = {2022},
	keywords = {Deep learning, Industry 4.0, Artificial Intelligence, Survey, Data-driven, Predictive maintenance, Review},
	pages = {10934--10964},
	file = {2022_Serradilla et al._Deep learning models for predictive maintenance a survey, comparison, challenges and prospects.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2022_Serradilla et al._Deep learning models for predictive maintenance a survey, comparison, challenges and prospects.pdf:application/pdf},
}

@article{2024a,
	title = {Improving deep learning with prior knowledge and cognitive models: {A} survey on enhancing explainability, adversarial robustness and zero-shot learning},
	volume = {84},
	issn = {1389-0417},
	shorttitle = {Improving deep learning with prior knowledge and cognitive models},
	url = {https://www.sciencedirect.com/science/article/pii/S1389041723001225},
	doi = {10.1016/j.cogsys.2023.101188},
	abstract = {We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, eXplainable Artificial Inte…},
	language = {en-US},
	urldate = {2024-08-27},
	journal = {Cognitive Systems Research},
	month = mar,
	year = {2024},
	note = {Publisher: Elsevier},
	pages = {101188},
	file = {Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\VUVZGR2U\\S1389041723001225.html:text/html},
}

@inproceedings{sharma2023,
	address = {Singapore},
	title = {Machine {Learning} {Techniques} in {Data} {Fusion}: {A} {Review}},
	isbn = {978-981-99-2100-3},
	shorttitle = {Machine {Learning} {Techniques} in {Data} {Fusion}},
	doi = {10.1007/978-981-99-2100-3_31},
	abstract = {In data fusion, various data sources are unified to get more data than a single source. Due to the increasing amount of real-time data and increasing data complexity, the traditional methods are not enough in today’s world, therefore the dependency on machine learning algorithms is increasing for data fusion. Many machine learning methods have been studied before but a structural review of machine learning algorithms in data fusion is yet to come. Therefore, it is vital to analyze and sum up the various machine learning techniques used in data fusion. This paper provides a structural analysis of machine learning techniques used in data fusion. Firstly, the paper explains the framework of machine learning and data fusion followed by various standards as well as essential machine learning techniques which are used in most of the paper, and at last, a quick introduction to all typical machine learning techniques are given for better understanding.},
	language = {en},
	booktitle = {Communication and {Intelligent} {Systems}},
	publisher = {Springer Nature},
	author = {Sharma, Muskan and Kushwaha, Priyanka and Kumari, Pragati and Kumari, Pushpanjali and Yadav, Richa},
	editor = {Sharma, Harish and Shrivastava, Vivek and Bharti, Kusum Kumari and Wang, Lipo},
	year = {2023},
	pages = {391--405},
}

@article{moleda2023,
	title = {From {Corrective} to {Predictive} {Maintenance}—{A} {Review} of {Maintenance} {Approaches} for the {Power} {Industry}},
	volume = {23},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/23/13/5970},
	doi = {10.3390/s23135970},
	abstract = {Appropriate maintenance of industrial equipment keeps production systems in good health and ensures the stability of production processes. In specific production sectors, such as the electrical power industry, equipment failures are rare but may lead to high costs and substantial economic losses not only for the power plant but for consumers and the larger society. Therefore, the power production industry relies on a variety of approaches to maintenance tasks, ranging from traditional solutions and engineering know-how to smart, AI-based analytics to avoid potential downtimes. This review shows the evolution of maintenance approaches to support maintenance planning, equipment monitoring and supervision. We present older techniques traditionally used in maintenance tasks and those that rely on IT analytics to automate tasks and perform the inference process for failure detection. We analyze prognostics and health-management techniques in detail, including their requirements, advantages and limitations. The review focuses on the power-generation sector. However, some of the issues addressed are common to other industries. The article also presents concepts and solutions that utilize emerging technologies related to Industry 4.0, touching on prescriptive analysis, Big Data and the Internet of Things. The primary motivation and purpose of the article are to present the existing practices and classic methods used by engineers, as well as modern approaches drawing from Artificial Intelligence and the concept of Industry 4.0. The summary of existing practices and the state of the art in the area of predictive maintenance provides two benefits. On the one hand, it leads to improving processes by matching existing tools and methods. On the other hand, it shows researchers potential directions for further analysis and new developments.},
	language = {en},
	number = {13},
	urldate = {2024-08-27},
	journal = {Sensors},
	author = {Molęda, Marek and Małysiak-Mrozek, Bożena and Ding, Weiping and Sunderam, Vaidy and Mrozek, Dariusz},
	month = jan,
	year = {2023},
	note = {Number: 13
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Industry 4.0, energy production, power industry, predictive maintenance (PdM), prognostics and health management (PHM), smart maintenance},
	pages = {5970},
	file = {2023_Molęda et al._From Corrective to Predictive Maintenance—A Review of Maintenance Approaches for the Power Industry:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2023_Molęda et al._From Corrective to Predictive Maintenance—A Review of Maintenance Approaches for the Power Industry.pdf:application/pdf},
}

@article{leokumar2019,
	title = {Knowledge-based expert system in manufacturing planning: state-of-the-art review},
	volume = {57},
	issn = {0020-7543},
	shorttitle = {Knowledge-based expert system in manufacturing planning},
	url = {https://doi.org/10.1080/00207543.2018.1424372},
	doi = {10.1080/00207543.2018.1424372},
	abstract = {In this paper, an effort has been made for intense review on Knowledge-Based Expert System (KB-ES) applications in manufacturing planning. Uniqueness of the present review work is addressed in terms of analysis on published review articles and their review gap. Research works exemplified between 1981 and 2016 were reviewed in terms of ES application in handling product variety, execution of process planning activities, machining, tool selection, tool design, welding, advanced manufacturing, product development. A statistical analysis was carried out in relation with number of publications, domain-specific area and their percentage contribution. It was inferred that, most of the work focused on ES applications related to tool design and machining apart from execution of various process planning activities. Future research can focus on the development frame-based, object oriented-based, ontology-based knowledge representation in order to develop robust system in decision-making for handling complex engineering problem. ES applications can be extended to field of micro fabrication, machine tool development and integrated system development from design to manufacturing.},
	number = {15-16},
	urldate = {2024-08-27},
	journal = {International Journal of Production Research},
	author = {Leo Kumar, S.P.},
	month = aug,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00207543.2018.1424372},
	keywords = {artificial intelligence, CAPP, expert system, knowledge-based system, manufacturing},
	pages = {4766--4790},
	file = {2019_Leo Kumar_Knowledge-based expert system in manufacturing planning state-of-the-art review:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2019_Leo Kumar_Knowledge-based expert system in manufacturing planning state-of-the-art review.pdf:application/pdf},
}

@article{donato2022,
	title = {A {Survey} on {Audio}-{Video} {Based} {Defect} {Detection} {Through} {Deep} {Learning} in {Railway} {Maintenance}},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/abstract/document/9795283},
	doi = {10.1109/ACCESS.2022.3183102},
	abstract = {Within Artificial Intelligence, Deep Learning (DL) represents a paradigm that has been showing unprecedented performance in image and audio processing by supporting or even replacing humans in defect and anomaly detection. The railway sector is expected to benefit from DL applications, especially in predictive maintenance applications, where smart audio and video sensors can be leveraged yet kept distinct from safety-critical functions. Such separation is crucial, as it allows for improving system dependability with no impact on its safety certification. This is further supported by the development of DL in other transportation domains, such as automotive and avionics, opening for knowledge transfer opportunities and highlighting the potential of such a paradigm in railways. In order to summarize the recent state-of-the-art while inquiring about future opportunities, this paper reviews DL approaches for the analysis of data generated by acoustic and visual sensors in railway maintenance applications that have been published until August 31st, 2021. In this paper, the current state of the research is investigated and evaluated using a structured and systematic method, in order to highlight promising approaches and successful applications, as well as to identify available datasets, current limitations, open issues, challenges, and recommendations about future research directions.},
	urldate = {2024-08-27},
	journal = {IEEE Access},
	author = {Donato, Lorenzo De and Flammini, Francesco and Marrone, Stefano and Mazzariello, Claudio and Nardone, Roberto and Sansone, Carlo and Vittorini, Valeria},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {Computer vision, Task analysis, Sensors, machine learning, CNN, fault detection, Focusing, inspection, Inspection, Maintenance engineering, Rail transportation, Rails, smart railways},
	pages = {65376--65400},
	file = {2022_Donato et al._A Survey on Audio-Video Based Defect Detection Through Deep Learning in Railway Maintenance:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2022_Donato et al._A Survey on Audio-Video Based Defect Detection Through Deep Learning in Railway Maintenance.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\5THVLUZ3\\9795283.html:text/html},
}

@inproceedings{aromaa2016,
	address = {New York, NY, USA},
	series = {{AcademicMindtrek} '16},
	title = {Use of wearable and augmented reality technologies in industrial maintenance work},
	isbn = {978-1-4503-4367-1},
	url = {https://dl.acm.org/doi/10.1145/2994310.2994321},
	doi = {10.1145/2994310.2994321},
	abstract = {Industrial maintenance is an increasingly complex and knowledge intensive field. Although new technologies in maintenance have been studied extensively, their usage is still lacking in the industry. We have studied knowledge-sharing solutions using augmented reality (AR) and wearable technologies in actual industry cases to find out if maintenance technicians find them useful and usable in their everyday work. Two test cases were included: the use of a wearable system consisting of three devices in the crane industry, and the use of AR guidance in the marine industry. In both cases two maintenance technicians tested the technologies and data were collected using questionnaires, interviews and observation. The maintenance technicians were positive towards the use of these technologies in their work. However, some practical issues were raised concerning the simultaneous use of multiple devices and the placement of the devices. A more system-level approach to designing wearable and AR technologies could be applied to ensure their utility in the field. Findings from this study can be used when designing and implementing wearable and AR technologies in maintenance, but also in other industry domains like the manufacturing industry.},
	urldate = {2024-08-27},
	booktitle = {Proceedings of the 20th {International} {Academic} {Mindtrek} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Aromaa, Susanna and Aaltonen, Iina and Kaasinen, Eija and Elo, Joona and Parkkinen, Ilari},
	month = oct,
	year = {2016},
	pages = {235--242},
	file = {2016_Aromaa et al._Use of wearable and augmented reality technologies in industrial maintenance work.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2016_Aromaa et al._Use of wearable and augmented reality technologies in industrial maintenance work.pdf:application/pdf},
}

@article{kaasinen2018,
	title = {Mobile {Service} {Technician} 4.0 – {Knowledge}-{Sharing} {Solutions} for {Industrial} {Field} {Maintenance}},
	issn = {22832998},
	url = {https://ixdea.org/38_1/},
	doi = {10.55612/s-5002-038-001},
	abstract = {With the fourth industrial revolution, Industry 4.0, many work tasks are becoming knowledge intensive. At the forefront of the change are workers that are already mobile, working in the field with customers. We describe the human-centred design process that resulted in the Mobile Service Technician 4.0 concept. The concept illustrates how industrial field maintenance work could benefit from knowledge-sharing solutions based on Industry 4.0. The solutions utilize industrial internet, virtual and augmented reality as well as wearable technologies to improve mobile service technicians’ daily work performance and work satisfaction. The Mobile Service Technician 4.0 concept illustrates the user experience of future maintenance work: feeling competent, feeling connected to the work community, and feeling of success and achievement by being better prepared for maintenance visits, getting situationally relevant support in maintenance operations, sharing knowledge with peers, and making maintenance reports effortless.},
	language = {en},
	number = {38},
	urldate = {2024-08-27},
	journal = {Interaction Design and Architecture(s)},
	author = {Kaasinen, Eija and Aromaa, Susanna and Väätänen, Antti and Mäkelä, Ville and Hakulinen, Jaakko and Keskinen, Tuuli and Elo, Joona and Siltanen, Sanni and Rauhala, Ville and Aaltonen, Iina and Hella, Juho and Honkamaa, Petri and Leppä, Mikael and Niemelä, Antti and Parviainen, Juha and Saarinen, Santeri and Turunen, Markku and Törnqvist, Jouni and Valtonen, Juha and Woodward, Charles},
	month = sep,
	year = {2018},
	pages = {6--27},
	file = {2018_Kaasinen et al._Mobile Service Technician 4.0 – Knowledge-Sharing Solutions for Industrial Field Maintenance:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2018_Kaasinen et al._Mobile Service Technician 4.0 – Knowledge-Sharing Solutions for Industrial Field Maintenance.pdf:application/pdf},
}

@article{nguyenngoc2022,
	title = {Human-centred design in industry 4.0: case study review and opportunities for future research},
	volume = {33},
	issn = {1572-8145},
	shorttitle = {Human-centred design in industry 4.0},
	url = {https://doi.org/10.1007/s10845-021-01796-x},
	doi = {10.1007/s10845-021-01796-x},
	abstract = {The transition to industry 4.0 has impacted factories, but it also affects the entire value chain. In this sense, human-centred factors play a core role in transitioning to sustainable manufacturing processes and consumption. The awareness of human roles in Industry 4.0 is increasing, as evidenced by active work in developing methods, exploring influencing factors, and proving the effectiveness of design oriented to humans. However, numerous studies have been brought into existence but then disconnected from other studies. As a consequence, these studies in industry and research alike are not regularly adopted, and the network of studies is seemingly broad and expands without forming a coherent structure. This study is a unique attempt to bridge the gap through the literature characteristics and lessons learnt derived from a collection of case studies regarding human-centred design (HCD) in the context of Industry 4.0. This objective is achieved by a well-rounded systematic literature review whose special unit of analysis is given to the case studies, delivering contributions in three ways: (1) providing an insight into how the literature has evolved through the cross-disciplinary lens; (2) identifying what research themes associated with design methods are emerging in the field; (3) and setting the research agenda in the context of HCD in Industry 4.0, taking into account the lessons learnt, as uncovered by the in-depth review of case studies.},
	language = {en},
	number = {1},
	urldate = {2024-08-27},
	journal = {Journal of Intelligent Manufacturing},
	author = {Nguyen Ngoc, Hien and Lasa, Ganix and Iriarte, Ion},
	month = jan,
	year = {2022},
	keywords = {Industry 4.0, Artificial Intelligence, Case study review, Human-centred design, Research opportunities},
	pages = {35--76},
	file = {2022_Nguyen Ngoc et al._Human-centred design in industry 4.0 case study review and opportunities for future research.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2022_Nguyen Ngoc et al._Human-centred design in industry 4.0 case study review and opportunities for future research.pdf:application/pdf},
}

@article{brunetti2022,
	title = {Smart {Interactive} {Technologies} in the {Human}-{Centric} {Factory} 5.0: {A} {Survey}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	shorttitle = {Smart {Interactive} {Technologies} in the {Human}-{Centric} {Factory} 5.0},
	url = {https://www.mdpi.com/2076-3417/12/16/7965},
	doi = {10.3390/app12167965},
	abstract = {In this survey paper, we focus on smart interactive technologies and providing a picture of the current state of the art, exploring the way new discoveries and recent technologies changed workers’ operations and activities on the factory floor. We focus in particular on the Industry 4.0 and 5.0 visions, wherein smart interactive technologies can bring benefits to the intelligent behavior machines can expose in a human-centric AI perspective. We consider smart technologies wherein the intelligence may be in and/or behind the user interfaces, and for both groups we try to highlight the importance of designing them with a human-centric approach, framed in the smart factory context. We review relevant work in the field with the aim of highlighting the pros and cons of each technology and its adoption in the industry. Furthermore, we try to collect guidelines for the human-centric integration of smart interactive technologies in the smart factory. In this wa y, we hope to provide the future designers and adopters of such technologies with concrete help in choosing among different options and implementing them in a user-centric manner. To this aim, surveyed works have been also classified based on the supported task(s) and production process phases/activities: access to knowledge, logistics, maintenance, planning, production, security, workers’ wellbeing, and warehousing.},
	language = {en},
	number = {16},
	urldate = {2024-08-27},
	journal = {Applied Sciences},
	author = {Brunetti, Davide and Gena, Cristina and Vernero, Fabiana},
	month = jan,
	year = {2022},
	note = {Number: 16
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {smart factory, smart interactive technologies, user-centered AI},
	pages = {7965},
	file = {2022_Brunetti et al._Smart Interactive Technologies in the Human-Centric Factory 5.0 A Survey.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2022_Brunetti et al._Smart Interactive Technologies in the Human-Centric Factory 5.0 A Survey.pdf:application/pdf},
}

@misc{serradilla2020,
	title = {Deep learning models for predictive maintenance: a survey, comparison, challenges and prospect},
	shorttitle = {Deep learning models for predictive maintenance},
	url = {http://arxiv.org/abs/2010.03207},
	doi = {10.48550/arXiv.2010.03207},
	abstract = {Given the growing amount of industrial data spaces worldwide, deep learning solutions have become popular for predictive maintenance, which monitor assets to optimise maintenance tasks. Choosing the most suitable architecture for each use-case is complex given the number of examples found in literature. This work aims at facilitating this task by reviewing state-of-the-art deep learning architectures, and how they integrate with predictive maintenance stages to meet industrial companies' requirements (i.e. anomaly detection, root cause analysis, remaining useful life estimation). They are categorised and compared in industrial applications, explaining how to fill their gaps. Finally, open challenges and future research paths are presented.},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Serradilla, Oscar and Zugasti, Ekhi and Zurutuza, Urko},
	month = oct,
	year = {2020},
	note = {arXiv:2010.03207 [cs]},
	keywords = {A.1, Computer Science - Machine Learning, I.5, J.2},
	file = {2020_Serradilla et al._Deep learning models for predictive maintenance a survey, comparison, challenges and prospect:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2020_Serradilla et al._Deep learning models for predictive maintenance a survey, comparison, challenges and prospect.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\Y8UG7QVH\\2010.html:text/html},
}

@article{soenksen2022,
	title = {Integrated multimodal artificial intelligence framework for healthcare applications},
	volume = {5},
	issn = {2398-6352},
	url = {http://arxiv.org/abs/2202.12998},
	doi = {10.1038/s41746-022-00689-4},
	abstract = {Artificial intelligence (AI) systems hold great promise to improve healthcare over the next decades. Specifically, AI systems leveraging multiple data sources and input modalities are poised to become a viable method to deliver more accurate results and deployable pipelines across a wide range of applications. In this work, we propose and evaluate a unified Holistic AI in Medicine (HAIM) framework to facilitate the generation and testing of AI systems that leverage multimodal inputs. Our approach uses generalizable data pre-processing and machine learning modeling stages that can be readily adapted for research and deployment in healthcare environments. We evaluate our HAIM framework by training and characterizing 14,324 independent models based on HAIM-MIMIC-MM, a multimodal clinical database (N=34,537 samples) containing 7,279 unique hospitalizations and 6,485 patients, spanning all possible input combinations of 4 data modalities (i.e., tabular, time-series, text, and images), 11 unique data sources and 12 predictive tasks. We show that this framework can consistently and robustly produce models that outperform similar single-source approaches across various healthcare demonstrations (by 6-33\%), including 10 distinct chest pathology diagnoses, along with length-of-stay and 48-hour mortality predictions. We also quantify the contribution of each modality and data source using Shapley values, which demonstrates the heterogeneity in data modality importance and the necessity of multimodal inputs across different healthcare-relevant tasks. The generalizable properties and flexibility of our Holistic AI in Medicine (HAIM) framework could offer a promising pathway for future multimodal predictive systems in clinical and operational healthcare settings.},
	number = {1},
	urldate = {2024-08-29},
	journal = {npj Digital Medicine},
	author = {Soenksen, Luis R. and Ma, Yu and Zeng, Cynthia and Boussioux, Leonard D. J. and Carballo, Kimberly Villalobos and Na, Liangyuan and Wiberg, Holly M. and Li, Michael L. and Fuentes, Ignacio and Bertsimas, Dimitris},
	month = sep,
	year = {2022},
	note = {arXiv:2202.12998 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	pages = {149},
	file = {2022_Soenksen et al._Integrated multimodal artificial intelligence framework for healthcare applications:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2022_Soenksen et al._Integrated multimodal artificial intelligence framework for healthcare applications.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\CDDTZ9J9\\2202.html:text/html},
}

@misc{bohus2021,
	title = {Platform for {Situated} {Intelligence}},
	url = {http://arxiv.org/abs/2103.15975},
	doi = {10.48550/arXiv.2103.15975},
	abstract = {We introduce Platform for Situated Intelligence, an open-source framework created to support the rapid development and study of multimodal, integrative-AI systems. The framework provides infrastructure for sensing, fusing, and making inferences from temporal streams of data across different modalities, a set of tools that enable visualization and debugging, and an ecosystem of components that encapsulate a variety of perception and processing technologies. These assets jointly provide the means for rapidly constructing and refining multimodal, integrative-AI systems, while retaining the efficiency and performance characteristics required for deployment in open-world settings.},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Bohus, Dan and Andrist, Sean and Feniello, Ashley and Saw, Nick and Jalobeanu, Mihai and Sweeney, Patrick and Thompson, Anne Loomis and Horvitz, Eric},
	month = mar,
	year = {2021},
	note = {arXiv:2103.15975 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {2021_Bohus et al._Platform for Situated Intelligence.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2021_Bohus et al._Platform for Situated Intelligence.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\J7T6YT3L\\2103.html:text/html},
}

@article{tevanov2017,
	title = {The use of {3D} printing in improving patient-doctor relationship and malpractice prevention},
	volume = {25},
	doi = {10.4323/rjlm.2017.279},
	abstract = {Doctor-patient relationship is mostly build on effective communication which plays an important role in delivering proper health care. Doctors have the duty to provide appropriate and sufficient information to the patient, concerning his medical condition and the available treatment options. The breakdown between doctor-patient relationship is the cause of majority of patients' complaints and aversions. Using customized 3D printed models for each patient and having the conversation and the explanations needed, based on the palpable particularities of the patient's medical condition, helps towards a more efficient communication and a better understanding of the ailment and the treatment's outcomes, thus reducing patients' insecurities to the medical act, preventing complaints, dissatisfaction and malpractice accusations.},
	journal = {Romanian Journal of Legal Medicine},
	author = {Tevanov, Iulia and Liciu, Eduard and Chirila, Marius and Dusca, Andrei and Ulici, Alexandru},
	month = oct,
	year = {2017},
	pages = {279--282},
}

@article{vuyk1998,
	title = {The role of computer imaging in facial plastic surgery consultation: a clinical study},
	volume = {23},
	copyright = {Blackwell Science Ltd},
	issn = {1365-2273},
	shorttitle = {The role of computer imaging in facial plastic surgery consultation},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1046/j.1365-2273.1998.00139.x},
	doi = {10.1046/j.1365-2273.1998.00139.x},
	abstract = {The influence of using computerized visual communication on preoperative communication between the surgeon and the patient was analysed. This was a retrospective study based on a questionnaire completed by 50 patients who had undergone various facial plastic surgical procedures. Prediction tracings and postoperative slides were compared by the surgeon. The role of computer imaging in communication between doctor and patient, as well as the patient–doctor relationship and trust in the judgement of the doctor was considered to be positive by most of the patients. The vast majority of patients thought computer imaging should be a routine part of preoperative evaluation. Both the surgeon and the patients agreed that the representative value of prediction tracing was about 80\%. In view of the possible positive influences on communication and relationship in the preoperative phase, computer imaging may help to provide a clear and realistic preoperative informed consent.},
	language = {en},
	number = {3},
	urldate = {2024-09-15},
	journal = {Clinical Otolaryngology \& Allied Sciences},
	author = {{Vuyk} and {Stroomer} and {Vinayak}},
	year = {1998},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1046/j.1365-2273.1998.00139.x},
	keywords = {computer imaging, facial plastic surgery},
	pages = {235--243},
	file = {1998_Vuyk et al._The role of computer imaging in facial plastic surgery consultation a clinical study:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Hexagon-Aura\\1998_Vuyk et al._The role of computer imaging in facial plastic surgery consultation a clinical study.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\SA6XKDEF\\j.1365-2273.1998.00139.html:text/html},
}

@article{2022b,
	title = {The use of artificial intelligence and virtual reality in doctor-patient risk communication: {A} scoping review},
	volume = {105},
	issn = {0738-3991},
	shorttitle = {The use of artificial intelligence and virtual reality in doctor-patient risk communication},
	url = {https://www.sciencedirect.com/science/article/pii/S0738399122002750},
	doi = {10.1016/j.pec.2022.06.006},
	abstract = {While the development of artificial intelligence (AI) and virtual reality (VR) technologies in medicine has been significant, their application to doc…},
	language = {en-US},
	number = {10},
	urldate = {2024-09-15},
	journal = {Patient Education and Counseling},
	month = oct,
	year = {2022},
	note = {Publisher: Elsevier},
	pages = {3038--3050},
	file = {Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\WPWRS9IG\\S0738399122002750.html:text/html},
}

@article{lekakis2016,
	title = {Three-{Dimensional} {Surface} {Imaging} and the {Continuous} {Evolution} of {Preoperative} and {Postoperative} {Assessment} in {Rhinoplasty}},
	volume = {32},
	copyright = {Thieme Medical Publishers 333 Seventh Avenue, New York, NY 10001, USA.},
	issn = {0736-6825},
	url = {https://www.thieme-connect.de/products/ejournals/abstract/10.1055/s-0035-1570122},
	doi = {10.1055/s-0035-1570122},
	abstract = {During the preoperative assessment in rhinoplasty, the surgeon takes a thorough history, performs a complete examination by assessing functional and aesthetic aspects of the nose, obtains a clear understanding of the patient's wishes, conducts facial analysis based on standardized photography, and communicates to the patient the goals and pitfalls of surgery. Computer imaging or morphing of the preoperative pictures of the nose has drawn a lot of interest in the last decade, and it is a sign of evolution of the preoperative consultation. Technological advances, also in the context of rhinoplasty, have led to the development of three-dimensional (3D) imaging techniques, and have completely revolutionized the way that surgeons manage their patients preoperatively and evaluate postoperative results today. The accurate 3D surface imaging aids the surgeon to communicate with the patient adequately before surgery, to set an appropriate surgical plan, and to measure the shape and volume changes of the patient's nose that result from the intervention. The present review provides an analysis on the current knowledge of 3D surface imaging in rhinoplasty derived from the literature, and highlights future directions of preoperative and postoperative assessment in the field.},
	language = {en},
	urldate = {2024-09-15},
	journal = {Facial Plastic Surgery},
	author = {Lekakis, Garyfalia and Claes, Peter and Iii, Grant S. Hamilton and Hellings, P. W.},
	month = feb,
	year = {2016},
	note = {Publisher: Thieme Medical Publishers},
	pages = {088--094},
	file = {2016_Lekakis et al._Three-Dimensional Surface Imaging and the Continuous Evolution of Preoperative and Postoperative Ass:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Hexagon-Aura\\2016_Lekakis et al._Three-Dimensional Surface Imaging and the Continuous Evolution of Preoperative and Postoperative Ass.pdf:application/pdf},
}

@article{wang2023,
	title = {Analysis of the role and value of {Three}-dimensional panoramic virtual reality technology — {Take} surgery as an example},
	volume = {165},
	copyright = {© The Authors, published by EDP Sciences, 2023},
	issn = {2261-2424},
	url = {https://www.shs-conferences.org/articles/shsconf/abs/2023/14/shsconf_cike2023_01004/shsconf_cike2023_01004.html},
	doi = {10.1051/shsconf/202316501004},
	abstract = {With the development of surgical medicine and people’s more requirements for disease treatment, as well as the realistic demand of reducing errors and reducing surgical risks based on accurate surgical operations. Three-dimensional (3D) panoramic virtual reality technology is applied to surgical operations to meet the needs of different patients. This article discusses the interactive immersion experience of surgery, 3D simulation of surgical process, operational accuracy and other aspects. The purpose of this study is to explore the role and value of the simulation exercise of panoramic photography in the virtual surgical environment for the application of real surgery, start to discuss the details of the operation process, improve the probability of success of surgery, and reduce doctor-patient disputes. Based on the above research, the application prospect of 3D panoramic virtual reality technology in the field of surgery and medicine is prospected.},
	language = {en},
	urldate = {2024-09-15},
	journal = {SHS Web of Conferences},
	author = {Wang, Yujie and Song, Jiajia},
	year = {2023},
	note = {Publisher: EDP Sciences},
	pages = {01004},
	file = {2023_Wang and Song_Analysis of the role and value of Three-dimensional panoramic virtual reality technology — Take surg.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Hexagon-Aura\\2023_Wang and Song_Analysis of the role and value of Three-dimensional panoramic virtual reality technology — Take surg.pdf:application/pdf},
}

@article{duong2024,
	title = {Artificial {Intelligence} in {Plastic} {Surgery}: {Advancements}, {Applications}, and {Future}},
	volume = {11},
	copyright = {© 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
	shorttitle = {Artificial {Intelligence} in {Plastic} {Surgery}},
	url = {https://www.proquest.com/docview/3097884861/abstract/7BA7496DBBFA44C6PQ/1},
	doi = {10.3390/cosmetics11040109},
	abstract = {Artificial intelligence (AI) is revolutionizing plastic surgery through its remarkable advancements in various domains such as image analysis, robotic assistance, predictive analytics, and augmented reality. Predictive analytics, powered by AI, harnesses patient data to predict surgical outcomes, minimize risks, and tailor treatment plans, thereby optimizing patient care and safety. Augmented reality and virtual reality technology are also reshaping the cosmetic surgery landscape, providing immersive experiences for preoperative imaging, intraoperative guidance, and advanced skills through simulation. Looking ahead, the future of AI in plastic surgery holds great promise, including personalized medicine, bioprinting of tissues and organs, and continuous learning through iterative improvement algorithms based on real-world surgical experience. However, amid these transformational advances, ethical considerations and regulatory frameworks must evolve to ensure the responsible deployment of AI, protect patient privacy, minimize errors and algorithmic deviation, and uphold standards of fairness and transparency. Our study aims to explore the role of AI in the field of plastic surgery with the potential for the future in mind. In summary, AI is considered a beacon of innovation in plastic surgery, enhancing surgical precision, enhancing patient outcomes, and heralding a future where interventions rely on personalized technology that will redefine the boundaries of aesthetic and regenerative medicine.},
	language = {English},
	number = {4},
	urldate = {2024-09-15},
	journal = {Cosmetics},
	author = {Duong, Tran Van and Vy, Vu Pham Thao and Hung, Truong Nguyen Khanh},
	year = {2024},
	note = {Num Pages: 109
Place: Basel, Switzerland
Publisher: MDPI AG},
	keywords = {Deep learning, Machine learning, artificial intelligence, Algorithms, Artificial intelligence, artificial neural network, Automation, Computer applications, Datasets, Decision making, decision-making, Electronic health records, Empowerment, Image processing, Language, Medical imaging, Medical innovations, Natural language processing, Neural networks, Patients, plastic surgery, Plastic surgery, Precision medicine, Regenerative medicine, Robotic surgery, simulating surgical outcomes, Simulation, Software, Speech, Surgeons, Voice recognition},
	pages = {109},
	file = {2024_Duong et al._Artificial Intelligence in Plastic Surgery Advancements, Applications, and Future:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Hexagon-Aura\\2024_Duong et al._Artificial Intelligence in Plastic Surgery Advancements, Applications, and Future.pdf:application/pdf},
}

@article{chinski2022,
	title = {An {Artificial} {Intelligence} {Tool} for {Image} {Simulation} in {Rhinoplasty}},
	volume = {38},
	issn = {1098-8793},
	doi = {10.1055/s-0041-1729911},
	abstract = {During rhinoplasty consultations, surgeons typically create a computer simulation of the expected result. An artificial intelligence model (AIM) can learn a surgeon's style and criteria and generate the simulation automatically. The objective of this study is to determine if an AIM is capable of imitating a surgeon's criteria to generate simulated images of an aesthetic rhinoplasty surgery. This is a cross-sectional survey study of resident and specialist doctors in otolaryngology conducted in the month of November 2019 during a rhinoplasty conference. Sequential images of rhinoplasty simulations created by a surgeon and by an AIM were shown at random. Participants used a seven-point Likert scale to evaluate their level of agreement with the simulation images they were shown, with 1 indicating total disagreement and 7 total agreement. Ninety-seven of 122 doctors agreed to participate in the survey. The median level of agreement between the participant and the surgeon was 6 (interquartile range or IQR 5-7); between the participant and the AIM it was 5 (IQR 4-6), p-value {\textless} 0.0001. The evaluators were in total or partial agreement with the results of the AIM's simulation 68.4\% of the time (95\% confidence interval or CI 64.9-71.7). They were in total or partial agreement with the surgeon's simulation 77.3\% of the time (95\% CI 74.2-80.3). An AIM can emulate a surgeon's aesthetic criteria to generate a computer-simulated image of rhinoplasty. This can allow patients to have a realistic approximation of the possible results of a rhinoplasty ahead of an in-person consultation. The level of evidence of the study is 4.},
	language = {eng},
	number = {2},
	journal = {Facial plastic surgery: FPS},
	author = {Chinski, Hernan and Lerch, Ricardo and Tournour, Damián and Chinski, Luis and Caruso, Diego},
	month = apr,
	year = {2022},
	pmid = {34051703},
	keywords = {Humans, Artificial Intelligence, Computer Simulation, Cross-Sectional Studies, Esthetics, Dental, Rhinoplasty},
	pages = {201--206},
}

@misc{zotero-1976,
	title = {{VECTRA} {M3} {3D} {Imaging} {System} {\textbar} {Canfield} {Scientific}},
	url = {https://www.canfieldsci.com/imaging-systems/vectra-m3-3d-imaging-system/},
	urldate = {2024-09-15},
	file = {VECTRA M3 3D Imaging System | Canfield Scientific:C\:\\Users\\antoine.widmer\\Zotero\\storage\\K8TL263L\\vectra-m3-3d-imaging-system.html:text/html},
}

@article{destefani2022,
	title = {Validation of {Vectra} {3D} {Imaging} {Systems}: {A} {Review}},
	volume = {19},
	issn = {1661-7827},
	shorttitle = {Validation of {Vectra} {3D} {Imaging} {Systems}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9318949/},
	doi = {10.3390/ijerph19148820},
	abstract = {Aim: Three-dimensional facial imaging systems are a useful tool that is gradually replacing two-dimensional imaging and traditional anthropometry with calipers. In this varied and growing landscape of new devices, Canfield (Canfield Scientific, Parsippany, NJ, USA) has proposed a series of static and portable 3D imaging systems. The aim of this systematic review was to evaluate the current literature regarding the validation of Canfield’s Vectra imaging systems. Materials and Methods: A search strategy was developed on electronic databases including PubMed, Web of Science and Scopus by using specific keywords. After the study selection phase, a total of 10 articles were included in the present review. Results: A total of 10 articles were finally included in the present review. For six articles, we conducted a validation of the Vectra static devices, focusing especially on the Vectra M5, Vectra M3 and Vectra XT. For four articles, we validated the Vectra H1 portable system. Conclusions: All of the reviewed articles concluded that Canfield’s Vectra 3D imaging systems are capable of capturing accurate and reproducible stereophotogrammetric images. Minor errors were reported, particularly in the acquisition of the perioral region, but all the evaluated devices are considered to be valid and accurate tools for clinicians.},
	number = {14},
	urldate = {2024-09-15},
	journal = {International Journal of Environmental Research and Public Health},
	author = {De Stefani, Alberto and Barone, Martina and Hatami Alamdari, Sam and Barjami, Arjola and Baciliero, Ugo and Apolloni, Federico and Gracco, Antonio and Bruno, Giovanni},
	month = jul,
	year = {2022},
	pmid = {35886670},
	pmcid = {PMC9318949},
	pages = {8820},
	file = {2022_De Stefani et al._Validation of Vectra 3D Imaging Systems A Review:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Hexagon-Aura\\2022_De Stefani et al._Validation of Vectra 3D Imaging Systems A Review.pdf:application/pdf},
}

@misc{zotero-1982,
	title = {{VISIA} {Skin} {Analysis} {\textbar} {Canfield} {Scientific}},
	url = {https://www.canfieldsci.com/imaging-systems/visia-complexion-analysis/},
	urldate = {2024-09-15},
	file = {VISIA Skin Analysis | Canfield Scientific:C\:\\Users\\antoine.widmer\\Zotero\\storage\\9YIF8MPK\\visia-complexion-analysis.html:text/html},
}

@misc{illusioimaging2020,
	title = {It's {Time} {For} {ILLUSIO}},
	url = {https://www.youtube.com/watch?v=bBM_IUQRbnE},
	urldate = {2024-09-15},
	author = {{Illusio Imaging}},
	month = aug,
	year = {2020},
}

@misc{2024b,
	title = {Face simulator},
	url = {https://arbrea-labs.com/face/},
	abstract = {Explore the benefits of using Arbrea Face Simulator: optimize consultation time and enhance patient satisfaction.},
	language = {en-US},
	urldate = {2024-09-15},
	journal = {Arbrea Labs},
	month = oct,
	year = {2024},
	file = {Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\BG727YYB\\face.html:text/html},
}

@incollection{tokgoz2023,
	address = {Cham},
	title = {Cosmetic and {Reconstructive} {Facial} {Plastic} {Surgery} {Related} {Simulation} and {Optimization} {Efforts}},
	isbn = {978-3-031-31168-0},
	url = {https://doi.org/10.1007/978-3-031-31168-0_7},
	abstract = {Advancements in facial plastic surgery optimal outcome attainment by using simulation and optimization efforts are progressing as technology advances. Computed tomography (CT) and magnetic resonance imaging (MRI) are the technologies applied in this area of interest while technologies such as virtual and augmented reality applications as well as advanced manufacturing techniques further advanced simulation and surgical planning aspects of plastic surgery. In this work, we will outline examples and review technologies used for facial plastic surgery that helped to improve simulation and optimization aspects of the plastic surgery processes.},
	language = {en},
	urldate = {2024-09-15},
	booktitle = {Cosmetic and {Reconstructive} {Facial} {Plastic} {Surgery}: {A} {Review} of {Medical} and {Biomedical} {Engineering} and {Science} {Concepts}},
	publisher = {Springer Nature Switzerland},
	author = {Tokgöz, Emre and Carro, Marina A.},
	editor = {Tokgöz, Emre and Carro, Marina A.},
	year = {2023},
	doi = {10.1007/978-3-031-31168-0_7},
	pages = {231--256},
}

@article{2022c,
	title = {High-fidelity {3D} real-time facial animation using infrared structured light sensing system},
	volume = {104},
	issn = {0097-8493},
	url = {https://www.sciencedirect.com/science/article/pii/S0097849322000395},
	doi = {10.1016/j.cag.2022.03.007},
	abstract = {The acquisition of 3D facial models is crucial in the gaming and film industries. In this study, we developed a facial acquisition system based on inf…},
	language = {en-US},
	urldate = {2024-09-16},
	journal = {Computers \& Graphics},
	month = may,
	year = {2022},
	note = {Publisher: Pergamon},
	pages = {46--58},
	file = {Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\PBUY7ACI\\S0097849322000395.html:text/html},
}

@misc{baltrusaitis2024,
	title = {{TadasBaltrusaitis}/{OpenFace}},
	url = {https://github.com/TadasBaltrusaitis/OpenFace},
	abstract = {OpenFace – a state-of-the art tool intended for facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation.},
	urldate = {2024-09-16},
	author = {Baltrusaitis, Tadas},
	month = sep,
	year = {2024},
	note = {original-date: 2016-03-05T20:08:49Z},
}

@misc{zhang2023b,
	title = {Video-{LLaMA}: {An} {Instruction}-tuned {Audio}-{Visual} {Language} {Model} for {Video} {Understanding}},
	shorttitle = {Video-{LLaMA}},
	url = {http://arxiv.org/abs/2306.02858},
	abstract = {We present Video-LLaMA a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual and audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual and audio encoders with LLM's embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.},
	urldate = {2024-10-16},
	publisher = {arXiv},
	author = {Zhang, Hang and Li, Xin and Bing, Lidong},
	month = oct,
	year = {2023},
	note = {arXiv:2306.02858},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {2023_Zhang et al._Video-LLaMA An Instruction-tuned Audio-Visual Language Model for Video Understanding.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2023_Zhang et al._Video-LLaMA An Instruction-tuned Audio-Visual Language Model for Video Understanding.pdf:application/pdf},
}

@inproceedings{kawakami2022,
	address = {New York, NY, USA},
	series = {{DIS} '22},
	title = {“{Why} {Do} {I} {Care} {What}’s {Similar}?” {Probing} {Challenges} in {AI}-{Assisted} {Child} {Welfare} {Decision}-{Making} through {Worker}-{AI} {Interface} {Design} {Concepts}},
	isbn = {978-1-4503-9358-4},
	shorttitle = {“{Why} {Do} {I} {Care} {What}’s {Similar}?},
	url = {https://dl.acm.org/doi/10.1145/3532106.3533556},
	doi = {10.1145/3532106.3533556},
	abstract = {Data-driven AI systems are increasingly used to augment human decision-making in complex, social contexts, such as social work or legal practice. Yet, most existing design knowledge regarding how to best support AI-augmented decision-making comes from studies in comparatively well-defined settings. In this paper, we present findings from design interviews with 12 social workers who use an algorithmic decision support tool (ADS) to assist their day-to-day child maltreatment screening decisions. We generated a range of design concepts, each envisioning different ways of redesigning or augmenting the ADS interface. Overall, workers desired ways to understand the risk score and incorporate contextual knowledge, which move beyond existing notions of AI interpretability. Conversations around our design concepts also surfaced more fundamental concerns around the assumptions underlying statistical prediction, such as inference based on similar historical cases and statistical notions of uncertainty. Based on our findings, we discuss how ADS may be better designed to support the roles of human decision-makers in social decision-making contexts.},
	urldate = {2024-10-24},
	booktitle = {Proceedings of the 2022 {ACM} {Designing} {Interactive} {Systems} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Kawakami, Anna and Sivaraman, Venkatesh and Stapleton, Logan and Cheng, Hao-Fei and Perer, Adam and Wu, Zhiwei Steven and Zhu, Haiyi and Holstein, Kenneth},
	month = jun,
	year = {2022},
	pages = {454--470},
	file = {2022_Kawakami et al._“Why Do I Care What’s Similar” Probing Challenges in AI-Assisted Child Welfare Decision-Making thro:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\HETS\\2022_Kawakami et al._“Why Do I Care What’s Similar” Probing Challenges in AI-Assisted Child Welfare Decision-Making thro.pdf:application/pdf},
}

@article{lehtiniemi2024,
	title = {Contextual social valences for artificial intelligence: anticipation that matters in social work},
	volume = {27},
	issn = {1369-118X},
	shorttitle = {Contextual social valences for artificial intelligence},
	url = {https://doi.org/10.1080/1369118X.2023.2234987},
	doi = {10.1080/1369118X.2023.2234987},
	abstract = {In pilot trials, Finnish caseworkers in child welfare services used an AI tool predicting severe risks faced by their clients. Based on interviews with the caseworkers involved, this article draws on those trials to discuss AI valences, or the range of expectations of AI’s value and performance, in social work and beyond. While AI travels across sites of application and sectors of society, its value is often expected to come from the production of anticipatory knowledge. The predictive AI tool used by Finnish caseworkers offers an example: it turned past data about clients into predictions about their future, with an aim of authorizing present interventions to optimize the future. In the pilot trials, however, AI met the practice of social work. In contrast to generic expectations of predictive performance, caseworkers had contextual expectations for AI, reflecting their situated knowledge about their field. For caseworkers, anticipation does not mean producing pieces of speculative knowledge about the future. Instead, for them, anticipation is a professional knowledge-making practice, based on intimate encounters with clients. Caseworkers therefore expect AI to produce contextually relevant information that can facilitate those interactions. This suggests that for AI developments to matter in social work, it is necessary to consider AI not as a tool that produces knowledge outcomes, but one that supports human experts’ knowledge-making processes. More broadly, as AI tools enter new sensitive areas of application, instead of expecting generic value and performance from them, careful attention should be paid on contextual AI valences.},
	number = {6},
	urldate = {2024-10-24},
	journal = {Information, Communication \& Society},
	author = {Lehtiniemi, Tuukka},
	month = apr,
	year = {2024},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/1369118X.2023.2234987},
	keywords = {artificial intelligence, Anticipation, expectations, prediction, social valence, social work},
	pages = {1110--1125},
	file = {2024_Lehtiniemi_Contextual social valences for artificial intelligence anticipation that matters in social work:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\HETS\\2024_Lehtiniemi_Contextual social valences for artificial intelligence anticipation that matters in social work.pdf:application/pdf},
}

@inproceedings{moon2024,
	address = {New York, NY, USA},
	series = {{GI} '24},
	title = {Beyond {Predictive} {Algorithms} in {Child} {Welfare}},
	isbn = {979-8-4007-1828-1},
	url = {https://dl.acm.org/doi/10.1145/3670947.3670976},
	doi = {10.1145/3670947.3670976},
	abstract = {Caseworkers in the child welfare (CW) sector use predictive decision-making algorithms built on risk assessment (RA) data to guide and support CW decisions. Researchers have highlighted that RAs can contain biased signals which flatten CW case complexities and that the algorithms may benefit from incorporating contextually rich case narratives, i.e. - the casenotes written by caseworkers. To investigate this hypothesized improvement, we quantitatively deconstructed two commonly used RAs from a United States CW agency. We trained classifier models to compare the predictive validity of RAs with and without casenote narratives and applied computational text analysis on casenotes to highlight topics uncovered in the casenotes. Our study finds that common risk metrics used to assess families and build CWS predictive risk models (PRMs) are unable to predict discharge outcomes for children who are not reunified with their birth parent(s). We also find that although casenotes cannot predict discharge outcomes, they contain contextual case signals. Given the lack of predictive validity of RA scores and casenotes, we propose moving beyond quantitative risk assessments for public sector algorithms and towards using contextual sources of information such as narratives to study public sociotechnical systems.},
	urldate = {2024-10-24},
	booktitle = {Proceedings of the 50th {Graphics} {Interface} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Moon, Erina Seh-Young and Saxena, Devansh and Maharaj, Tegan and Guha, Shion},
	month = sep,
	year = {2024},
	pages = {1--13},
	file = {2024_Moon et al._Beyond Predictive Algorithms in Child Welfare.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\HETS\\2024_Moon et al._Beyond Predictive Algorithms in Child Welfare.pdf:application/pdf},
}

@inproceedings{reinmund2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {Transitioning {Towards} a {Proactive} {Practice}: {A} {Longitudinal} {Field} {Study} on the {Implementation} of a {ML} {System} in {Adult} {Social} {Care}},
	isbn = {979-8-4007-0330-0},
	shorttitle = {Transitioning {Towards} a {Proactive} {Practice}},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642247},
	doi = {10.1145/3613904.3642247},
	abstract = {Politicians and care associations advocate for the use of machine learning (ML) systems to improve the delivery of adult social services. Yet, guidance on how to implement ML systems remains limited and research indicates that future implementation efforts are likely to encounter difficulties. We aim to enhance the understanding of ML system implementations by conducting a longitudinal field study with a team responsible for deploying a ML system within an adult social services department. The ML system implementation represented a cross-organisational effort to facilitate the department’s transition to a proactive practice. Throughout this process, stakeholders adapted to numerous challenges in real-time. This study makes three contributions. First, we provide a description of how ML systems are implemented and highlight practical challenges. Second, we illustrate the utility of HCI knowledge in designing workflows for ML-assisted preventative care programmes. Finally, we provide recommendations for future deployments of ML systems in social care.},
	urldate = {2024-10-24},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Reinmund, Tyler and Kunze, Lars and Jirotka, Marina Denise},
	month = may,
	year = {2024},
	pages = {1--19},
	file = {2024_Reinmund et al._Transitioning Towards a Proactive Practice A Longitudinal Field Study on the Implementation of a ML.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\HETS\\2024_Reinmund et al._Transitioning Towards a Proactive Practice A Longitudinal Field Study on the Implementation of a ML.pdf:application/pdf},
}

@article{ahn,
	title = {Qualitative exploration of child welfare workers’ decision-making experiences and perspectives on fairness},
	volume = {0},
	issn = {1554-8732},
	url = {https://doi.org/10.1080/15548732.2024.2312846},
	doi = {10.1080/15548732.2024.2312846},
	abstract = {Assessing and responding to risks to children’s safety is a primary concern of the child protection system (CPS), and decision-support tools have been developed to assist child welfare workers (CWW). Yet, a limited understanding of CWWs’ decision-making experiences impedes our efforts to effectively support them. This qualitative study examines the unique characteristics of decision-making in CPS through focus groups involving CWWs from an agency in California. Five themes emerged: CWWs’ responsibilities, decision-making characteristics, domain-specific complexities, and CWWs’ perspectives on fairness in decision-making. The findings highlight the need to incorporate CWWs’ experiences and insights in developing future decision-support tools.},
	number = {0},
	urldate = {2024-10-24},
	journal = {Journal of Public Child Welfare},
	author = {Ahn, Eunhye and Morstatter, Fred and Waters-Roman, Debra and Palmer, Lindsey and McCroskey, Jacquelyn},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/15548732.2024.2312846},
	keywords = {decision-making, Child protection system, child welfare workers, fairness in decision-making, qualitative analysis},
	pages = {1--24},
	file = {Ahn et al._Qualitative exploration of child welfare workers’ decision-making experiences and perspectives on fa:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\HETS\\Ahn et al._Qualitative exploration of child welfare workers’ decision-making experiences and perspectives on fa.pdf:application/pdf},
}

@article{tomasev2020,
	title = {{AI} for social good: unlocking the opportunity for positive impact},
	volume = {11},
	copyright = {2020 Crown},
	issn = {2041-1723},
	shorttitle = {{AI} for social good},
	url = {https://www.nature.com/articles/s41467-020-15871-z},
	doi = {10.1038/s41467-020-15871-z},
	abstract = {Advances in machine learning (ML) and artificial intelligence (AI) present an opportunity to build better tools and solutions to help address some of the world’s most pressing challenges, and deliver positive social impact in accordance with the priorities outlined in the United Nations’ 17 Sustainable Development Goals (SDGs). The AI for Social Good (AI4SG) movement aims to establish interdisciplinary partnerships centred around AI applications towards SDGs. We provide a set of guidelines for establishing successful long-term collaborations between AI researchers and application-domain experts, relate them to existing AI4SG projects and identify key opportunities for future AI applications targeted towards social good.},
	language = {en},
	number = {1},
	urldate = {2024-10-24},
	journal = {Nature Communications},
	author = {Tomašev, Nenad and Cornebise, Julien and Hutter, Frank and Mohamed, Shakir and Picciariello, Angela and Connelly, Bec and Belgrave, Danielle C. M. and Ezer, Daphne and Haert, Fanny Cachat van der and Mugisha, Frank and Abila, Gerald and Arai, Hiromi and Almiraat, Hisham and Proskurnia, Julia and Snyder, Kyle and Otake-Matsuura, Mihoko and Othman, Mustafa and Glasmachers, Tobias and Wever, Wilfried de and Teh, Yee Whye and Khan, Mohammad Emtiyaz and Winne, Ruben De and Schaul, Tom and Clopath, Claudia},
	month = may,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	keywords = {Developing world, Ethics},
	pages = {2468},
	file = {2020_Tomašev et al._AI for social good unlocking the opportunity for positive impact.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\HETS\\2020_Tomašev et al._AI for social good unlocking the opportunity for positive impact.pdf:application/pdf},
}

@article{zhang2024a,
	title = {Deep learning-based multimodal emotion recognition from audio, visual, and text modalities: {A} systematic review of recent advancements and future prospects},
	volume = {237},
	issn = {0957-4174},
	shorttitle = {Deep learning-based multimodal emotion recognition from audio, visual, and text modalities},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417423021942},
	doi = {10.1016/j.eswa.2023.121692},
	abstract = {Emotion recognition has recently attracted extensive interest due to its significant applications to human–computer interaction. The expression of human emotion depends on various verbal and non-verbal languages like audio, visual, text, etc. Emotion recognition is thus well suited as a multimodal rather than single-modal learning problem. Owing to the powerful feature learning capability, extensive deep learning methods have been recently leveraged to capture high-level emotional feature representations for multimodal emotion recognition (MER). Therefore, this paper makes the first effort in comprehensively summarize recent advances in deep learning-based multimodal emotion recognition (DL-MER) involved in audio, visual, and text modalities. We focus on: (1) MER milestones are given to summarize the development tendency of MER, and conventional multimodal emotional datasets are provided; (2) The core principles of typical deep learning models and its recent advancements are overviewed; (3) A systematic survey and taxonomy is provided to cover the state-of-the-art methods related to two key steps in a MER system, including feature extraction and multimodal information fusion; (4) The research challenges and open issues in this field are discussed, and promising future directions are given.},
	urldate = {2024-10-24},
	journal = {Expert Systems with Applications},
	author = {Zhang, Shiqing and Yang, Yijiao and Chen, Chen and Zhang, Xingnan and Leng, Qingming and Zhao, Xiaoming},
	month = mar,
	year = {2024},
	keywords = {Deep learning, Feature extraction, Multimodal emotion recognition, Multimodal information fusion, review},
	pages = {121692},
	file = {ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\ANFIXIY5\\S0957417423021942.html:text/html},
}

@article{zhang2024b,
	title = {Deep learning-based multimodal emotion recognition from audio, visual, and text modalities: {A} systematic review of recent advancements and future prospects},
	volume = {237},
	issn = {09574174},
	shorttitle = {Deep learning-based multimodal emotion recognition from audio, visual, and text modalities},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417423021942},
	doi = {10.1016/j.eswa.2023.121692},
	abstract = {Emotion recognition has recently attracted extensive interest due to its significant applications to human–computer interaction. The expression of human emotion depends on various verbal and non-verbal languages like audio, visual, text, etc. Emotion recognition is thus well suited as a multimodal rather than single-modal learning problem. Owing to the powerful feature learning capability, extensive deep learning methods have been recently leveraged to capture high-level emotional feature representations for multimodal emotion recognition (MER). Therefore, this paper makes the first effort in comprehensively summarize recent advances in deep learning-based multimodal emotion recognition (DL-MER) involved in audio, visual, and text modalities. We focus on: (1) MER milestones are given to summarize the development tendency of MER, and conventional multimodal emotional datasets are provided; (2) The core principles of typical deep learning models and its recent advancements are overviewed; (3) A systematic survey and taxonomy is provided to cover the state-of-theart methods related to two key steps in a MER system, including feature extraction and multimodal information fusion; (4) The research challenges and open issues in this field are discussed, and promising future directions are given.},
	language = {en},
	urldate = {2024-10-24},
	journal = {Expert Systems with Applications},
	author = {Zhang, Shiqing and Yang, Yijiao and Chen, Chen and Zhang, Xingnan and Leng, Qingming and Zhao, Xiaoming},
	month = mar,
	year = {2024},
	pages = {121692},
	file = {2024_Zhang et al._Deep learning-based multimodal emotion recognition from audio, visual, and text modalities A system:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\HETS\\2024_Zhang et al._Deep learning-based multimodal emotion recognition from audio, visual, and text modalities A system.pdf:application/pdf},
}

@article{wei2024,
	title = {Learning facial expression and body gesture visual information for video emotion recognition},
	volume = {237},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417423019218},
	doi = {10.1016/j.eswa.2023.121419},
	abstract = {Recent research has shown that facial expressions and body gestures are two significant implications in identifying human emotions. However, these studies mainly focus on contextual information of adjacent frames, and rarely explore the spatio-temporal relationships between distant or global frames. In this paper, we revisit the facial expression and body gesture emotion recognition problems, and propose to improve the performance of video emotion recognition by extracting the spatio-temporal features via further encoding temporal information. Specifically, for facial expression, we propose a super image-based spatio-temporal convolutional model (SISTCM) and a two-stream LSTM model to capture the local spatio-temporal features and learn global temporal cues of emotion changes. For body gestures, a novel representation method and an attention-based channel-wise convolutional model (ACCM) are introduced to learn key joints features and independent characteristics of each joint. Extensive experiments on five common datasets are carried out to prove the superiority of the proposed method, and the results proved learning two visual information leads to significant improvement over the existing state-of-the-art methods.},
	urldate = {2024-10-24},
	journal = {Expert Systems with Applications},
	author = {Wei, Jie and Hu, Guanyu and Yang, Xinyu and Luu, Anh Tuan and Dong, Yizhuo},
	month = mar,
	year = {2024},
	keywords = {Body joints, Facial expression, Gesture representation, Spatio-temporal features, Video emotion recognition},
	pages = {121419},
	file = {ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\WF8BEERI\\S0957417423019218.html:text/html},
}

@article{wei2024a,
	title = {Learning facial expression and body gesture visual information for video emotion recognition},
	volume = {237},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417423019218},
	doi = {10.1016/j.eswa.2023.121419},
	abstract = {Recent research has shown that facial expressions and body gestures are two significant implications in identifying human emotions. However, these studies mainly focus on contextual information of adjacent frames, and rarely explore the spatio-temporal relationships between distant or global frames. In this paper, we revisit the facial expression and body gesture emotion recognition problems, and propose to improve the performance of video emotion recognition by extracting the spatio-temporal features via further encoding temporal information. Specifically, for facial expression, we propose a super image-based spatio-temporal convolutional model (SISTCM) and a two-stream LSTM model to capture the local spatio-temporal features and learn global temporal cues of emotion changes. For body gestures, a novel representation method and an attention-based channel-wise convolutional model (ACCM) are introduced to learn key joints features and independent characteristics of each joint. Extensive experiments on five common datasets are carried out to prove the superiority of the proposed method, and the results proved learning two visual information leads to significant improvement over the existing state-of-the-art methods.},
	language = {en},
	urldate = {2024-10-24},
	journal = {Expert Systems with Applications},
	author = {Wei, Jie and Hu, Guanyu and Yang, Xinyu and Luu, Anh Tuan and Dong, Yizhuo},
	month = mar,
	year = {2024},
	pages = {121419},
	file = {2024_Wei et al._Learning facial expression and body gesture visual information for video emotion recognition:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\HETS\\2024_Wei et al._Learning facial expression and body gesture visual information for video emotion recognition.pdf:application/pdf},
}

@article{leong2023,
	title = {Facial expression and body gesture emotion recognition: {A} systematic review on the use of visual data in affective computing},
	volume = {48},
	issn = {15740137},
	shorttitle = {Facial expression and body gesture emotion recognition},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574013723000126},
	doi = {10.1016/j.cosrev.2023.100545},
	abstract = {Emotion is an important driver of human decision-making and communication. With the recent rise of human–computer interaction, affective computing has become a trending research topic, aiming to develop computational systems that can understand human emotions and respond to them. A systematic review has been conducted to fill these gaps since previous reviews regarding machine-enabled automated visual emotion recognition neglect important methodological aspects, including emotion models and hardware usage. 467 relevant papers were initially found and examined. After the screening process with specific inclusion and exclusion criteria, 30 papers were selected. Methodological aspects including emotion models, devices, architectures, and classification techniques employed by the selected studies were analyzed, and the most popular techniques and current trends in visual emotion recognition were identified. This review not only offers a comprehensive and upto-date overview of the topic but also provides researchers with insights regarding methodological aspects like emotion models employed, devices used, and classification techniques for automated visual emotion recognition. By identifying current trends, like the increased use of deep learning algorithms and the need for further study on body gestures, this review advocates the advantages of implementing emotion recognition with the use of visual data and builds a solid foundation for applying relevant techniques in different fields.},
	language = {en},
	urldate = {2024-10-24},
	journal = {Computer Science Review},
	author = {Leong, Sze Chit and Tang, Yuk Ming and Lai, Chung Hin and Lee, C.K.M.},
	month = may,
	year = {2023},
	pages = {100545},
	file = {2023_Leong et al._Facial expression and body gesture emotion recognition A systematic review on the use of visual dat:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\HETS\\2023_Leong et al._Facial expression and body gesture emotion recognition A systematic review on the use of visual dat.pdf:application/pdf},
}

@article{mai2023,
	title = {Hybrid {Contrastive} {Learning} of {Tri}-{Modal} {Representation} for {Multimodal} {Sentiment} {Analysis}},
	volume = {14},
	issn = {1949-3045},
	url = {https://ieeexplore.ieee.org/document/9767560},
	doi = {10.1109/TAFFC.2022.3172360},
	abstract = {The wide application of smart devices enables the availability of multimodal data, which can be utilized in many tasks. In the field of multimodal sentiment analysis, most previous works focus on exploring intra- and inter-modal interactions. However, training a network with cross-modal information (language, audio and visual) is still challenging due to the modality gap. Besides, while learning dynamics within each sample draws great attention, the learning of inter-sample and inter-class relationships is neglected. Moreover, the size of datasets limits the generalization ability of the models. To address the afore-mentioned issues, we propose a novel framework HyCon for hybrid contrastive learning of tri-modal representation. Specifically, we simultaneously perform intra-/inter-modal contrastive learning and semi-contrastive learning, with which the model can fully explore cross-modal interactions, learn inter-sample and inter-class relationships, and reduce the modality gap. Besides, refinement term and modality margin are introduced to enable a better learning of unimodal pairs. Moreover, we devise pair selection mechanism to identify and assign weights to the informative negative and positive pairs. HyCon can naturally generate many training pairs for better generalization and reduce the negative effect of limited datasets. Extensive experiments demonstrate that our method outperforms baselines on multimodal sentiment analysis and emotion recognition.},
	number = {3},
	urldate = {2024-10-24},
	journal = {IEEE Transactions on Affective Computing},
	author = {Mai, Sijie and Zeng, Ying and Zheng, Shuangjia and Hu, Haifeng},
	month = jul,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {Task analysis, Training, Visualization, Computational modeling, Bit error rate, multimodal learning, Multimodal sentiment analysis, representation learning, Representation learning, Sentiment analysis, supervised contrastive learning},
	pages = {2276--2289},
	file = {2023_Mai et al._Hybrid Contrastive Learning of Tri-Modal Representation for Multimodal Sentiment Analysis:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\HETS\\2023_Mai et al._Hybrid Contrastive Learning of Tri-Modal Representation for Multimodal Sentiment Analysis.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\BM9AQ9R7\\9767560.html:text/html},
}

@article{clemente2024,
	title = {Feasibility of {3D} {Body} {Tracking} from {Monocular} {2D} {Video} {Feeds} in {Musculoskeletal} {Telerehabilitation}},
	volume = {24},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/24/1/206},
	doi = {10.3390/s24010206},
	abstract = {Musculoskeletal conditions affect millions of people globally; however, conventional treatments pose challenges concerning price, accessibility, and convenience. Many telerehabilitation solutions offer an engaging alternative but rely on complex hardware for body tracking. This work explores the feasibility of a model for 3D Human Pose Estimation (HPE) from monocular 2D videos (MediaPipe Pose) in a physiotherapy context, by comparing its performance to ground truth measurements. MediaPipe Pose was investigated in eight exercises typically performed in musculoskeletal physiotherapy sessions, where the Range of Motion (ROM) of the human joints was the evaluated parameter. This model showed the best performance for shoulder abduction, shoulder press, elbow flexion, and squat exercises. Results have shown a MAPE ranging between 14.9\% and 25.0\%, Pearson’s coefficient ranging between 0.963 and 0.996, and cosine similarity ranging between 0.987 and 0.999. Some exercises (e.g., seated knee extension and shoulder flexion) posed challenges due to unusual poses, occlusions, and depth ambiguities, possibly related to a lack of training data. This study demonstrates the potential of HPE from monocular 2D videos, as a markerless, affordable, and accessible solution for musculoskeletal telerehabilitation approaches. Future work should focus on exploring variations of the 3D HPE models trained on physiotherapy-related datasets, such as the Fit3D dataset, and post-preprocessing techniques to enhance the model’s performance.},
	language = {en},
	number = {1},
	urldate = {2024-11-13},
	journal = {Sensors},
	author = {Clemente, Carolina and Chambel, Gonçalo and Silva, Diogo C. F. and Montes, António Mesquita and Pinto, Joana F. and Silva, Hugo Plácido da},
	month = jan,
	year = {2024},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, 3D Human Pose Estimation, 2D camera, MediaPipe Pose, monocular, musculoskeletal, ROM, telerehabilitation, videos},
	pages = {206},
	file = {2024_Clemente et al._Feasibility of 3D Body Tracking from Monocular 2D Video Feeds in Musculoskeletal Telerehabilitation.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Body-Tracking\\2024_Clemente et al._Feasibility of 3D Body Tracking from Monocular 2D Video Feeds in Musculoskeletal Telerehabilitation.pdf:application/pdf},
}

@article{tian2021,
	title = {Emotional arousal in {2D} versus {3D} virtual reality environments},
	volume = {16},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256211},
	doi = {10.1371/journal.pone.0256211},
	abstract = {Previous studies have suggested that virtual reality (VR) can elicit emotions in different visual modes using 2D or 3D headsets. However, the effects on emotional arousal by using these two visual modes have not been comprehensively investigated, and the underlying neural mechanisms are not yet clear. This paper presents a cognitive psychological experiment that was conducted to analyze how these two visual modes impact emotional arousal. Forty volunteers were recruited and were randomly assigned to two groups. They were asked to watch a series of positive, neutral and negative short VR videos in 2D and 3D. Multichannel electroencephalograms (EEG) and skin conductance responses (SCR) were recorded simultaneously during their participation. The results indicated that emotional stimulation was more intense in the 3D environment due to the improved perception of the environment; greater emotional arousal was generated; and higher beta (21–30 Hz) EEG power was identified in 3D than in 2D. We also found that both hemispheres were involved in stereo vision processing and that brain lateralization existed in the processing.},
	language = {en},
	number = {9},
	urldate = {2024-11-25},
	journal = {PLOS ONE},
	author = {Tian, Feng and Hua, Minlei and Zhang, Wenrui and Li, Yingjie and Yang, Xiaoli},
	month = sep,
	year = {2021},
	note = {Publisher: Public Library of Science},
	keywords = {Virtual reality, Electroencephalography, Emotions, Analysis of variance, Cognitive psychology, Left hemisphere, Right hemisphere, Skin physiology},
	pages = {e0256211},
	file = {2021_Tian et al._Emotional arousal in 2D versus 3D virtual reality environments.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Emotions\\2021_Tian et al._Emotional arousal in 2D versus 3D virtual reality environments.pdf:application/pdf},
}

@article{xie2023a,
	title = {Brain {Activation} {Differences} of {Six} {Basic} {Emotions} {Between} {2D} {Screen} and {Virtual} {Reality} {Modalities}},
	volume = {31},
	issn = {1558-0210},
	url = {https://ieeexplore.ieee.org/abstract/document/9987528},
	doi = {10.1109/TNSRE.2022.3229389},
	abstract = {To our knowledge, it has been widely studied in Screen-2D modality for the six basic emotions proposed by Professor Paul Ekman, but there are only studies on their positive and negative valence in VR-3D modality. In this study, we will investigate whether the six basic emotions have stronger brain activation states in VR-3D modality than in Screen-2D modality. We designed an emotion-inducing experiment with six basic emotions (happiness, surprise, sadness, fear, anger, and disgust) to record the electroencephalogram (EEG) signals during watching VR-3D and Screen-2D videos. The power spectral density (PSD) was calculated to compare the brain activation differences between VR-3D and Screen-2D modalities during the induction of the six basic emotions. The results of statistical analysis of the relative power differences between VR-3D and Screen-2D modalities for each emotion revealed that both happiness and surprise presented greater differences in the {\textbackslash}alpha and {\textbackslash}gamma frequency bands, while sad, fear, disgust and anger all presented greater differences in the {\textbackslash}alpha and þeta frequency bands, which are mainly observed in the frontal and occipital regions. On the other hand, the six emotions all yielded satisfactory classification accuracy (above 85\%) by classification from a subset of power feature of the brain activation states in the same emotion between the two modalities. Overall, there are significant differences in the induction of same discrete emotions in VR-3D and Screen-2D modalities, with greater brain activation in VR-3D modalities. These findings provide a better understanding about the neural activity of discrete emotional tasks assessed in VR environments.},
	urldate = {2024-11-25},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	author = {Xie, Jialan and Lan, Ping and Wang, Shiyuan and Luo, Yutong and Liu, Guangyuan},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	keywords = {virtual reality, Three-dimensional displays, Electroencephalography, Visualization, Videos, EEG, Atmospheric measurements, Band-pass filters, brain activation, Particle measurements, Six basic emotions},
	pages = {700--709},
	file = {2023_Xie et al._Brain Activation Differences of Six Basic Emotions Between 2D Screen and Virtual Reality Modalities:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Emotions\\2023_Xie et al._Brain Activation Differences of Six Basic Emotions Between 2D Screen and Virtual Reality Modalities.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\PRIEMJSZ\\9987528.html:text/html},
}

@article{kerbl2023,
	title = {{3D} {Gaussian} {Splatting} for {Real}-{Time} {Radiance} {Field} {Rendering}},
	volume = {42},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3592433},
	doi = {10.1145/3592433},
	abstract = {Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (≥ 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.},
	language = {en},
	number = {4},
	urldate = {2024-12-11},
	journal = {ACM Transactions on Graphics},
	author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkuehler, Thomas and Drettakis, George},
	month = aug,
	year = {2023},
	pages = {1--14},
	file = {2023_Kerbl et al._3D Gaussian Splatting for Real-Time Radiance Field Rendering:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Hexagon-Aura\\Gaussian Splatting\\2023_Kerbl et al._3D Gaussian Splatting for Real-Time Radiance Field Rendering.pdf:application/pdf},
}

@misc{jiang2024,
	title = {{VR}-{GS}: {A} {Physical} {Dynamics}-{Aware} {Interactive} {Gaussian} {Splatting} {System} in {Virtual} {Reality}},
	shorttitle = {{VR}-{GS}},
	url = {http://arxiv.org/abs/2401.16663},
	doi = {10.48550/arXiv.2401.16663},
	abstract = {As consumer Virtual Reality (VR) and Mixed Reality (MR) technologies gain momentum, there's a growing focus on the development of engagements with 3D virtual content. Unfortunately, traditional techniques for content creation, editing, and interaction within these virtual spaces are fraught with difficulties. They tend to be not only engineering-intensive but also require extensive expertise, which adds to the frustration and inefficiency in virtual object manipulation. Our proposed VR-GS system represents a leap forward in human-centered 3D content interaction, offering a seamless and intuitive user experience. By developing a physical dynamics-aware interactive Gaussian Splatting in a Virtual Reality setting, and constructing a highly efficient two-level embedding strategy alongside deformable body simulations, VR-GS ensures real-time execution with highly realistic dynamic responses. The components of our Virtual Reality system are designed for high efficiency and effectiveness, starting from detailed scene reconstruction and object segmentation, advancing through multi-view image in-painting, and extending to interactive physics-based editing. The system also incorporates real-time deformation embedding and dynamic shadow casting, ensuring a comprehensive and engaging virtual experience.Our project page is available at: https://yingjiang96.github.io/VR-GS/.},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Jiang, Ying and Yu, Chang and Xie, Tianyi and Li, Xuan and Feng, Yutao and Wang, Huamin and Li, Minchen and Lau, Henry and Gao, Feng and Yang, Yin and Jiang, Chenfanfu},
	month = may,
	year = {2024},
	note = {arXiv:2401.16663 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction},
	file = {2024_Jiang et al._VR-GS A Physical Dynamics-Aware Interactive Gaussian Splatting System in Virtual Reality:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Hexagon-Aura\\Gaussian Splatting\\2024_Jiang et al._VR-GS A Physical Dynamics-Aware Interactive Gaussian Splatting System in Virtual Reality.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\3WNXPMUI\\2401.html:text/html},
}

@misc{xie2024,
	title = {{PhysGaussian}: {Physics}-{Integrated} {3D} {Gaussians} for {Generative} {Dynamics}},
	shorttitle = {{PhysGaussian}},
	url = {http://arxiv.org/abs/2311.12198},
	doi = {10.48550/arXiv.2311.12198},
	abstract = {We introduce PhysGaussian, a new method that seamlessly integrates physically grounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel motion synthesis. Employing a custom Material Point Method (MPM), our approach enriches 3D Gaussian kernels with physically meaningful kinematic deformation and mechanical stress attributes, all evolved in line with continuum mechanics principles. A defining characteristic of our method is the seamless integration between physical simulation and visual rendering: both components utilize the same 3D Gaussian kernels as their discrete representations. This negates the necessity for triangle/tetrahedron meshing, marching cubes, "cage meshes," or any other geometry embedding, highlighting the principle of "what you see is what you simulate (WS\${\textasciicircum}2\$)." Our method demonstrates exceptional versatility across a wide variety of materials--including elastic entities, metals, non-Newtonian fluids, and granular materials--showcasing its strong capabilities in creating diverse visual content with novel viewpoints and movements. Our project page is at: https://xpandora.github.io/PhysGaussian/},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Xie, Tianyi and Zong, Zeshun and Qiu, Yuxing and Li, Xuan and Feng, Yutao and Yang, Yin and Jiang, Chenfanfu},
	month = apr,
	year = {2024},
	note = {arXiv:2311.12198 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Graphics},
	file = {2024_Xie et al._PhysGaussian Physics-Integrated 3D Gaussians for Generative Dynamics.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Hexagon-Aura\\Gaussian Splatting\\2024_Xie et al._PhysGaussian Physics-Integrated 3D Gaussians for Generative Dynamics.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\GLZ6JEX9\\2311.html:text/html},
}

@inproceedings{barthel2024,
	title = {Gaussian {Splatting} {Decoder} for {3D}-aware {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/2404.10625},
	doi = {10.1109/CVPRW63382.2024.00794},
	abstract = {NeRF-based 3D-aware Generative Adversarial Networks (GANs) like EG3D or GIRAFFE have shown very high rendering quality under large representational variety. However, rendering with Neural Radiance Fields poses challenges for 3D applications: First, the significant computational demands of NeRF rendering preclude its use on low-power devices, such as mobiles and VR/AR headsets. Second, implicit representations based on neural networks are difficult to incorporate into explicit 3D scenes, such as VR environments or video games. 3D Gaussian Splatting (3DGS) overcomes these limitations by providing an explicit 3D representation that can be rendered efficiently at high frame rates. In this work, we present a novel approach that combines the high rendering quality of NeRF-based 3D-aware GANs with the flexibility and computational advantages of 3DGS. By training a decoder that maps implicit NeRF representations to explicit 3D Gaussian Splatting attributes, we can integrate the representational diversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting for the first time. Additionally, our approach allows for a high resolution GAN inversion and real-time GAN editing with 3D Gaussian Splatting scenes. Project page: florian-barthel.github.io/gaussian\_decoder},
	urldate = {2024-12-12},
	booktitle = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Barthel, Florian and Beckmann, Arian and Morgenstern, Wieland and Hilsmann, Anna and Eisert, Peter},
	month = jun,
	year = {2024},
	note = {arXiv:2404.10625 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {7963--7972},
	file = {2024_Barthel et al._Gaussian Splatting Decoder for 3D-aware Generative Adversarial Networks:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Hexagon-Aura\\Gaussian Splatting\\2024_Barthel et al._Gaussian Splatting Decoder for 3D-aware Generative Adversarial Networks.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\Z46I54ZU\\2404.html:text/html},
}

@misc{zhang2024c,
	title = {{GaRField}++: {Reinforced} {Gaussian} {Radiance} {Fields} for {Large}-{Scale} {3D} {Scene} {Reconstruction}},
	shorttitle = {{GaRField}++},
	url = {http://arxiv.org/abs/2409.12774},
	doi = {10.48550/arXiv.2409.12774},
	abstract = {This paper proposes a novel framework for large-scale scene reconstruction based on 3D Gaussian splatting (3DGS) and aims to address the scalability and accuracy challenges faced by existing methods. For tackling the scalability issue, we split the large scene into multiple cells, and the candidate point-cloud and camera views of each cell are correlated through a visibility-based camera selection and a progressive point-cloud extension. To reinforce the rendering quality, three highlighted improvements are made in comparison with vanilla 3DGS, which are a strategy of the ray-Gaussian intersection and the novel Gaussians density control for learning efficiency, an appearance decoupling module based on ConvKAN network to solve uneven lighting conditions in large-scale scenes, and a refined final loss with the color loss, the depth distortion loss, and the normal consistency loss. Finally, the seamless stitching procedure is executed to merge the individual Gaussian radiance field for novel view synthesis across different cells. Evaluation of Mill19, Urban3D, and MatrixCity datasets shows that our method consistently generates more high-fidelity rendering results than state-of-the-art methods of large-scale scene reconstruction. We further validate the generalizability of the proposed approach by rendering on self-collected video clips recorded by a commercial drone.},
	urldate = {2024-12-18},
	publisher = {arXiv},
	author = {Zhang, Hanyue and Yang, Zhiliu and Zuo, Xinhe and Tong, Yuxin and Long, Ying and Liu, Chen},
	month = sep,
	year = {2024},
	note = {arXiv:2409.12774 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {2024_Zhang et al._GaRField++ Reinforced Gaussian Radiance Fields for Large-Scale 3D Scene Reconstruction:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Hexagon-Aura\\Gaussian Splatting\\2024_Zhang et al._GaRField++ Reinforced Gaussian Radiance Fields for Large-Scale 3D Scene Reconstruction.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\VVRBENA2\\2409.html:text/html},
}

@article{marschik2023,
	title = {Mobile {Solutions} for {Clinical} {Surveillance} and {Evaluation} in {Infancy}—{General} {Movement} {Apps}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2077-0383},
	url = {https://www.mdpi.com/2077-0383/12/10/3576},
	doi = {10.3390/jcm12103576},
	abstract = {The Prechtl General Movements Assessment (GMA) has become a clinician and researcher toolbox for evaluating neurodevelopment in early infancy. Given that it involves the observation of infant movements from video recordings, utilising smartphone applications to obtain these recordings seems like the natural progression for the field. In this review, we look back on the development of apps for acquiring general movement videos, describe the application and research studies of available apps, and discuss future directions of mobile solutions and their usability in research and clinical practice. We emphasise the importance of understanding the background that has led to these developments while introducing new technologies, including the barriers and facilitators along the pathway. The GMApp and Baby Moves apps were the first ones developed to increase accessibility of the GMA, with two further apps, NeuroMotion and InMotion, designed since. The Baby Moves app has been applied most frequently. For the mobile future of GMA, we advocate collaboration to boost the field’s progression and to reduce research waste. We propose future collaborative solutions, including standardisation of cross-site data collection, adaptation to local context and privacy laws, employment of user feedback, and sustainable IT structures enabling continuous software updating.},
	language = {en},
	number = {10},
	urldate = {2025-01-09},
	journal = {Journal of Clinical Medicine},
	author = {Marschik, Peter B. and Kwong, Amanda K. L. and Silva, Nelson and Olsen, Joy E. and Schulte-Rüther, Martin and Bölte, Sven and Örtqvist, Maria and Eeles, Abbey and Poustka, Luise and Einspieler, Christa and Nielsen-Saines, Karin and Zhang, Dajie and Spittle, Alicia J.},
	month = jan,
	year = {2023},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {cerebral palsy, eHealth, general movements (GMs), General Movements Assessment (GMA), infancy, mHealth, smartphone, tele health},
	pages = {3576},
	file = {2023_Marschik et al._Mobile Solutions for Clinical Surveillance and Evaluation in Infancy—General Movement Apps:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\THCS2025\\2023_Marschik et al._Mobile Solutions for Clinical Surveillance and Evaluation in Infancy—General Movement Apps.pdf:application/pdf},
}

@article{ruan2024,
	title = {Can micro-expressions be used as a biomarker for autism spectrum disorder?},
	volume = {18},
	issn = {1662-5196},
	url = {https://www.frontiersin.org/journals/neuroinformatics/articles/10.3389/fninf.2024.1435091/full},
	doi = {10.3389/fninf.2024.1435091},
	abstract = {{\textless}sec{\textgreater}{\textless}title{\textgreater}Introduction{\textless}/title{\textgreater}{\textless}p{\textgreater}Early and accurate diagnosis of autism spectrum disorder (ASD) is crucial for effective intervention, yet it remains a significant challenge due to its complexity and variability. Micro-expressions are rapid, involuntary facial movements indicative of underlying emotional states. It is unknown whether micro-expression can serve as a valid bio-marker for ASD diagnosis.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Methods{\textless}/title{\textgreater}{\textless}p{\textgreater}This study introduces a novel machine-learning (ML) framework that advances ASD diagnostics by focusing on facial micro-expressions. We applied cutting-edge algorithms to detect and analyze these micro-expressions from video data, aiming to identify distinctive patterns that could differentiate individuals with ASD from typically developing peers. Our computational approach included three key components: (1) micro-expression spotting using Shallow Optical Flow Three-stream CNN (SOFTNet), (2) feature extraction via Micron-BERT, and (3) classification with majority voting of three competing models (MLP, SVM, and ResNet).{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Results{\textless}/title{\textgreater}{\textless}p{\textgreater}Despite the sophisticated methodology, the ML framework's ability to reliably identify ASD-specific patterns was limited by the quality of video data. This limitation raised concerns about the efficacy of using micro-expressions for ASD diagnostics and pointed to the necessity for enhanced video data quality.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Discussion{\textless}/title{\textgreater}{\textless}p{\textgreater}Our research has provided a cautious evaluation of micro-expression diagnostic value, underscoring the need for advancements in behavioral imaging and multimodal AI technology to leverage the full capabilities of ML in an ASD-specific clinical context.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}},
	language = {English},
	urldate = {2025-01-09},
	journal = {Frontiers in Neuroinformatics},
	author = {Ruan, Mindi and Zhang, Na and Yu, Xiangxu and Li, Wenqi and Hu, Chuanbo and Webster, Paula J. and K. Paul, Lynn and Wang, Shuo and Li, Xin},
	month = oct,
	year = {2024},
	note = {Publisher: Frontiers},
	keywords = {Autism spectrum disorder (ASD), Autism Diagnostic Observation Schedule (ADOS), Face Videos, Interpretable machine learning, Micro-expressions},
	file = {2024_Ruan et al._Can micro-expressions be used as a biomarker for autism spectrum disorder:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Emotions\\2024_Ruan et al._Can micro-expressions be used as a biomarker for autism spectrum disorder.pdf:application/pdf},
}

@misc{hu2024,
	title = {Exploring {Speech} {Pattern} {Disorders} in {Autism} using {Machine} {Learning}},
	url = {https://ui.adsabs.harvard.edu/abs/2024arXiv240505126H},
	doi = {10.48550/arXiv.2405.05126},
	abstract = {Diagnosing autism spectrum disorder (ASD) by identifying abnormal speech patterns from examiner-patient dialogues presents significant challenges due to the subtle and diverse manifestations of speech-related symptoms in affected individuals. This study presents a comprehensive approach to identify distinctive speech patterns through the analysis of examiner-patient dialogues. Utilizing a dataset of recorded dialogues, we extracted 40 speech-related features, categorized into frequency, zero-crossing rate, energy, spectral characteristics, Mel Frequency Cepstral Coefficients (MFCCs), and balance. These features encompass various aspects of speech such as intonation, volume, rhythm, and speech rate, reflecting the complex nature of communicative behaviors in ASD. We employed machine learning for both classification and regression tasks to analyze these speech features. The classification model aimed to differentiate between ASD and non-ASD cases, achieving an accuracy of 87.75\%. Regression models were developed to predict speech pattern related variables and a composite score from all variables, facilitating a deeper understanding of the speech dynamics associated with ASD. The effectiveness of machine learning in interpreting intricate speech patterns and the high classification accuracy underscore the potential of computational methods in supporting the diagnostic processes for ASD. This approach not only aids in early detection but also contributes to personalized treatment planning by providing insights into the speech and communication profiles of individuals with ASD.},
	urldate = {2025-01-09},
	author = {Hu, Chuanbo and Thrasher, Jacob and Li, Wenqi and Ruan, Mindi and Yu, Xiangxu and Paul, Lynn K and Wang, Shuo and Li, Xin},
	month = may,
	year = {2024},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2024arXiv240505126H},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {2024_Hu et al._Exploring Speech Pattern Disorders in Autism using Machine Learning:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2024_Hu et al._Exploring Speech Pattern Disorders in Autism using Machine Learning.pdf:application/pdf},
}

@article{frye2019,
	title = {Emerging biomarkers in autism spectrum disorder: a systematic review},
	volume = {7},
	issn = {2305-5839},
	shorttitle = {Emerging biomarkers in autism spectrum disorder},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6989979/},
	doi = {10.21037/atm.2019.11.53},
	abstract = {Autism spectrum disorder (ASD) affects approximately 2\% of children in the United States (US) yet its etiology is unclear and effective treatments are lacking. Therapeutic interventions are most effective if started early in life, yet diagnosis often remains delayed, partly because the diagnosis of ASD is based on identifying abnormal behaviors that may not emerge until the disorder is well established. Biomarkers that identify children at risk during the pre-symptomatic period, assist with early diagnosis, confirm behavioral observations, stratify patients into subgroups, and predict therapeutic response would be a great advance. Here we underwent a systematic review of the literature on ASD to identify promising biomarkers and rated the biomarkers in regards to a Level of Evidence and Grade of Recommendation using the Oxford Centre for Evidence-Based Medicine scale. Biomarkers identified by our review included physiological biomarkers that identify neuroimmune and metabolic abnormalities, neurological biomarkers including abnormalities in brain structure, function and neurophysiology, subtle behavioral biomarkers including atypical development of visual attention, genetic biomarkers and gastrointestinal biomarkers. Biomarkers of ASD may be found prior to birth and after diagnosis and some may predict response to specific treatments. Many promising biomarkers have been developed for ASD. However, many biomarkers are preliminary and need to be validated and their role in the diagnosis and treatment of ASD needs to be defined. It is likely that biomarkers will need to be combined to be effective to identify ASD early and guide treatment.},
	number = {23},
	urldate = {2025-01-09},
	journal = {Annals of Translational Medicine},
	author = {Frye, Richard E. and Vassall, Sarah and Kaur, Gurjot and Lewis, Christina and Karim, Mohammand and Rossignol, Daniel},
	month = dec,
	year = {2019},
	pmid = {32042808},
	pmcid = {PMC6989979},
	pages = {792},
	file = {2019_Frye et al._Emerging biomarkers in autism spectrum disorder a systematic review.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Event-Based\\2019_Frye et al._Emerging biomarkers in autism spectrum disorder a systematic review.pdf:application/pdf},
}

@misc{hu2024a,
	title = {Exploiting {ChatGPT} for {Diagnosing} {Autism}-{Associated} {Language} {Disorders} and {Identifying} {Distinct} {Features}},
	url = {http://arxiv.org/abs/2405.01799},
	doi = {10.48550/arXiv.2405.01799},
	abstract = {Diagnosing language disorders associated with autism is a complex challenge, often hampered by the subjective nature and variability of traditional assessment methods. Traditional diagnostic methods not only require intensive human effort but also often result in delayed interventions due to their lack of speed and precision. In this study, we explored the application of ChatGPT, a large language model, to overcome these obstacles by enhancing sensitivity and profiling linguistic features for autism diagnosis. This research utilizes ChatGPT natural language processing capabilities to simplify and improve the diagnostic process, focusing on identifying autism related language patterns. Specifically, we compared ChatGPT performance with that of conventional supervised learning models, including BERT, a model acclaimed for its effectiveness in various natural language processing tasks. We showed that ChatGPT substantially outperformed these models, achieving over 10\% improvement in both sensitivity and positive predictive value, in a zero shot learning configuration. The findings underscore the model potential as a diagnostic tool, combining accuracy and applicability. We identified ten key features of autism associated language disorders across scenarios. Features such as echolalia, pronoun reversal, and atypical language usage play a critical role in diagnosing ASD and informing tailored treatment plans. Together, our findings advocate for adopting sophisticated AI tools like ChatGPT in clinical settings to assess and diagnose developmental disorders. Our approach promises enhanced diagnostic precision and supports personalized medicine, potentially transforming the evaluation landscape for autism and similar neurological conditions.},
	urldate = {2025-01-09},
	publisher = {arXiv},
	author = {Hu, Chuanbo and Li, Wenqi and Ruan, Mindi and Yu, Xiangxu and Deshpande, Shalaka and Paul, Lynn K. and Wang, Shuo and Li, Xin},
	month = nov,
	year = {2024},
	note = {arXiv:2405.01799 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {2024_Hu et al._Exploiting ChatGPT for Diagnosing Autism-Associated Language Disorders and Identifying Distinct Feat:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Event-Based\\2024_Hu et al._Exploiting ChatGPT for Diagnosing Autism-Associated Language Disorders and Identifying Distinct Feat.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\42DPQKQV\\2405.html:text/html},
}

@article{dollion2022,
	title = {Emotion {Facial} {Processing} in {Children} {With} {Autism} {Spectrum} {Disorder}: {A} {Pilot} {Study} of the {Impact} of {Service} {Dogs}},
	volume = {13},
	issn = {1664-1078},
	shorttitle = {Emotion {Facial} {Processing} in {Children} {With} {Autism} {Spectrum} {Disorder}},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2022.869452/full},
	doi = {10.3389/fpsyg.2022.869452},
	abstract = {{\textless}p{\textgreater}Processing and recognizing facial expressions are key factors in human social interaction. Past research suggests that individuals with autism spectrum disorder (ASD) present difficulties to decode facial expressions. Those difficulties are notably attributed to altered strategies in the visual scanning of expressive faces. Numerous studies have demonstrated the multiple benefits of exposure to pet dogs and service dogs on the interaction skills and psychosocial development of children with ASD. However, no study has investigated if those benefits also extend to the processing of facial expressions. The aim of this study was to investigate if having a service dog had an influence on facial expression processing skills of children with ASD. Two groups of 15 children with ASD, with and without a service dog, were compared using a facial expression recognition computer task while their ocular movements were measured using an eye-tracker. While the two groups did not differ in their accuracy and reaction time, results highlighted that children with ASD owning a service dog directed less attention toward areas that were not relevant to facial expression processing. They also displayed a more differentiated scanning of relevant facial features according to the displayed emotion (i.e., they spent more time on the mouth for joy than for anger, and vice versa for the eyes area). Results from the present study suggest that having a service dog and interacting with it on a daily basis may promote the development of specific visual exploration strategies for the processing of human faces.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-01-09},
	journal = {Frontiers in Psychology},
	author = {Dollion, Nicolas and Grandgeorge, Marine and Saint-Amour, Dave and Hosein Poitras Loewen, Anthony and François, Nathe and Fontaine, Nathalie M. G. and Champagne, Noël and Plusquellec, Pierrich},
	month = may,
	year = {2022},
	note = {Publisher: Frontiers},
	keywords = {emotion recognition, Eye-tracking, Autism spectrum disorder (ASD), Facial expression processing, service dog},
	file = {2022_Dollion et al._Emotion Facial Processing in Children With Autism Spectrum Disorder A Pilot Study of the Impact of:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\autism\\2022_Dollion et al._Emotion Facial Processing in Children With Autism Spectrum Disorder A Pilot Study of the Impact of.pdf:application/pdf},
}

@article{eack2015,
	title = {Misinterpretation of facial expressions of emotion in verbal adults with autism spectrum disorder},
	volume = {19},
	issn = {1362-3613},
	url = {https://doi.org/10.1177/1362361314520755},
	doi = {10.1177/1362361314520755},
	abstract = {Facial emotion perception is significantly affected in autism spectrum disorder, yet little is known about how individuals with autism spectrum disorder misinterpret facial expressions that result in their difficulty in accurately recognizing emotion in faces. This study examined facial emotion perception in 45 verbal adults with autism spectrum disorder and 30 age- and gender-matched volunteers without autism spectrum disorder to identify patterns of emotion misinterpretation during face processing that contribute to emotion recognition impairments in autism. Results revealed that difficulty distinguishing emotional from neutral facial expressions characterized much of the emotion perception impairments exhibited by participants with autism spectrum disorder. In particular, adults with autism spectrum disorder uniquely misinterpreted happy faces as neutral, and were significantly more likely than typical volunteers to attribute negative valence to nonemotional faces. The over-attribution of emotions to neutral faces was significantly related to greater communication and emotional intelligence impairments in individuals with autism spectrum disorder. These findings suggest a potential negative bias toward the interpretation of facial expressions and may have implications for interventions designed to remediate emotion perception in autism spectrum disorder.},
	language = {en},
	number = {3},
	urldate = {2025-01-09},
	journal = {Autism},
	author = {Eack, Shaun M and Mazefsky, Carla A and Minshew, Nancy J},
	month = apr,
	year = {2015},
	note = {Publisher: SAGE Publications Ltd},
	pages = {308--315},
	file = {2015_Eack et al._Misinterpretation of facial expressions of emotion in verbal adults with autism spectrum disorder.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\autism\\2015_Eack et al._Misinterpretation of facial expressions of emotion in verbal adults with autism spectrum disorder.pdf:application/pdf},
}

@article{jensen2022,
	title = {Modern {Biomarkers} for {Autism} {Spectrum} {Disorder}: {Future} {Directions}},
	volume = {26},
	issn = {1177-1062},
	shorttitle = {Modern {Biomarkers} for {Autism} {Spectrum} {Disorder}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9411091/},
	doi = {10.1007/s40291-022-00600-7},
	abstract = {Autism spectrum disorder is an increasingly prevalent neurodevelopmental disorder in the world today, with an estimated 2\% of the population being affected in the USA. A major complicating factor in diagnosing, treating, and understanding autism spectrum disorder is that defining the disorder is solely based on the observation of behavior. Thus, recent research has focused on identifying specific biological abnormalities in autism spectrum disorder that can provide clues to diagnosis and treatment. Biomarkers are an objective way to identify and measure biological abnormalities for diagnostic purposes as well as to measure changes resulting from treatment. This current opinion paper discusses the state of research of various biomarkers currently in development for autism spectrum disorder. The types of biomarkers identified include prenatal history, genetics, neurological including neuroimaging, neurophysiologic, and visual attention, metabolic including abnormalities in mitochondrial, folate, trans-methylation, and trans-sulfuration pathways, immune including autoantibodies and cytokine dysregulation, autonomic nervous system, and nutritional. Many of these biomarkers have promising preliminary evidence for prenatal and post-natal pre-symptomatic risk assessment, confirmation of diagnosis, subtyping, and treatment response. However, most biomarkers have not undergone validation studies and most studies do not investigate biomarkers with clinically relevant comparison groups. Although the field of biomarker research in autism spectrum disorder is promising, it appears that it is currently in the early stages of development.},
	number = {5},
	urldate = {2025-01-09},
	journal = {Molecular Diagnosis \& Therapy},
	author = {Jensen, Amanda R. and Lane, Alison L. and Werner, Brianna A. and McLees, Sallie E. and Fletcher, Tessa S. and Frye, Richard E.},
	year = {2022},
	pmid = {35759118},
	pmcid = {PMC9411091},
	pages = {483--495},
	file = {2022_Jensen et al._Modern Biomarkers for Autism Spectrum Disorder Future Directions.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\autism\\2022_Jensen et al._Modern Biomarkers for Autism Spectrum Disorder Future Directions.pdf:application/pdf},
}

@article{mason2021,
	title = {Preference for biological motion is reduced in {ASD}: implications for clinical trials and the search for biomarkers},
	volume = {12},
	issn = {2040-2392},
	shorttitle = {Preference for biological motion is reduced in {ASD}},
	url = {https://doi.org/10.1186/s13229-021-00476-0},
	doi = {10.1186/s13229-021-00476-0},
	abstract = {The neurocognitive mechanisms underlying autism spectrum disorder (ASD) remain unclear. Progress has been largely hampered by small sample sizes, variable age ranges and resulting inconsistent findings. There is a pressing need for large definitive studies to delineate the nature and extent of key case/control differences to direct research towards fruitful areas for future investigation. Here we focus on perception of biological motion, a promising index of social brain function which may be altered in ASD. In a large sample ranging from childhood to adulthood, we assess whether biological motion preference differs in ASD compared to neurotypical participants (NT), how differences are modulated by age and sex and whether they are associated with dimensional variation in concurrent or later symptomatology.},
	language = {en},
	number = {1},
	urldate = {2025-01-09},
	journal = {Molecular Autism},
	author = {Mason, L. and Shic, F. and Falck-Ytter, T. and Chakrabarti, B. and Charman, T. and Loth, E. and Tillmann, J. and Banaschewski, T. and Baron-Cohen, S. and Bölte, S. and Buitelaar, J. and Durston, S. and Oranje, B. and Persico, A. M. and Beckmann, C. and Bougeron, T. and Dell’Acqua, F. and Ecker, C. and Moessnang, C. and Murphy, D. and Johnson, M. H. and Jones, E. J. H. and Ahmad, Jumana and Ambrosino, Sara and Baumeister, Sarah and Bours, Carsten and Brammer, Michael and Brandeis, Daniel and Brogna, Claudia and de Bruijn, Yvette and Chatham, Chris and Cornelissen, Ineke and Crawley, Daisy and Dumas, Guillaume and Faulkner, Jessica and Frouin, Vincent and Garcés, Pilar and Goyard, David and Ham, Lindsay and Hipp, Joerg and Holt, Rosemary and Lai, Meng-Chuan and D’ardhuy, Xavier Liogier and Lombardo, Michael V. and Lythgoe, David J. and Mandl, René and Marquand, Andre and Mennes, Maarten and Meyer-Lindenberg, Andreas and Bast, Nico and Oakley, Bethany and O’Dwyer, Laurence and Oldehinkel, Marianne and Pandina, Gahan and Ruggeri, Barbara and Ruigrok, Amber and Sabet, Jessica and Sacco, Roberto and Cáceres, Antonia San José and Simonoff, Emily and Spooren, Will and Toro, Roberto and Tost, Heike and Waldman, Jack and Williams, Steve C. R. and Wooldridge, Caroline and Zwiers, Marcel P. and {the LEAP Team*}},
	month = dec,
	year = {2021},
	keywords = {Autism, Eye tracking, Biomarker, Development, Biological motion},
	pages = {74},
	file = {2021_Mason et al._Preference for biological motion is reduced in ASD implications for clinical trials and the search.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\autism\\2021_Mason et al._Preference for biological motion is reduced in ASD implications for clinical trials and the search.pdf:application/pdf},
}

@misc{bensch2024,
	title = {{AI} {Assistants} for {Spaceflight} {Procedures}: {Combining} {Generative} {Pre}-{Trained} {Transformer} and {Retrieval}-{Augmented} {Generation} on {Knowledge} {Graphs} {With} {Augmented} {Reality} {Cues}},
	shorttitle = {{AI} {Assistants} for {Spaceflight} {Procedures}},
	url = {http://arxiv.org/abs/2409.14206},
	doi = {10.48550/arXiv.2409.14206},
	abstract = {This paper describes the capabilities and potential of the intelligent personal assistant (IPA) CORE (Checklist Organizer for Research and Exploration), designed to support astronauts during procedures onboard the International Space Station (ISS), the Lunar Gateway station, and beyond. We reflect on the importance of a reliable and flexible assistant capable of offline operation and highlight the usefulness of audiovisual interaction using augmented reality elements to intuitively display checklist information. We argue that current approaches to the design of IPAs in space operations fall short of meeting these criteria. Therefore, we propose CORE as an assistant that combines Knowledge Graphs (KGs), Retrieval-Augmented Generation (RAG) for a Generative Pre-Trained Transformer (GPT), and Augmented Reality (AR) elements to ensure an intuitive understanding of procedure steps, reliability, offline availability, and flexibility in terms of response style and procedure updates.},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Bensch, Oliver and Bensch, Leonie and Nilsson, Tommy and Saling, Florian and Bewer, Bernd and Jentzsch, Sophie and Hecking, Tobias and Kutz, J. Nathan},
	month = sep,
	year = {2024},
	note = {arXiv:2409.14206 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {2024_Bensch et al._AI Assistants for Spaceflight Procedures Combining Generative Pre-Trained Transformer and Retrieval:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2024_Bensch et al._AI Assistants for Spaceflight Procedures Combining Generative Pre-Trained Transformer and Retrieval.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\E6GTI922\\2409.html:text/html},
}

@article{zhu2024,
	title = {Multi-{Modal} {Knowledge} {Graph} {Construction} and {Application}: {A} {Survey}},
	volume = {36},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {Multi-{Modal} {Knowledge} {Graph} {Construction} and {Application}},
	url = {http://arxiv.org/abs/2202.05786},
	doi = {10.1109/TKDE.2022.3224228},
	abstract = {Recent years have witnessed the resurgence of knowledge engineering which is featured by the fast growth of knowledge graphs. However, most of existing knowledge graphs are represented with pure symbols, which hurts the machine's capability to understand the real world. The multi-modalization of knowledge graphs is an inevitable key step towards the realization of human-level machine intelligence. The results of this endeavor are Multi-modal Knowledge Graphs (MMKGs). In this survey on MMKGs constructed by texts and images, we first give definitions of MMKGs, followed with the preliminaries on multi-modal tasks and techniques. We then systematically review the challenges, progresses and opportunities on the construction and application of MMKGs respectively, with detailed analyses of the strength and weakness of different solutions. We finalize this survey with open research problems relevant to MMKGs.},
	number = {2},
	urldate = {2025-01-10},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhu, Xiangru and Li, Zhixu and Wang, Xiaodan and Jiang, Xueyao and Sun, Penglei and Wang, Xuwu and Xiao, Yanghua and Yuan, Nicholas Jing},
	month = feb,
	year = {2024},
	note = {arXiv:2202.05786 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {715--735},
	file = {2024_Zhu et al._Multi-Modal Knowledge Graph Construction and Application A Survey:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2024_Zhu et al._Multi-Modal Knowledge Graph Construction and Application A Survey.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\9NHQN4CK\\2202.html:text/html},
}

@article{zhang2025,
	title = {{LLM}-{TSFD}: {An} industrial time series human-in-the-loop fault diagnosis method based on a large language model},
	volume = {264},
	issn = {0957-4174},
	shorttitle = {{LLM}-{TSFD}},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417424027283},
	doi = {10.1016/j.eswa.2024.125861},
	abstract = {Industrial time series data provides real-time information about the operational status of equipment and helps identify anomalies. Data-driven and knowledge-guided methods have become predominant in this field. However, these methods depend on industrial domain knowledge and high-quality industrial data which can lead to issues such as unclear diagnostic results and lengthy development cycles. This paper introduces a novel human-in-the-loop task-driven approach to reduce reliance on manually annotated data and improve the interpretability of diagnostic outcomes. This approach utilises a large language model for fault detection, fostering process autonomy and enhancing human–machine collaboration. Furthermore, this paper explores four key roles of the large language model: managing the data pipeline, correcting causality, controlling model management, and making decisions about diagnostic results. Additionally, it presents a prompt structure designed for fault diagnosis of time series data, enabling the large language model to realize task-driven. Finally, the paper validates the proposed framework through a case study in the context of steel metallurgy.},
	urldate = {2025-01-10},
	journal = {Expert Systems with Applications},
	author = {Zhang, Qi and Xu, Chao and Li, Jie and Sun, Yicheng and Bao, Jinsong and Zhang, Dan},
	month = mar,
	year = {2025},
	keywords = {Fault diagnosis 2.0, Human-in-the-loop, Large language model, Task-driven, Time series},
	pages = {125861},
	file = {2025_Zhang et al._LLM-TSFD An industrial time series human-in-the-loop fault diagnosis method based on a large langua:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2025_Zhang et al._LLM-TSFD An industrial time series human-in-the-loop fault diagnosis method based on a large langua.pdf:application/pdf},
}

@article{devries2021,
	title = {Autism spectrum disorder and pupillometry: {A} systematic review and meta-analysis},
	volume = {120},
	issn = {0149-7634},
	shorttitle = {Autism spectrum disorder and pupillometry},
	url = {https://www.sciencedirect.com/science/article/pii/S014976342030590X},
	doi = {10.1016/j.neubiorev.2020.09.032},
	abstract = {Pupillometry, measuring pupil size and reactivity, has been proposed as a measure of autonomic nervous system functioning, the latter which might be altered in individuals with autism spectrum disorder (ASD). This study aims to evaluate if pupillary responses differ in individuals with and without ASD. After performing a systematic literature search, we conducted a meta-analysis and constructed a qualitative synthesis. The meta-analysis shows a longer latency of the pupil response in the ASD-group as a substantial group difference, with a Hedges’ g of 1.03 (95\% CI 0.49–1.56, p = 0.008). Evidence on baseline pupil size and amplitude change is conflicting. We used the framework method to perform a qualitative evaluation of these differences. Explanations for the group differences vary between studies and are inconclusive, but many authors point to involvement of the autonomous nervous system and more specifically the locus coeruleus-norepinephrine system. Pupillometry reveals differences between people with and without ASD, but the exact meaning of these differences remains unknown. Future studies should align research designs and investigate a possible effect of maturation.},
	urldate = {2025-01-14},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {de Vries, Lyssa and Fouquaet, Iris and Boets, Bart and Naulaers, Gunnar and Steyaert, Jean},
	month = jan,
	year = {2021},
	keywords = {Eye-tracking, Autism spectrum disorder, Meta-analysis, Autonomic nervous system, Pupillometry, Qualitative research},
	pages = {479--508},
	file = {2021_de Vries et al._Autism spectrum disorder and pupillometry A systematic review and meta-analysis:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\autism\\2021_de Vries et al._Autism spectrum disorder and pupillometry A systematic review and meta-analysis.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\I7QHTZW9\\S014976342030590X.html:text/html},
}

@article{sun2024,
	title = {Interest paradigm for early identification of autism spectrum disorder: an analysis from electroencephalography combined with eye tracking},
	volume = {18},
	issn = {1662-4548},
	shorttitle = {Interest paradigm for early identification of autism spectrum disorder},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11631861/},
	doi = {10.3389/fnins.2024.1502045},
	abstract = {Introduction
Early identification of Autism Spectrum Disorder (ASD) is critical for effective intervention. Restricted interests (RIs), a subset of repetitive behaviors, are a prominent but underutilized domain for early ASD diagnosis. This study aimed to identify objective biomarkers for ASD by integrating electroencephalography (EEG) and eye-tracking (ET) to analyze toddlers’ visual attention and cortical responses to RI versus neutral interest (NI) objects.

Methods
The study involved 59 toddlers aged 2-4 years, including 32 with ASD and 27 non-ASD controls. Participants underwent a 24-object passive viewing paradigm, featuring RI (e.g., transportation items) and NI objects (e.g., balloons). ET metrics (fixation time and pupil size) and EEG time-frequency (TF) power in theta (4-8 Hz) and alpha (8-13 Hz) bands were analyzed. Statistical methods included logistic regression models to assess the predictive potential of combined EEG and ET biomarkers.

Results
Toddlers with ASD exhibited significantly increased fixation times and pupil sizes for RI objects compared to NI objects, alongside distinct EEG patterns with elevated theta and reduced alpha power in occipital regions during RI stimuli. The multimodal logistic regression model, incorporating EEG and ET metrics, achieved an area under the curve (AUC) of 0.75, demonstrating robust predictive capability for ASD.

Discussion
This novel integration of ET and EEG metrics highlights the potential of RIs as diagnostic markers for ASD. The observed neural and attentional distinctions underscore the utility of multimodal biomarkers for early diagnosis and personalized intervention strategies. Future work should validate findings across broader age ranges and diverse populations.},
	urldate = {2025-01-14},
	journal = {Frontiers in Neuroscience},
	author = {Sun, Binbin and Calvert, Elombe Issa and Ye, Alyssa and Mao, Heng and Liu, Kevin and Wang, Raymond Kong and Wang, Xin-Yuan and Wu, Zhi-Liu and Wei, Zhen and Kong, Xue-jun},
	month = nov,
	year = {2024},
	pmid = {39664447},
	pmcid = {PMC11631861},
	pages = {1502045},
	file = {2024_Sun et al._Interest paradigm for early identification of autism spectrum disorder an analysis from electroence:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\autism\\2024_Sun et al._Interest paradigm for early identification of autism spectrum disorder an analysis from electroence.pdf:application/pdf},
}

@article{wei2024b,
	title = {Early identification of autism spectrum disorder based on machine learning with eye-tracking data},
	volume = {358},
	issn = {0165-0327},
	url = {https://www.sciencedirect.com/science/article/pii/S0165032724006554},
	doi = {10.1016/j.jad.2024.04.049},
	abstract = {Background
Early identification of autism spectrum disorder (ASD) improves long-term outcomes, yet significant diagnostic delays persist.
Methods
A retrospective cohort of 449 children (ASD: 246, typically developing [TD]: 203) was used for model development. Eye-movement data were collected from the participants watching videos that featured eye-tracking paradigms for assessing social and non-social cognition. Five machine learning algorithms, namely random forest, support vector machine, logistic regression, artificial neural network, and extreme gradient boosting, were trained to classify children with ASD and TD. The best-performing algorithm was selected to build the final model which was further evaluated in a prospective cohort of 80 children. The Shapley values interpreted important eye-tracking features.
Results
Random forest outperformed other algorithms during model development and achieved an area under the curve of 0.849 ({\textless} 3 years: 0.832, ≥ 3 years: 0.868) on the external validation set. Of the ten most important eye-tracking features, three measured social cognition, and the rest were related to non-social cognition. A deterioration in model performance was observed using only the social or non-social cognition-related eye-tracking features.
Limitations
The sample size of this study, although larger than that of existing studies of ASD based on eye-tracking data, was still relatively small compared to the number of features.
Conclusions
Machine learning models based on eye-tracking data have the potential to be cost- and time-efficient digital tools for the early identification of ASD. Eye-tracking phenotypes related to social and non-social cognition play an important role in distinguishing children with ASD from TD children.},
	urldate = {2025-01-14},
	journal = {Journal of Affective Disorders},
	author = {Wei, Qiuhong and Dong, Wenxin and Yu, Dongchuan and Wang, Ke and Yang, Ting and Xiao, Yuanjie and Long, Dan and Xiong, Haiyi and Chen, Jie and Xu, Ximing and Li, Tingyu},
	month = aug,
	year = {2024},
	keywords = {Machine learning, Eye-tracking, Autism spectrum disorder},
	pages = {326--334},
	file = {ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\XFS78GD6\\S0165032724006554.html:text/html},
}

@article{saputra,
	title = {How to {Prevent} {Student} {Mental} {Health} {Problems} in {Metaverse} {Era}?},
	url = {https://www.academia.edu/103827113/How_to_Prevent_Student_Mental_Health_Problems_in_Metaverse_Era},
	abstract = {Metaverse, a combination of social media, gaming, and virtual reality technology innovation, is a new concept that has attracted the attention of various groups. This concept has also aided numerous groups of people, including the academician. As},
	language = {en},
	urldate = {2025-01-16},
	journal = {Jurnal Kajian Bimbingan dan Konseling},
	author = {Saputra, Nur Mega Aris},
	file = {Saputra_How to Prevent Student Mental Health Problems in Metaverse Era:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Metaverse\\Saputra_How to Prevent Student Mental Health Problems in Metaverse Era.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\8J5FXVCN\\How_to_Prevent_Student_Mental_Health_Problems_in_Metaverse_Era.html:text/html},
}

@article{abbasi2023,
	title = {Deep-learning for automated markerless tracking of infants general movements},
	volume = {15},
	issn = {2511-2112},
	url = {https://doi.org/10.1007/s41870-023-01497-z},
	doi = {10.1007/s41870-023-01497-z},
	abstract = {The presence of abnormal infant General Movements (GMs) is a strong predictor of progressive neurodevelopmental disorders, including cerebral palsy (CP). Automation of the assessment will overcome scalability barriers that limit its delivery to at-risk individuals. Here, we report a robust markerless pose-estimation scheme, based on advanced deep-learning technology, to track infant movements in consumer mobile device video recordings. Two deep neural network models, namely Efficientnet-b6 and resnet-152, were trained on manually annotated data across twelve anatomical locations (3 per limb) in 12 videos from 6 full-term infants (mean age = 17.33 (SD 2.9) wks, 4 male, 2 female), using the DeepLabCut™ framework. K-fold cross-validation indicates the generalization capability of the deep-nets for GM tracking on out-of-domain data with an overall performance of 95.52\% (SD 2.43) from the best performing model (Efficientnet-b6) across all infants (performance range: 84.32–99.24\% across all anatomical locations). The paper further introduces an automatic, unsupervised strategy for performance evaluation on extensive out-of-domain recordings through a fusion of likelihoods from a Kalman filter and the deep-net. Findings indicate the possibility of establishing an automated GM tracking platform, as a suitable alternative to, or support for, the current observational protocols for early diagnosis of neurodevelopmental disorders in early infancy.},
	language = {en},
	number = {8},
	urldate = {2025-01-22},
	journal = {International Journal of Information Technology},
	author = {Abbasi, H. and Mollet, S. R. and Williams, S. A. and Lim, L. and Battin, M. R. and Besier, T. F. and McMorland, A. J. C.},
	month = dec,
	year = {2023},
	keywords = {Artificial Intelligence, Image processing, Neurodevelopmental Disorders, Automated markerless motion tracking, Convolutional neural networks (CNN), Deep-learning, DeepLabCut, Motion capture, Neonatal general movement assessment (GMA)},
	pages = {4073--4083},
	file = {2023_Abbasi et al._Deep-learning for automated markerless tracking of infants general movements.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\THCS2025\\2023_Abbasi et al._Deep-learning for automated markerless tracking of infants general movements.pdf:application/pdf},
}

@article{shin2022,
	title = {Deep learning-based quantitative analyses of spontaneous movements and their association with early neurological development in preterm infants},
	volume = {12},
	issn = {2045-2322},
	url = {https://doi.org/10.1038/s41598-022-07139-x},
	doi = {10.1038/s41598-022-07139-x},
	abstract = {This study aimed to develop quantitative assessments of spontaneous movements in high-risk preterm infants based on a deep learning algorithm. Video images of spontaneous movements were recorded in very preterm infants at the term-equivalent age. The Hammersmith Infant Neurological Examination (HINE) was performed in infants at 4 months of corrected age. Joint positional data were extracted using a pretrained pose-estimation model. Complexity and similarity indices of joint angle and angular velocity in terms of sample entropy and Pearson correlation coefficient were compared between the infants with HINE {\textless} 60 and ≥ 60. Video images of spontaneous movements were recorded in 65 preterm infants at term-equivalent age. Complexity indices of joint angles and angular velocities differed between the infants with HINE {\textless} 60 and ≥ 60 and correlated positively with HINE scores in most of the joints at the upper and lower extremities (p {\textless} 0.05). Similarity indices between each joint angle or joint angular velocity did not differ between the two groups in most of the joints at the upper and lower extremities. Quantitative assessments of spontaneous movements in preterm infants are feasible using a deep learning algorithm and sample entropy. The results indicated that complexity indices of joint movements at both the upper and lower extremities can be potential candidates for detecting developmental outcomes in preterm infants.},
	language = {en},
	number = {1},
	urldate = {2025-01-22},
	journal = {Scientific Reports},
	author = {Shin, Hyun Iee and Shin, Hyung-Ik and Bang, Moon Suk and Kim, Don-Kyu and Shin, Seung Han and Kim, Ee-Kyung and Kim, Yoo-Jin and Lee, Eun Sun and Park, Seul Gi and Ji, Hye Min and Lee, Woo Hyung},
	month = feb,
	year = {2022},
	pages = {3138},
	file = {2022_Shin et al._Deep learning-based quantitative analyses of spontaneous movements and their association with early.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\THCS2025\\2022_Shin et al._Deep learning-based quantitative analyses of spontaneous movements and their association with early.pdf:application/pdf},
}

@misc{cotton2023,
	title = {Dynamic {Gaussian} {Splatting} from {Markerless} {Motion} {Capture} can {Reconstruct} {Infants} {Movements}},
	url = {http://arxiv.org/abs/2310.19441},
	doi = {10.48550/arXiv.2310.19441},
	abstract = {Easy access to precise 3D tracking of movement could benefit many aspects of rehabilitation. A challenge to achieving this goal is that while there are many datasets and pretrained algorithms for able-bodied adults, algorithms trained on these datasets often fail to generalize to clinical populations including people with disabilities, infants, and neonates. Reliable movement analysis of infants and neonates is important as spontaneous movement behavior is an important indicator of neurological function and neurodevelopmental disability, which can help guide early interventions. We explored the application of dynamic Gaussian splatting to sparse markerless motion capture (MMC) data. Our approach leverages semantic segmentation masks to focus on the infant, significantly improving the initialization of the scene. Our results demonstrate the potential of this method in rendering novel views of scenes and tracking infant movements. This work paves the way for advanced movement analysis tools that can be applied to diverse clinical populations, with a particular emphasis on early detection in infants.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Cotton, R. James and Peyton, Colleen},
	month = oct,
	year = {2023},
	note = {arXiv:2310.19441 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {2023_Cotton and Peyton_Dynamic Gaussian Splatting from Markerless Motion Capture can Reconstruct Infants Movements:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\THCS2025\\2023_Cotton and Peyton_Dynamic Gaussian Splatting from Markerless Motion Capture can Reconstruct Infants Movements.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\3TVPEM4T\\2310.html:text/html},
}

@inproceedings{abbasi2023a,
	title = {Deep-{Learning} {Markerless} {Tracking} of {Infant} {General} {Movements} using {Standard} {Video} {Recordings}},
	url = {https://ieeexplore.ieee.org/abstract/document/10340116},
	doi = {10.1109/EMBC40787.2023.10340116},
	abstract = {Monitoring spontaneous General Movements (GM) of infants 6–20 weeks post-term age is a reliable tool to assess the quality of neurodevelopment in early infancy. Abnormal or absent GMs are reliable prognostic indicators of whether an infant is at risk of developing neurological impairments and disorders such as cerebral palsy (CP). Therapeutic interventions are most effective at improving neuromuscular outcomes if administered in early infancy. Current clinical protocols require trained assessors to rate videos of infant movements, a time-intensive task. This work proposes a simple, inexpensive, and broadly applicable markerless pose-estimation approach for automatic infant movement tracking using conventional video recordings from handheld devices (e.g., tablets and mobile phones). We leverage the enhanced capabilities of deep-learning technology in image processing to identify 12 anatomical locations (3 per limb) in each video frame, tracking a baby’s natural movement throughout the recordings. We validate the capability of resnet152 and a mobile-net-v2-1 to identify body-parts in unseen frames from a full-term male infant, using a novel automatic unsupervised approach that fuses likelihood outputs of a Kalman filter and the deep-nets. Both deep-net models were found to perform very well in the identification of anatomical locations in the unseen data with high average Percentage of Correct Keypoints (aPCK) performances of {\textgreater}99.65\% across all locations.Clinical relevance—Results of this research confirm the feasibility of a low-cost and publicly accessible technology to automatically track infants’ GMs and diagnose those at higher risk of developing neurological conditions early, when clinical interventions are most effective.},
	urldate = {2025-01-22},
	booktitle = {2023 45th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} \& {Biology} {Society} ({EMBC})},
	author = {Abbasi, Hamid and Mollet, Sarah R. and Williams, Sîan A. and Lim, Lilian and Battin, Malcolm R. and Besier, Thor F. and McMorland, Angus J.C.},
	month = jul,
	year = {2023},
	note = {ISSN: 2694-0604},
	keywords = {Tracking, Protocols, Reliability, Kalman filters, Neuromuscular, Pediatrics, Performance evaluation},
	pages = {1--4},
	file = {2023_Abbasi et al._Deep-Learning Markerless Tracking of Infant General Movements using Standard Video Recordings:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\THCS2025\\2023_Abbasi et al._Deep-Learning Markerless Tracking of Infant General Movements using Standard Video Recordings.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\RSXA69GW\\10340116.html:text/html},
}

@article{zhang2025a,
	title = {Exploring {Eye}-tracking based {Biomarkers} to {Assess} {Cognitive} {Abilities} in {Autistic} {Children}: {A} {Feasibility} {Study}},
	issn = {2168-2208},
	shorttitle = {Exploring {Eye}-tracking based {Biomarkers} to {Assess} {Cognitive} {Abilities} in {Autistic} {Children}},
	url = {https://ieeexplore.ieee.org/abstract/document/10845174},
	doi = {10.1109/JBHI.2025.3531421},
	abstract = {Cognitive assessment can reveal a person's cognitive processing and behavioral patterns, making it an indispensable component of autism intervention and prognosis. Existing machine-assisted cognitive assessment methods primarily focus on children's performance outcomes, overlooking distinctive behavioral models, particularly characteristics of eye movement behavior, which have been demonstrated as the most direct indicators of cognitive abilities. In this study, we explore eye-tracking biomarkers for assisting cognitive assessment through a series of meticulously designed multi-level human-computer interaction protocols, encompassing three cognitive abilities: pairing and categorization, emotion recognition, and social interaction. A platform embedded with an eye-tracking module has been developed to reliably collect and analyze eye movement data, even in the presence of unrestricted large head movements in children. Experimental results indicate that there are significant group differences between autism and typically developing children in the eye-tracking features of total fixation duration, response latency, time to first fixation, mean fixation duration, and visit count in the absence of significant intergroup differences in the Wechsler Preschool and Primary Scale of Intelligence (WPPSI) and Wechsler Intelligence Scale for Children (WISC) assessment results. In addition, certain eye movement features in each group are correlated with WPPSI/WISC scale scores, enabling clinical cognitive assessments within each group based on these eye movement features. This study suggests that using eye-tracking features as biomarkers to assist detailed cognitive assessments holds significant potential for the intervention and prognosis of autism.},
	urldate = {2025-01-28},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Zhang, Hanlin and Hu, Chunchun and Wang, Zhiyong and Zhou, Bingrui and Wang, Xinming and Nie, Wei and Ye, Qinyi and Lin, Ruihan and Xu, Xiu and Liu, Honghai},
	year = {2025},
	note = {Conference Name: IEEE Journal of Biomedical and Health Informatics},
	keywords = {Autism, human-computer interaction, eye tracking, Gaze tracking, Emotion recognition, Visualization, Games, Protocols, Cognition, Biomarkers, autism spectrum disorder, Bioinformatics, cognitive assessment, Magnetic heads, obsidian},
	pages = {1--14},
	file = {2025_Zhang et al._Exploring Eye-tracking based Biomarkers to Assess Cognitive Abilities in Autistic Children A Feasib.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2025_Zhang et al._Exploring Eye-tracking based Biomarkers to Assess Cognitive Abilities in Autistic Children A Feasib.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\ADSWJ63D\\10845174.html:text/html},
}

@misc{yu2024,
	title = {Video-based {Analysis} {Reveals} {Atypical} {Social} {Gaze} in {People} with {Autism} {Spectrum} {Disorder}},
	url = {http://arxiv.org/abs/2409.00664},
	doi = {10.48550/arXiv.2409.00664},
	abstract = {In this study, we present a quantitative and comprehensive analysis of social gaze in people with autism spectrum disorder (ASD). Diverging from traditional first-person camera perspectives based on eye-tracking technologies, this study utilizes a third-person perspective database from the Autism Diagnostic Observation Schedule, 2nd Edition (ADOS-2) interview videos, encompassing ASD participants and neurotypical individuals as a reference group. Employing computational models, we extracted and processed gaze-related features from the videos of both participants and examiners. The experimental samples were divided into three groups based on the presence of social gaze abnormalities and ASD diagnosis. This study quantitatively analyzed four gaze features: gaze engagement, gaze variance, gaze density map, and gaze diversion frequency. Furthermore, we developed a classifier trained on these features to identify gaze abnormalities in ASD participants. Together, we demonstrated the effectiveness of analyzing social gaze in people with ASD in naturalistic settings, showcasing the potential of third-person video perspectives in enhancing ASD diagnosis through gaze analysis.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Yu, Xiangxu and Ruan, Mindi and Hu, Chuanbo and Li, Wenqi and Paul, Lynn K. and Li, Xin and Wang, Shuo},
	month = sep,
	year = {2024},
	note = {arXiv:2409.00664 [q-bio]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, obsidian},
	file = {2024_Yu et al._Video-based Analysis Reveals Atypical Social Gaze in People with Autism Spectrum Disorder.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2024_Yu et al._Video-based Analysis Reveals Atypical Social Gaze in People with Autism Spectrum Disorder.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\N6CXXLWD\\2409.html:text/html},
}

@misc{ricou2024,
	title = {Invariant response to faces in {ASD}: unexpected trajectory of oculo-pupillometric biomarkers from childhood to adulthoo},
	shorttitle = {Invariant response to faces in {ASD}},
	url = {https://doi.org/10.31234/osf.io/vfte9},
	doi = {10.31234/osf.io/vfte9},
	abstract = {Human faces contain a large amount of information, attracting our attention and inducing physiological engagement. Attraction to human faces can be observed from birth and develops with age and social experience. The oculometric (visual exploration) and pupillometric (physiological reactivity) parameters quantified by eye-tracking can be relevant measures to study attractiveness and engagement with human faces. Autistic people have particularities in terms of visual exploration and physiological reactivity to faces, with a reduction in the time spent on the eyes associated with a reduced pupil dilation. To date, no study has assessed the developmental dynamics of oculo-pupillometric parameters in response to faces. This study aimed to characterize these parameters throughout typical and autistic development. 109 autistic participants (3-34 years old) were compared to 150 neurotypical participants (3-32 years old). Visual stimuli were organized along a gradient of social saliency, going from static objects to static neutral faces, dynamic neutral faces and dynamic emotional faces. Ocular exploration and physiological reactivity in response to faces appear invariant throughout life in the autistic population. Their developmental dynamics differ from those of the neurotypical population, which shows first, an increasing attentional focus on the eye region with age, and second, a pupillary hypersensitivity to social salient stimuli at an early age which then decreases linearly. Our results highlight an apparent lack of maturation of face processing in the case of autism spectrum disorder (ASD) at the population level, possibly hiding atypical and complex individual developmental trajectories. On the other hand, the neurotypical population exhibits a maturation of both oculometric and pupillometric parameters, pointing towards limited age windows in which specific parameters could be used as discriminating biomarkers of ASD.},
	urldate = {2025-02-03},
	author = {Ricou, Camille and Mofid, Yassine and Roché, Laetitia and Bufo, Maria Rosa and Houy-Durand, Emmanuelle and Malvy, Joëlle and Lemaire, Mathieu and Elian, Jean-Claude and Martineau, Joëlle and Bonnet-Brilhault, Frederique and Wardak, Claire and Aguillon-Hernandez, Nadia},
	year = {2024},
	keywords = {obsidian},
	file = {2024_Ricou et al._Invariant response to faces in ASD unexpected trajectory of oculo-pupillometric biomarkers from chi.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\2024_Ricou et al._Invariant response to faces in ASD unexpected trajectory of oculo-pupillometric biomarkers from chi.pdf:application/pdf},
}

@article{perochon2023a,
	title = {Early detection of autism using digital behavioral phenotyping},
	volume = {29},
	copyright = {2023 The Author(s)},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-023-02574-3},
	doi = {10.1038/s41591-023-02574-3},
	abstract = {Early detection of autism, a neurodevelopmental condition associated with challenges in social communication, ensures timely access to intervention. Autism screening questionnaires have been shown to have lower accuracy when used in real-world settings, such as primary care, as compared to research studies, particularly for children of color and girls. Here we report findings from a multiclinic, prospective study assessing the accuracy of an autism screening digital application (app) administered during a pediatric well-child visit to 475 (17–36 months old) children (269 boys and 206 girls), of which 49 were diagnosed with autism and 98 were diagnosed with developmental delay without autism. The app displayed stimuli that elicited behavioral signs of autism, quantified using computer vision and machine learning. An algorithm combining multiple digital phenotypes showed high diagnostic accuracy with the area under the receiver operating characteristic curve = 0.90, sensitivity = 87.8\%, specificity = 80.8\%, negative predictive value = 97.8\% and positive predictive value = 40.6\%. The algorithm had similar sensitivity performance across subgroups as defined by sex, race and ethnicity. These results demonstrate the potential for digital phenotyping to provide an objective, scalable approach to autism screening in real-world settings. Moreover, combining results from digital phenotyping and caregiver questionnaires may increase autism screening accuracy and help reduce disparities in access to diagnosis and intervention.},
	language = {en},
	number = {10},
	urldate = {2025-02-03},
	journal = {Nature Medicine},
	author = {Perochon, Sam and Di Martino, J. Matias and Carpenter, Kimberly L. H. and Compton, Scott and Davis, Naomi and Eichner, Brian and Espinosa, Steven and Franz, Lauren and Krishnappa Babu, Pradeep Raj and Sapiro, Guillermo and Dawson, Geraldine},
	month = oct,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Machine learning, Disability},
	pages = {2489--2497},
	file = {2023_Perochon et al._Early detection of autism using digital behavioral phenotyping.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Body-Tracking\\2023_Perochon et al._Early detection of autism using digital behavioral phenotyping.pdf:application/pdf},
}

@article{wen2022,
	title = {Large scale validation of an early-age eye-tracking biomarker of an autism spectrum disorder subtype},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-08102-6},
	doi = {10.1038/s41598-022-08102-6},
	abstract = {Few clinically validated biomarkers of ASD exist which can rapidly, accurately, and objectively identify autism during the first years of life and be used to support optimized treatment outcomes and advances in precision medicine. As such, the goal of the present study was to leverage both simple and computationally-advanced approaches to validate an eye-tracking measure of social attention preference, the GeoPref Test, among 1,863 ASD, delayed, or typical toddlers (12–48 months) referred from the community or general population via a primary care universal screening program. Toddlers participated in diagnostic and psychometric evaluations and the GeoPref Test: a 1-min movie containing side-by-side dynamic social and geometric images. Following testing, diagnosis was denoted as ASD, ASD features, LD, GDD, Other, typical sibling of ASD proband, or typical. Relative to other diagnostic groups, ASD toddlers exhibited the highest levels of visual attention towards geometric images and those with especially high fixation levels exhibited poor clinical profiles. Using the 69\% fixation threshold, the GeoPref Test had 98\% specificity, 17\% sensitivity, 81\% PPV, and 65\% NPV. Sensitivity increased to 33\% when saccades were included, with comparable validity across sex, ethnicity, or race. The GeoPref Test was also highly reliable up to 24 months following the initial test. Finally, fixation levels among twins concordant for ASD were significantly correlated, indicating that GeoPref Test performance may be genetically driven. As the GeoPref Test yields few false positives ({\textasciitilde} 2\%) and is equally valid across demographic categories, the current findings highlight the ability of the GeoPref Test to rapidly and accurately detect autism before the 2nd birthday in a subset of children and serve as a biomarker for a unique ASD subtype in clinical trials.},
	language = {en},
	number = {1},
	urldate = {2025-02-03},
	journal = {Scientific Reports},
	author = {Wen, Teresa H. and Cheng, Amanda and Andreason, Charlene and Zahiri, Javad and Xiao, Yaqiong and Xu, Ronghui and Bao, Bokan and Courchesne, Eric and Barnes, Cynthia Carter and Arias, Steven J. and Pierce, Karen},
	month = mar,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Autism spectrum disorders, Diagnostic markers, Paediatric research},
	pages = {4253},
	file = {2022_Wen et al._Large scale validation of an early-age eye-tracking biomarker of an autism spectrum disorder subtype.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2022_Wen et al._Large scale validation of an early-age eye-tracking biomarker of an autism spectrum disorder subtype.pdf:application/pdf},
}

@article{shirama2016,
	title = {Ocular {Fixation} {Abnormality} in {Patients} with {Autism} {Spectrum} {Disorder}},
	volume = {46},
	issn = {1573-3432},
	url = {https://doi.org/10.1007/s10803-015-2688-y},
	doi = {10.1007/s10803-015-2688-y},
	abstract = {We examined the factors that influence ocular fixation control in adults with autism spectrum disorder (ASD) including sensory information, individuals’ motor characteristics, and inhibitory control. The ASD group showed difficulty in maintaining fixation especially when there was no fixation target. The fixational eye movement characteristics of individuals were consistent regardless of the presence or absence of a fixation target in the controls, but not in the ASD group. Additionally, fixation stability did not correlate with an ability to suppress reflexive saccades measured by an antisaccade task. These findings suggest that ASD adults have deficits in converting alternative sensory information, such as retinal signals in the peripheral visual field or extraretinal signals, to motor commands when the foveal information is unavailable.},
	language = {en},
	number = {5},
	urldate = {2025-02-03},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Shirama, Aya and Kanai, Chieko and Kato, Nobumasa and Kashino, Makio},
	month = may,
	year = {2016},
	keywords = {ASD, Neurodevelopmental Disorders, Antisaccade task, Ocular fixation, Sensorimotor function},
	pages = {1613--1622},
	file = {2016_Shirama et al._Ocular Fixation Abnormality in Patients with Autism Spectrum Disorder.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2016_Shirama et al._Ocular Fixation Abnormality in Patients with Autism Spectrum Disorder.pdf:application/pdf},
}

@article{chien2023,
	title = {Game-{Based} {Social} {Interaction} {Platform} for {Cognitive} {Assessment} of {Autism} {Using} {Eye} {Tracking}},
	volume = {31},
	issn = {1558-0210},
	url = {https://ieeexplore.ieee.org/document/10000269},
	doi = {10.1109/TNSRE.2022.3232369},
	abstract = {The design goals of recently developed serious games are to improve attention, affective recognition, and social interactions among individuals with autism. However, most previous studies on serious games used behavioral questionnaires to evaluate their effectiveness. The cognitive assessment of individuals with autism after behavioral intervention or drug treatment has become important because it provides promising biomarkers to assess improvement after cognitive intervention. In this study, we developed a game-based social interaction platform incorporating an eye-tracking system for children and preadolescents with autism. Three modules (focusing on gaze following, facial emotion recognition, and social interaction skills) are included in the platform; participants with autism learn these according to their cognitive abilities. The eye-tracking results showed decreased fixation durations when autistic children looked at positive emotional expressions and focused on multiple targets. Prolonged saccade durations and shorter fixation times for social-related facial emotion expressions were also found in preadolescents and teenagers with autism. Our findings suggest that these atypical gaze patterns are reliable biomarkers for evaluating the social and cognitive functions of autistic individuals while playing serious games. The proposed platform’s game-based modules and the findings regarding aberrant gaze patterns in autistic individuals demonstrate the possibility of evaluating cognitive functions and intervention effectiveness by using eye-tracking signals in a serious game or real-life environment.},
	urldate = {2025-02-04},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	author = {Chien, Yi-Ling and Lee, Chia-Hsin and Chiu, Yen-Nan and Tsai, Wen-Che and Min, Yuan-Che and Lin, Yang-Min and Wong, Jui-Shen and Tseng, Yi-Li},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	keywords = {Face recognition, Autism, eye tracking, Gaze tracking, Emotion recognition, Games, Biomarkers, autism spectrum disorder, serious games, Pediatrics, Social interaction game},
	pages = {749--758},
	file = {2023_Chien et al._Game-Based Social Interaction Platform for Cognitive Assessment of Autism Using Eye Tracking.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2023_Chien et al._Game-Based Social Interaction Platform for Cognitive Assessment of Autism Using Eye Tracking.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\XL4IGI6N\\10000269.html:text/html},
}

@article{forbes2025,
	title = {Oculomotor {Function} in {Children} and {Adolescents} with {Autism}, {ADHD} or {Co}-occurring {Autism} and {ADHD}},
	issn = {1573-3432},
	url = {https://doi.org/10.1007/s10803-024-06718-3},
	doi = {10.1007/s10803-024-06718-3},
	abstract = {Oculomotor characteristics, including accuracy, timing, and sensorimotor processing, are considered sensitive intermediate phenotypes for understanding the etiology of neurodevelopmental conditions, such as autism and ADHD. Oculomotor characteristics have predominantly been studied separately in autism and ADHD. Despite the high rates of co-occurrence between these conditions, only one study has investigated oculomotor processes among those with co-occurring autism + ADHD. Four hundred and five (n = 405; 226 males) Australian children and adolescents aged 4 to 18 years (M = 9.64 years; SD = 3.20 years) with ADHD (n = 64), autism (n = 66), autism + ADHD (n = 146), or neurotypical individuals (n = 129) were compared across four different oculomotor tasks: visually guided saccade, anti-saccade, sinusoidal pursuit and step-ramp pursuit. Confirmatory analyses were conducted using separate datasets acquired from the University of Nottingham UK (n = 17 autism, n = 22 ADHD, n = 32 autism + ADHD, n = 30 neurotypical) and University of Kansas USA (n = 29 autism, n = 41 neurotypical). Linear mixed effect models controlling for sex, age and family revealed that children and adolescents with autism + ADHD exhibited increased variability in the accuracy of the final saccadic eye position compared to neurotypical children and adolescents. Autistic children and adolescents demonstrated a greater number of catch-up saccades during step-ramp pursuit compared to neurotypical children and adolescents. These findings suggest that select differences in saccadic precision are unique to autistic individuals with co-occurring ADHD, indicating that measuring basic sensorimotor processes may be useful for parsing neurodevelopment and clinical heterogeneity in autism.},
	language = {en},
	urldate = {2025-02-04},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Forbes, Elana J. and Tiego, Jeggan and Langmead, Joshua and Unruh, Kathryn E. and Mosconi, Matthew W. and Finlay, Amy and Kallady, Kathryn and Maclachlan, Lydia and Moses, Mia and Cappel, Kai and Knott, Rachael and Chau, Tracey and Sindhu, Vishnu Priya Mohanakumar and Bellato, Alessio and Groom, Madeleine J. and Kerestes, Rebecca and Bellgrove, Mark A. and Johnson, Beth P.},
	month = jan,
	year = {2025},
	keywords = {Autism, ADHD, Endophenotype, Neurodevelopment, Oculomotor Control, obsidian},
	file = {2025_Forbes et al._Oculomotor Function in Children and Adolescents with Autism, ADHD or Co-occurring Autism and ADHD.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2025_Forbes et al._Oculomotor Function in Children and Adolescents with Autism, ADHD or Co-occurring Autism and ADHD.pdf:application/pdf},
}

@article{perkovich2024,
	title = {Conducting head-mounted eye-tracking research with young children with autism and children with increased likelihood of later autism diagnosis},
	volume = {16},
	issn = {1866-1955},
	url = {https://doi.org/10.1186/s11689-024-09524-1},
	doi = {10.1186/s11689-024-09524-1},
	abstract = {Over the past years, researchers have been using head-mounted eye-tracking systems to study young children’s gaze behaviors in everyday activities through which children learn about the world. This method has great potential to further our understanding of how millisecond-level gaze behaviors create multisensory experiences and fluctuate around social environments. While this line of work can yield insight into early perceptual experiences and potential learning mechanisms, the majority of the work is exclusively conducted with typically-developing children. Sensory sensitivities, social-communication difficulties, and challenging behaviors (e.g., disruption, elopement) are common among children with developmental disorders, and they may represent potential methodological challenges for collecting high-quality data.},
	number = {1},
	urldate = {2025-02-05},
	journal = {Journal of Neurodevelopmental Disorders},
	author = {Perkovich, E. and Laakman, A. and Mire, S. and Yoshida, H.},
	month = mar,
	year = {2024},
	keywords = {Attention, Eye-tracking, Autism spectrum disorder, Social behavior, Social cognition},
	pages = {7},
	file = {2024_Perkovich et al._Conducting head-mounted eye-tracking research with young children with autism and children with incr.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2024_Perkovich et al._Conducting head-mounted eye-tracking research with young children with autism and children with incr.pdf:application/pdf},
}

@article{kalb2022,
	title = {Analysis of {Race} and {Sex} {Bias} in the {Autism} {Diagnostic} {Observation} {Schedule} ({ADOS}-2)},
	volume = {5},
	issn = {2574-3805},
	url = {https://doi.org/10.1001/jamanetworkopen.2022.9498},
	doi = {10.1001/jamanetworkopen.2022.9498},
	abstract = {There are long-standing disparities in the prevalence of autism spectrum disorder (ASD) across race and sex. Surprisingly, few studies have examined whether these disparities arise partially out of systematic biases in the Autism Diagnostic Observation Schedule, Second Edition (ADOS-2), the reference standard measure of ASD.To examine differential item functioning (DIF) of ADOS-2 items across sex and race.This is a cross-sectional study of children who were evaluated for ASD between 2014 and 2020 at a specialty outpatient clinic located in the Mid-Atlantic region of the US. Data were analyzed from July 2021 to February 2022.Child race (Black/African American vs White) and sex (female vs male).Item-level biases across ADOS-2 harmonized algorithm items, including social affect (SA; 10 items) and repetitive/restricted behaviors (RRBs; 4 items), were evaluated across 3 modules. Measurement bias was identified by examining DIF and differential test functioning (DTF), within a graded response, item response theory framework. Statistical significance was determined by a likelihood ratio χ2 test, and a series of metrics was used to examine the magnitude of DIF and DTF.A total of 6269 children (mean [SD] age, 6.77 [3.27] years; 1619 Black/African American [25.9\%], 3151 White [50.3\%], and 4970 male [79.4\%]), were included in this study. Overall, 16 of 140 ADOS-2 diagnostic items (11\%) had a significant DIF. For race, 8 items had a significant DIF, 6 of which involved SA. No single item showed DIF consistently across all modules. Most items with DIF had greater difficulty and poorer discrimination in Black/African American children compared with White children. For sex, 5 items showed significant DIF. DIF was split across SA and RRB. However, hand mannerisms evidenced DIF across all 5 algorithms, with generally greater difficulty. The magnitude of DIF was only moderate to large for 2 items: hand mannerisms (among female children) and repetitive interests (among Black/African American children). The overall estimated effect of DIF on total DTF was not large.These findings suggest that the ADOS-2 does not have widespread systematic measurement bias across race or sex. However, the findings raise some concerns around underdetection that warrant further research.},
	number = {4},
	urldate = {2025-02-05},
	journal = {JAMA Network Open},
	author = {Kalb, Luther G. and Singh, Vini and Hong, Ji Su and Holingue, Calliope and Ludwig, Natasha N. and Pfeiffer, Danika and Reetzke, Rachel and Gross, Alden L. and Landa, Rebecca},
	month = apr,
	year = {2022},
	pages = {e229498},
	file = {2022_Kalb et al._Analysis of Race and Sex Bias in the Autism Diagnostic Observation Schedule (ADOS-2).pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2022_Kalb et al._Analysis of Race and Sex Bias in the Autism Diagnostic Observation Schedule (ADOS-2).pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\T6A8MXVZ\\2791527.html:text/html},
}

@article{gotham2009,
	title = {Standardizing {ADOS} {Scores} for a {Measure} of {Severity} in {Autism} {Spectrum} {Disorders}},
	volume = {39},
	issn = {0162-3257},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2922918/},
	doi = {10.1007/s10803-008-0674-3},
	abstract = {The aim of this study is to standardize Autism Diagnostic Observation Schedule (ADOS) scores within a large sample to approximate an autism severity metric. Using a dataset of 1415 individuals aged 2–16 years with autism spectrum disorders (ASD) or nonspectrum diagnoses, a subset of 1807 assessments from 1118 individuals with ASD were divided into narrow age- and language-cells. Within each cell, severity scores were based on percentiles of raw totals corresponding to each ADOS diagnostic classification. Calibrated severity scores had more uniform distributions across developmental groups and were less influenced by participant demographics than raw totals. This metric should be useful in comparing assessments across modules and time, and identifying trajectories of autism severity for clinical, genetic, and neurobiological research.},
	number = {5},
	urldate = {2025-02-05},
	journal = {Journal of autism and developmental disorders},
	author = {Gotham, Katherine and Pickles, Andrew and Lord, Catherine},
	month = may,
	year = {2009},
	pmid = {19082876},
	pmcid = {PMC2922918},
	pages = {693--705},
	file = {2009_Gotham et al._Standardizing ADOS Scores for a Measure of Severity in Autism Spectrum Disorders.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2009_Gotham et al._Standardizing ADOS Scores for a Measure of Severity in Autism Spectrum Disorders.pdf:application/pdf},
}

@article{medda2019,
	title = {Sensitivity and {Specificity} of the {ADOS}-2 {Algorithm} in a {Large} {German} {Sample}},
	volume = {49},
	issn = {0162-3257},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6373322/},
	doi = {10.1007/s10803-018-3750-3},
	abstract = {The aim of the present study was to establish diagnostic validity of the new algorithm of the Autism Diagnostic Observation Scale, the ADOS-2, to differentiate between ASD and other clinically relevant psychiatric and developmental disorders in a large German sample. Validity of ADOS and ADOS-2 diagnostic algorithms was established in 826 individuals (n = 455 autism, n = 216 autism spectrum, n = 155 non-ASD patients) by receiver operating curves. Confidence intervals overlapped largely for ADOS and ADOS-2 algorithms, confirming diagnostic validity of both algorithms. Adding information of the Social Communication Questionnaire and the Social Responsiveness Scale resulted in slightly improved classification rates for autism in Module 4. We thus replicated previous findings of the diagnostic validity of the ADOS-2 algorithms.},
	number = {2},
	urldate = {2025-02-05},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Medda, Juliane E. and Cholemkery, Hannah and Freitag, Christine M.},
	year = {2019},
	pmid = {30238180},
	pmcid = {PMC6373322},
	pages = {750--761},
	file = {2019_Medda et al._Sensitivity and Specificity of the ADOS-2 Algorithm in a Large German Sample.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2019_Medda et al._Sensitivity and Specificity of the ADOS-2 Algorithm in a Large German Sample.pdf:application/pdf},
}

@article{megerian2022,
	title = {Performance of {Canvas} {Dx}, a {Novel} {Software}-based {Autism} {Spectrum} {Disorder} {Diagnosis} {Aid} for {Use} in a {Primary} {Care} {Setting} ({P13}-5.001)},
	volume = {98},
	url = {https://www.neurology.org/doi/abs/10.1212/WNL.98.18_supplement.1025},
	doi = {10.1212/WNL.98.18_supplement.1025},
	abstract = {Objective:
The lack of diagnostic tools for Autism Spectrum Disorder in primary care settings and long wait lists for specialist assessment contribute to an average delay of 3 years between first parental concern and diagnosis. This study examined the performance of an artificial intelligence-based device intended to aid PCPs in the diagnosis of ASD.
Background:
NA
Design/Methods:
Methods:
This was a prospective multi-site pivotal study conducted in 6 states using a double-blind active comparator design with 425 completed subjects (36\% female) ages 18–72 months with concern for developmental delay. Previous research developed, tuned, and tested a device that uses a gradient boosted decision tree machine learning algorithm which analyzes 64 behavioral features from 3 distinct inputs: 1) Caregiver questionnaire 2) two, 90 second minimum home videos analyzed by trained video analysts 3) PCP questionnaire.
Results:
Device results were compared to diagnosis by independent agreement of specialist clinicians based on clinical assessment, including a modified CARS-2 and DSM-5 criteria. Specialists were child psychiatrists, child psychologists, pediatric neurologists, and developmental behavioral pediatricians experienced in diagnosing ASD.
Results Comparison of device results to specialist diagnosis found the PPV: 80.8\% [95\%CI, 70.3\%–88.8\%], NPV: 98.3\% [90.6\%–100\%], sensitivity: 98.4\% [91.6\%–100\%], specificity: 78.9\% [67.6\%–87.7\%] for subjects with determinate device results. There was no evidence that device performance significantly varied when PCP used the device remotely compared to in-person.
Conclusions:
Using this device, PCPs could efficiently, accurately, and equitably diagnose a subset of children 18–72 months old, thereby streamlining specialist referrals and facilitating earlier ASD diagnosis and interventions. The results further provide preliminary evidence that PCP evaluation of the child can be done via telemedicine or in-person with no degradation in device performance.
Disclosure: Dr. Megerian has received personal compensation for serving as an employee of Yamo Pharmaceuticals. Dr. Megerian has received personal compensation in the range of \$10,000-\$49,999 for serving as a Consultant for Cognito Therapeutics. Dr. Megerian has received personal compensation in the range of \$10,000-\$49,999 for serving as a Consultant for Kuzani Pharmaceuticals. Dr. Megerian has received research support from Childrens Hospital of Orange County. Dr. Dey has nothing to disclose. Dr. Melmed has received personal compensation in the range of \$500-\$4,999 for serving on a Scientific Advisory or Data Safety Monitoring board for Ovid. Dr. Melmed has received personal compensation in the range of \$500-\$4,999 for serving on a Scientific Advisory or Data Safety Monitoring board for Cognoa. Dr. Melmed has received personal compensation in the range of \$500-\$4,999 for serving on a Scientific Advisory or Data Safety Monitoring board for Akili. Dr. Melmed has received personal compensation in the range of \$500-\$4,999 for serving on a Speakers Bureau for Akili. Dr. Melmed has received research support from Akili. Dr. Melmed has received research support from Quadrant. The institution of Dr. Melmed has received research support from Cognoa. Dr. Coury has received personal compensation in the range of \$0-\$499 for serving as a Consultant for BioRosa. Dr. Coury has received personal compensation in the range of \$10,000-\$49,999 for serving as a Consultant for Quadrant. Dr. Coury has received personal compensation in the range of \$0-\$499 for serving as a Consultant for Stalicla. Dr. Coury has received personal compensation in the range of \$500-\$4,999 for serving as a Consultant for Cognoa. Dr. Coury has received personal compensation in the range of \$500-\$4,999 for serving as a Consultant for MaraBio. Dr. Coury has received personal compensation in the range of \$0-\$499 for serving on a Scientific Advisory or Data Safety Monitoring board for BioRosa. Dr. Coury has received personal compensation in the range of \$500-\$4,999 for serving on a Scientific Advisory or Data Safety Monitoring board for Cognoa. Dr. Coury has received personal compensation in the range of \$500-\$4,999 for serving on a Scientific Advisory or Data Safety Monitoring board for GW Biosciences. Dr. Coury has received personal compensation in the range of \$500-\$4,999 for serving on a Scientific Advisory or Data Safety Monitoring board for MaraBio. Dr. Coury has received personal compensation in the range of \$10,000-\$49,999 for serving on a Scientific Advisory or Data Safety Monitoring board for Quadrant Biosciences. Dr. Coury has received personal compensation in the range of \$500-\$4,999 for serving on a Scientific Advisory or Data Safety Monitoring board for Stalicla SA. Dr. Coury has received personal compensation in the range of \$500-\$4,999 for serving on a Scientific Advisory or Data Safety Monitoring board for AMO Pharma. The institution of Dr. Coury has received research support from GW Biosciences. The institution of Dr. Coury has received research support from Stalicla SA. Prof. Lerner has received personal compensation for serving as an employee of Cognoa. Prof. Lerner has received personal compensation in the range of \$5,000-\$9,999 for serving as a Consultant for Cognoa. Dr. Nicholls has received personal compensation in the range of \$50,000-\$99,999 for serving as an Expert Witness for various attorneys. Dr. Nicholls has received publishing royalties from a publication relating to health care. Kristin Sohl has received personal compensation in the range of \$500-\$4,999 for serving as a Consultant for Autism Navigator. Kristin Sohl has received personal compensation in the range of \$500-\$4,999 for serving as a Consultant for Cognoa . Kristin Sohl has received personal compensation in the range of \$5,000-\$9,999 for serving as an Editor, Associate Editor, or Editorial Advisory Board Member for Quadrant Biosciences . The institution of Kristin Sohl has received research support from Autism Speaks. The institution of Kristin Sohl has received research support from NIH. The institution of Kristin Sohl has received research support from DOD. The institution of Kristin Sohl has received research support from HRSA. The institution of Kristin Sohl has received research support from Missouri Department of Mental Health. Dr. Rouhbakhsh has a non-compensated relationship as a Editorial Board Member with The Journal of Occupational and Environmental Medicine that is relevant to AAN interests or activities. Dr. Narasimhan has nothing to disclose. Dr. Romain has received personal compensation for serving as an employee of CHOC children's Hospital . Dr. Romain has received personal compensation in the range of \$10,000-\$49,999 for serving as a Consultant for COGNOA. Dr. Golla has received personal compensation in the range of \$500-\$4,999 for serving as a Consultant for Cognoa . Dr. Shareef has nothing to disclose. Dr. Ostrovsky has received personal compensation in the range of \$500-\$4,999 for serving on a Scientific Advisory or Data Safety Monitoring board for Cognoa. Dr. Ostrovsky has received stock or an ownership interest from Social Innovation Ventures. Dr. Shannon has received personal compensation for serving as an employee of Cognoa. Dr. Shannon has received personal compensation in the range of \$500-\$4,999 for serving as a Consultant for Pear. Dr. Shannon has received personal compensation in the range of \$500-\$4,999 for serving as an Expert Witness for Advanced Medical Group. Dr. Kraft has received personal compensation for serving as an employee of Cognoa. Dr. Kraft has received personal compensation in the range of \$500-\$4,999 for serving as a Consultant for Happiest Baby, Inc. Dr. Kraft has received personal compensation in the range of \$0-\$499 for serving as an officer or member of the Board of Directors for DotCom Therapy. Dr. Kraft has received personal compensation in the range of \$0-\$499 for serving as an officer or member of the Board of Directors for Cognoa, Inc. Mr. Liu-Mayo has received personal compensation for serving as an employee of Cognoa. Mr. Liu-Mayo has received stock or an ownership interest from Cognoa. Mr. Abbas has received personal compensation for serving as an employee of Cognoa. Mr. Abbas has received intellectual property interests from a discovery or technology relating to health care. Dr. Gal-Szabo has received personal compensation in the range of \$10,000-\$49,999 for serving as a Consultant for Cognoa Inc. Prof. Wall has received personal compensation in the range of \$500-\$4,999 for serving as a Consultant for Cognoa. Dr. Taraman has received personal compensation for serving as an employee of Cognoa. Dr. Taraman has received personal compensation for serving as an employee of Pediatric Subspecialty Faculty. Dr. Taraman has received personal compensation in the range of \$10,000-\$49,999 for serving as a Consultant for Cognito Therapeutics . Dr. Taraman has received stock or an ownership interest from Handzin. Dr. Taraman has received stock or an ownership interest from Cognoa. Dr. Taraman has received research support from Innovative Health Solutions. Dr. Taraman has received intellectual property interests from a discovery or technology relating to health care. Dr. Taraman has received intellectual property interests from a discovery or technology relating to health care. Dr. Taraman has received intellectual property interests from a discovery or technology relating to health care.},
	number = {18\_supplement},
	urldate = {2025-02-05},
	journal = {Neurology},
	author = {Megerian, Jonathan T. and Dey, Sangeeta and Melmed, Raun D. and Coury, Daniel L. and Lerner, Marc and Nicholls, Christopher and Sohl, Kristin and Rouhbakhsh, Rambod and Narasimhan, Anandhi and Romain, Jonathan and Golla, Sailaja and Shareef, Safiullah and Ostrovsky, Andrey and Shannon, Jennifer and Kraft, Colleen and Liu-Mayo, Stuart and Abbas, Halim and Gal-Szabo, Diana E. and Wall, Dennis P. and Taraman, Sharief},
	month = may,
	year = {2022},
	note = {Publisher: Wolters Kluwer},
	pages = {1025},
}

@article{abbasi2021,
	title = {Mobile {Device} {App} {Helps} {Distinguish} {Toddlers} {With} {Autism}},
	volume = {325},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.2021.8627},
	doi = {10.1001/jama.2021.8627},
	abstract = {A prototype app reliably distinguished toddlers with autism from those with typical development in a National Institutes of Health–funded study. The app uses a tablet or smartphone’s camera to record eye-gaze patterns while children watch short videos on the device.The study involved 993 patients with an average age of about 21 months who were seen at 4 Duke Children’s Primary Care clinics. The kids watched custom-made videos of people in social situations—smiling, making eye contact, and having conversations. Computer vision analysis of the gaze patterns revealed quantifiable differences in how the children visually tracked the social cues, which predicted with high accuracy those who went on to receive an autism spectrum disorder (ASD) diagnosis after traditional screening.},
	number = {22},
	urldate = {2025-02-07},
	journal = {JAMA},
	author = {Abbasi, Jennifer},
	month = jun,
	year = {2021},
	pages = {2243},
}

@article{jones2023,
	title = {Eye-{Tracking}–{Based} {Measurement} of {Social} {Visual} {Engagement} {Compared} {With} {Expert} {Clinical} {Diagnosis} of {Autism}},
	volume = {330},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.2023.13295},
	doi = {10.1001/jama.2023.13295},
	abstract = {In the US, children with signs of autism often experience more than 1 year of delay before diagnosis and often experience longer delays if they are from racially, ethnically, or economically disadvantaged backgrounds. Most diagnoses are also received without use of standardized diagnostic instruments. To aid in early autism diagnosis, eye-tracking measurement of social visual engagement has shown potential as a performance-based biomarker.To evaluate the performance of eye-tracking measurement of social visual engagement (index test) relative to expert clinical diagnosis in young children referred to specialty autism clinics.In this study of 16- to 30-month-old children enrolled at 6 US specialty centers from April 2018 through May 2019, staff blind to clinical diagnoses used automated devices to measure eye-tracking–based social visual engagement. Expert clinical diagnoses were made using best practice standardized protocols by specialists blind to index test results. This study was completed in a 1-day protocol for each participant.Primary outcome measures were test sensitivity and specificity relative to expert clinical diagnosis. Secondary outcome measures were test correlations with expert clinical assessments of social disability, verbal ability, and nonverbal cognitive ability.Eye-tracking measurement of social visual engagement was successful in 475 (95.2\%) of the 499 enrolled children (mean [SD] age, 24.1 [4.4] months; 38 [8.0\%] were Asian; 37 [7.8\%], Black; 352 [74.1\%], White; 44 [9.3\%], other; and 68 [14.3\%], Hispanic). By expert clinical diagnosis, 221 children (46.5\%) had autism and 254 (53.5\%) did not. In all children, measurement of social visual engagement had sensitivity of 71.0\% (95\% CI, 64.7\% to 76.6\%) and specificity of 80.7\% (95\% CI, 75.4\% to 85.1\%). In the subgroup of 335 children whose autism diagnosis was certain, sensitivity was 78.0\% (95\% CI, 70.7\% to 83.9\%) and specificity was 85.4\% (95\% CI, 79.5\% to 89.8\%). Eye-tracking test results correlated with expert clinical assessments of individual levels of social disability (r = −0.75 [95\% CI, −0.79 to −0.71]), verbal ability (r = 0.65 [95\% CI, 0.59 to 0.70]), and nonverbal cognitive ability (r = 0.65 [95\% CI, 0.59 to 0.70]).In 16- to 30-month-old children referred to specialty clinics, eye-tracking–based measurement of social visual engagement was predictive of autism diagnoses by clinical experts. Further evaluation of this test’s role in early diagnosis and assessment of autism in routine specialty clinic practice is warranted.ClinicalTrials.gov Identifier: NCT03469986},
	number = {9},
	urldate = {2025-02-07},
	journal = {JAMA},
	author = {Jones, Warren and Klaiman, Cheryl and Richardson, Shana and Aoki, Christa and Smith, Christopher and Minjarez, Mendy and Bernier, Raphael and Pedapati, Ernest and Bishop, Somer and Ence, Whitney and Wainer, Allison and Moriuchi, Jennifer and Tay, Sew-Wah and Klin, Ami},
	month = sep,
	year = {2023},
	pages = {854--865},
	file = {2023_Jones et al._Eye-Tracking–Based Measurement of Social Visual Engagement Compared With Expert Clinical Diagnosis o.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2023_Jones et al._Eye-Tracking–Based Measurement of Social Visual Engagement Compared With Expert Clinical Diagnosis o.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\B2Y28LNH\\2808996.html:text/html},
}

@article{chang2021a,
	title = {Computational {Methods} to {Measure} {Patterns} of {Gaze} in {Toddlers} {With} {Autism} {Spectrum} {Disorder}},
	volume = {175},
	issn = {2168-6203},
	url = {https://doi.org/10.1001/jamapediatrics.2021.0530},
	doi = {10.1001/jamapediatrics.2021.0530},
	abstract = {Atypical eye gaze is an early-emerging symptom of autism spectrum disorder (ASD) and holds promise for autism screening. Current eye-tracking methods are expensive and require special equipment and calibration. There is a need for scalable, feasible methods for measuring eye gaze.Using computational methods based on computer vision analysis, we evaluated whether an app deployed on an iPhone or iPad that displayed strategically designed brief movies could elicit and quantify differences in eye-gaze patterns of toddlers with ASD vs typical development.A prospective study in pediatric primary care clinics was conducted from December 2018 to March 2020, comparing toddlers with and without ASD. Caregivers of 1564 toddlers were invited to participate during a well-child visit. A total of 993 toddlers (63\%) completed study measures. Enrollment criteria were aged 16 to 38 months, healthy, English- or Spanish-speaking caregiver, and toddler able to sit and view the app. Participants were screened with the Modified Checklist for Autism in Toddlers–Revised With Follow-up during routine care. Children were referred by their pediatrician for diagnostic evaluation based on results of the checklist or if the caregiver or pediatrician was concerned. Forty toddlers subsequently were diagnosed with ASD.A mobile app displayed on a smartphone or tablet.Computer vision analysis quantified eye-gaze patterns elicited by the app, which were compared between toddlers with ASD vs typical development.Mean age of the sample was 21.1 months (range, 17.1-36.9 months), and 50.6\% were boys, 59.8\% White individuals, 16.5\% Black individuals, 23.7\% other race, and 16.9\% Hispanic/Latino individuals. Distinctive eye-gaze patterns were detected in toddlers with ASD, characterized by reduced gaze to social stimuli and to salient social moments during the movies, and previously unknown deficits in coordination of gaze with speech sounds. The area under the receiver operating characteristic curve discriminating ASD vs non-ASD using multiple gaze features was 0.90 (95\% CI, 0.82-0.97).The app reliably measured both known and new gaze biomarkers that distinguished toddlers with ASD vs typical development. These novel results may have potential for developing scalable autism screening tools, exportable to natural settings, and enabling data sets amenable to machine learning.},
	number = {8},
	urldate = {2025-02-07},
	journal = {JAMA Pediatrics},
	author = {Chang, Zhuoqing and Di Martino, J. Matias and Aiello, Rachel and Baker, Jeffrey and Carpenter, Kimberly and Compton, Scott and Davis, Naomi and Eichner, Brian and Espinosa, Steven and Flowers, Jacqueline and Franz, Lauren and Harris, Adrianne and Howard, Jill and Perochon, Sam and Perrin, Eliana M. and Krishnappa Babu, Pradeep Raj and Spanos, Marina and Sullivan, Connor and Walter, Barbara K. and Kollins, Scott H. and Dawson, Geraldine and Sapiro, Guillermo},
	month = aug,
	year = {2021},
	keywords = {eye tracking},
	pages = {827--836},
	file = {2021_Chang et al._Computational Methods to Measure Patterns of Gaze in Toddlers With Autism Spectrum Disorder.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2021_Chang et al._Computational Methods to Measure Patterns of Gaze in Toddlers With Autism Spectrum Disorder.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\NMRLWHSN\\2779395.html:text/html},
}

@misc{cheng2024,
	title = {Appearance-based {Gaze} {Estimation} {With} {Deep} {Learning}: {A} {Review} and {Benchmark}},
	shorttitle = {Appearance-based {Gaze} {Estimation} {With} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2104.12668},
	doi = {10.48550/arXiv.2104.12668},
	abstract = {Human gaze provides valuable information on human focus and intentions, making it a crucial area of research. Recently, deep learning has revolutionized appearance-based gaze estimation. However, due to the unique features of gaze estimation research, such as the unfair comparison between 2D gaze positions and 3D gaze vectors and the different pre-processing and post-processing methods, there is a lack of a definitive guideline for developing deep learning-based gaze estimation algorithms. In this paper, we present a systematic review of the appearance-based gaze estimation methods using deep learning. Firstly, we survey the existing gaze estimation algorithms along the typical gaze estimation pipeline: deep feature extraction, deep learning model design, personal calibration and platforms. Secondly, to fairly compare the performance of different approaches, we summarize the data pre-processing and post-processing methods, including face/eye detection, data rectification, 2D/3D gaze conversion and gaze origin conversion. Finally, we set up a comprehensive benchmark for deep learning-based gaze estimation. We characterize all the public datasets and provide the source code of typical gaze estimation algorithms. This paper serves not only as a reference to develop deep learning-based gaze estimation methods, but also a guideline for future gaze estimation research. The project web page can be found at https://phi-ai.buaa.edu.cn/Gazehub.},
	urldate = {2025-02-07},
	publisher = {arXiv},
	author = {Cheng, Yihua and Wang, Haofei and Bao, Yiwei and Lu, Feng},
	month = apr,
	year = {2024},
	note = {arXiv:2104.12668 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {2024_Cheng et al._Appearance-based Gaze Estimation With Deep Learning A Review and Benchmark.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2024_Cheng et al._Appearance-based Gaze Estimation With Deep Learning A Review and Benchmark.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\8CQZGWDG\\2104.html:text/html},
}

@article{megerian2022a,
	title = {Evaluation of an artificial intelligence-based medical device for diagnosis of autism spectrum disorder},
	volume = {5},
	copyright = {2022 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-022-00598-6},
	doi = {10.1038/s41746-022-00598-6},
	abstract = {Autism spectrum disorder (ASD) can be reliably diagnosed at 18 months, yet significant diagnostic delays persist in the United States. This double-blinded, multi-site, prospective, active comparator cohort study tested the accuracy of an artificial intelligence-based Software as a Medical Device designed to aid primary care healthcare providers (HCPs) in diagnosing ASD. The Device combines behavioral features from three distinct inputs (a caregiver questionnaire, analysis of two short home videos, and an HCP questionnaire) in a gradient boosted decision tree machine learning algorithm to produce either an ASD positive, ASD negative, or indeterminate output. This study compared Device outputs to diagnostic agreement by two or more independent specialists in a cohort of 18–72-month-olds with developmental delay concerns (425 study completers, 36\% female, 29\% ASD prevalence). Device output PPV for all study completers was 80.8\% (95\% confidence intervals (CI), 70.3\%–88.8\%) and NPV was 98.3\% (90.6\%–100\%). For the 31.8\% of participants who received a determinate output (ASD positive or negative) Device sensitivity was 98.4\% (91.6\%–100\%) and specificity was 78.9\% (67.6\%–87.7\%). The Device’s indeterminate output acts as a risk control measure when inputs are insufficiently granular to make a determinate recommendation with confidence. If this risk control measure were removed, the sensitivity for all study completers would fall to 51.6\% (63/122) (95\% CI 42.4\%, 60.8\%), and specificity would fall to 18.5\% (56/303) (95\% CI 14.3\%, 23.3\%). Among participants for whom the Device abstained from providing a result, specialists identified that 91\% had one or more complex neurodevelopmental disorders. No significant differences in Device performance were found across participants’ sex, race/ethnicity, income, or education level. For nearly a third of this primary care sample, the Device enabled timely diagnostic evaluation with a high degree of accuracy. The Device shows promise to significantly increase the number of children able to be diagnosed with ASD in a primary care setting, potentially facilitating earlier intervention and more efficient use of specialist resources.},
	language = {en},
	number = {1},
	urldate = {2025-02-08},
	journal = {npj Digital Medicine},
	author = {Megerian, Jonathan T. and Dey, Sangeeta and Melmed, Raun D. and Coury, Daniel L. and Lerner, Marc and Nicholls, Christopher J. and Sohl, Kristin and Rouhbakhsh, Rambod and Narasimhan, Anandhi and Romain, Jonathan and Golla, Sailaja and Shareef, Safiullah and Ostrovsky, Andrey and Shannon, Jennifer and Kraft, Colleen and Liu-Mayo, Stuart and Abbas, Halim and Gal-Szabo, Diana E. and Wall, Dennis P. and Taraman, Sharief},
	month = may,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Machine learning, Autism spectrum disorders, Diagnosis, Paediatric research},
	pages = {1--11},
	file = {2022_Megerian et al._Evaluation of an artificial intelligence-based medical device for diagnosis of autism spectrum disor.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2022_Megerian et al._Evaluation of an artificial intelligence-based medical device for diagnosis of autism spectrum disor.pdf:application/pdf},
}

@article{jin2024,
	title = {Early diagnostic value of home video–based machine learning in autism spectrum disorder: a meta-analysis},
	volume = {184},
	issn = {1432-1076},
	shorttitle = {Early diagnostic value of home video–based machine learning in autism spectrum disorder},
	url = {https://doi.org/10.1007/s00431-024-05837-4},
	doi = {10.1007/s00431-024-05837-4},
	abstract = {Machine learning (ML) based on remote video has shown ideal diagnostic value in autism spectrum disorder (ASD). Here, we conducted a meta-analysis of the diagnostic value of home video–based ML in ASD. Relevant articles were systematically searched in PubMed, Cochrane, Embase, and Web of Science from inception to September 2023 with no language restriction, and the literature search was updated in September 2024. The overall risk of bias and suitability of the ML prediction models in the included studies were assessed using PROBAST. Nineteen articles involving 89 prediction models and 9959 subjects were included. The mean video duration was 5.63 ± 1.23 min, and the mean number of behavioral features during initial modeling was 23.53. Among the 19 included studies, 13 models had been trained. Seven of the 13 models were not cross-validated (c-index = 0.92, 95\% CI 0.88–0.96), while 6 of the 13 models were tenfold cross-validated (c-index = 0.95, 95\% CI 0.94–0.97). There were 8 validation cohorts (c-index = 0.83, 95\% CI 0.77–0.89). The pooled sensitivity and specificity were 0.87 (95\% CI 0.77–0.93) and 0.79 (95\% CI 0.76–0.81) in the training cohort, 0.90 (95\% CI 0.85–0.94) and 0.87 (95\% CI 0.72–0.94) in the cross-validation, and 0.81 (95\% CI 0.74–0.86) and 0.72 (95\% CI 0.68–0.75) in the validation cohort, respectively. These results indicated that this model is a highly sensitive and user-friendly tool for early ASD diagnosis.},
	language = {en},
	number = {1},
	urldate = {2025-02-08},
	journal = {European Journal of Pediatrics},
	author = {Jin, Longjie and Cui, Hualei and Zhang, Peiyuan and Cai, Chunquan},
	month = nov,
	year = {2024},
	keywords = {Artificial Intelligence, Diagnosis, Autistic disorder, Home video, ML},
	pages = {37},
	file = {2024_Jin et al._Early diagnostic value of home video–based machine learning in autism spectrum disorder a meta-anal.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Jin et al._Early diagnostic value of home video–based machine learning in autism spectrum disorder a meta-anal.pdf:application/pdf},
}

@article{meng2023,
	title = {Machine learning-based early diagnosis of autism according to eye movements of real and artificial faces scanning},
	volume = {17},
	issn = {1662-4548},
	doi = {10.3389/fnins.2023.1170951},
	abstract = {BACKGROUND: Studies on eye movements found that children with autism spectrum disorder (ASD) had abnormal gaze behavior to social stimuli. The current study aimed to investigate whether their eye movement patterns in relation to cartoon characters or real people could be useful in identifying ASD children.
METHODS: Eye-tracking tests based on videos of cartoon characters and real people were performed for ASD and typically developing (TD) children aged between 12 and 60 months. A three-level hierarchical structure including participants, events, and areas of interest was used to arrange the data obtained from eye-tracking tests. Random forest was adopted as the feature selection tool and classifier, and the flattened vectors and diagnostic information were used as features and labels. A logistic regression was used to evaluate the impact of the most important features.
RESULTS: A total of 161 children (117 ASD and 44 TD) with a mean age of 39.70 ± 12.27 months were recruited. The overall accuracy, precision, and recall of the model were 0.73, 0.73, and 0.75, respectively. Attention to human-related elements was positively related to the diagnosis of ASD, while fixation time for cartoons was negatively related to the diagnosis.
CONCLUSION: Using eye-tracking techniques with machine learning algorithms might be promising for identifying ASD. The value of artificial faces, such as cartoon characters, in the field of ASD diagnosis and intervention is worth further exploring.},
	language = {eng},
	journal = {Frontiers in Neuroscience},
	author = {Meng, Fanchao and Li, Fenghua and Wu, Shuxian and Yang, Tingyu and Xiao, Zhou and Zhang, Yujian and Liu, Zhengkui and Lu, Jianping and Luo, Xuerong},
	year = {2023},
	pmid = {37795184},
	pmcid = {PMC10545898},
	keywords = {eye-tracking, machine learning, autism spectrum disorder, cartoon character, random forest},
	pages = {1170951},
	file = {2023_Meng et al._Machine learning-based early diagnosis of autism according to eye movements of real and artificial f.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2023_Meng et al._Machine learning-based early diagnosis of autism according to eye movements of real and artificial f.pdf:application/pdf},
}

@article{paolucci2023,
	title = {Early prediction of {Autism} {Spectrum} {Disorders} through interaction analysis in home videos and explainable artificial intelligence},
	volume = {148},
	issn = {0747-5632},
	url = {https://www.sciencedirect.com/science/article/pii/S0747563223002285},
	doi = {10.1016/j.chb.2023.107877},
	abstract = {There is considerable discussion about the advantages and disadvantages of early ASD diagnosis. However, the development of easily understandable and administrable tools for teachers or caregivers in order to identify potentially alarming behaviours (red flags) is usually considered valuable even by scholars who are concerned with very early diagnosis. This study proposes an AI pre-screening tool with the aim of creating an easily administrable tool for non-competent observers useful to identify potentially alarming signs in pre-verbal interactions. The use of these features is evaluated using an explainable artificial intelligence algorithm to assess which of the proposed new interaction characteristics were more effective in classifying individuals with ASD vs. controls. We used a rating scale with three core sections – sensorimotor, behavioural, and emotional – each further divided into four items. By seeing home videos of children doing everyday activities, two experienced observers rated each of these items from 1 (highly typical interaction) to 8 (extremely atypical interaction). Then, a machine learning model based on XGBoost was developed for identifying ASD children. The classification obtained was interpreted through the use of SHAP explanations, obtaining an area under the receiver operating curve of 0.938 and 0.914 for the two observers, respectively. These results demonstrated the significance of early detection of body-related sensorimotor features.},
	urldate = {2025-02-08},
	journal = {Computers in Human Behavior},
	author = {Paolucci, Claudio and Giorgini, Federica and Scheda, Riccardo and Alessi, Flavio Valerio and Diciotti, Stefano},
	month = nov,
	year = {2023},
	keywords = {Autism spectrum disorder, Attunement, Early diagnosis, Explainable artificial intelligence, Interaction analysis, Semiotics},
	pages = {107877},
}

@article{paolucci2023a,
	title = {Early prediction of {Autism} {Spectrum} {Disorders} through interaction analysis in home videos and explainable artificial intelligence},
	volume = {148},
	issn = {07475632},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0747563223002285},
	doi = {10.1016/j.chb.2023.107877},
	abstract = {There is considerable discussion about the advantages and disadvantages of early ASD diagnosis. However, the development of easily understandable and administrable tools for teachers or caregivers in order to identify potentially alarming behaviours (red flags) is usually considered valuable even by scholars who are concerned with very early diagnosis. This study proposes an AI pre-screening tool with the aim of creating an easily administrable tool for non-competent observers useful to identify potentially alarming signs in pre-verbal interactions. The use of these features is evaluated using an explainable artificial intelligence algorithm to assess which of the proposed new interaction characteristics were more effective in classifying individuals with ASD vs. controls. We used a rating scale with three core sections – sensorimotor, behavioural, and emotional – each further divided into four items. By seeing home videos of children doing everyday activities, two experienced observers rated each of these items from 1 (highly typical interaction) to 8 (extremely atypical interaction). Then, a machine learning model based on XGBoost was developed for identifying ASD children. The classification obtained was interpreted through the use of SHAP explanations, obtaining an area under the receiver operating curve of 0.938 and 0.914 for the two observers, respectively. These results demonstrated the significance of early detection of body-related sensorimotor features.},
	language = {en},
	urldate = {2025-02-08},
	journal = {Computers in Human Behavior},
	author = {Paolucci, Claudio and Giorgini, Federica and Scheda, Riccardo and Alessi, Flavio Valerio and Diciotti, Stefano},
	month = nov,
	year = {2023},
	pages = {107877},
	file = {2023_Paolucci et al._Early prediction of Autism Spectrum Disorders through interaction analysis in home videos and explai.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2023_Paolucci et al._Early prediction of Autism Spectrum Disorders through interaction analysis in home videos and explai.pdf:application/pdf},
}

@article{ko2023,
	title = {Development and {Validation} of a {Joint} {Attention}-{Based} {Deep} {Learning} {System} for {Detection} and {Symptom} {Severity} {Assessment} of {Autism} {Spectrum} {Disorder}},
	volume = {6},
	issn = {2574-3805},
	doi = {10.1001/jamanetworkopen.2023.15174},
	abstract = {IMPORTANCE: Joint attention, composed of complex behaviors, is an early-emerging social function that is deficient in children with autism spectrum disorder (ASD). Currently, no methods are available for objectively quantifying joint attention.
OBJECTIVE: To train deep learning (DL) models to distinguish ASD from typical development (TD) and to differentiate ASD symptom severities using video data of joint attention behaviors.
DESIGN, SETTING, AND PARTICIPANTS: In this diagnostic study, joint attention tasks were administered to children with and without ASD, and video data were collected from multiple institutions from August 5, 2021, to July 18, 2022. Of 110 children, 95 (86.4\%) completed study measures. Enrollment criteria were 24 to 72 months of age and ability to sit with no history of visual or auditory deficits.
EXPOSURES: Children were screened using the Childhood Autism Rating Scale. Forty-five children were diagnosed with ASD. Three types of joint attention were assessed using a specific protocol.
MAIN OUTCOMES AND MEASURES: Correctly distinguishing ASD from TD and different levels of ASD symptom severity using the DL model area under the receiver operating characteristic curve (AUROC), accuracy, precision, and recall.
RESULTS: The analytical population consisted of 45 children with ASD (mean [SD] age, 48.0 [13.4] months; 24 [53.3\%] boys) vs 50 with TD (mean [SD] age, 47.9 [12.5] months; 27 [54.0\%] boys). The DL ASD vs TD models showed good predictive performance for initiation of joint attention (IJA) (AUROC, 99.6\% [95\% CI, 99.4\%-99.7\%]; accuracy, 97.6\% [95\% CI, 97.1\%-98.1\%]; precision, 95.5\% [95\% CI, 94.4\%-96.5\%]; and recall, 99.2\% [95\% CI, 98.7\%-99.6\%]), low-level response to joint attention (RJA) (AUROC, 99.8\% [95\% CI, 99.6\%-99.9\%]; accuracy, 98.8\% [95\% CI, 98.4\%-99.2\%]; precision, 98.9\% [95\% CI, 98.3\%-99.4\%]; and recall, 99.1\% [95\% CI, 98.6\%-99.5\%]), and high-level RJA (AUROC, 99.5\% [95\% CI, 99.2\%-99.8\%]; accuracy, 98.4\% [95\% CI, 97.9\%-98.9\%]; precision, 98.8\% [95\% CI, 98.2\%-99.4\%]; and recall, 98.6\% [95\% CI, 97.9\%-99.2\%]). The DL-based ASD symptom severity models showed reasonable predictive performance for IJA (AUROC, 90.3\% [95\% CI, 88.8\%-91.8\%]; accuracy, 84.8\% [95\% CI, 82.3\%-87.2\%]; precision, 76.2\% [95\% CI, 72.9\%-79.6\%]; and recall, 84.8\% [95\% CI, 82.3\%-87.2\%]), low-level RJA (AUROC, 84.4\% [95\% CI, 82.0\%-86.7\%]; accuracy, 78.4\% [95\% CI, 75.0\%-81.7\%]; precision, 74.7\% [95\% CI, 70.4\%-78.8\%]; and recall, 78.4\% [95\% CI, 75.0\%-81.7\%]), and high-level RJA (AUROC, 84.2\% [95\% CI, 81.8\%-86.6\%]; accuracy, 81.0\% [95\% CI, 77.3\%-84.4\%]; precision, 68.6\% [95\% CI, 63.8\%-73.6\%]; and recall, 81.0\% [95\% CI, 77.3\%-84.4\%]).
CONCLUSIONS AND RELEVANCE: In this diagnostic study, DL models for identifying ASD and differentiating levels of ASD symptom severity were developed and the premises for DL-based predictions were visualized. The findings suggest that this method may allow digital measurement of joint attention; however, follow-up studies are necessary for further validation.},
	language = {eng},
	number = {5},
	journal = {JAMA network open},
	author = {Ko, Chanyoung and Lim, Jae-Hyun and Hong, JaeSeong and Hong, Soon-Beom and Park, Yu Rang},
	month = may,
	year = {2023},
	pmid = {37227727},
	pmcid = {PMC10214037},
	keywords = {Autism Spectrum Disorder, Autistic Disorder, Child, Humans, Deep Learning, Female, Male, Middle Aged},
	pages = {e2315174},
	file = {2023_Ko et al._Development and Validation of a Joint Attention-Based Deep Learning System for Detection and Symptom.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2023_Ko et al._Development and Validation of a Joint Attention-Based Deep Learning System for Detection and Symptom.pdf:application/pdf},
}

@article{debelen2024,
	title = {Using visual attention estimation on videos for automated prediction of autism spectrum disorder and symptom severity in preschool children},
	volume = {19},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0282818},
	abstract = {Atypical visual attention in individuals with autism spectrum disorders (ASD) has been utilised as a unique diagnosis criterion in previous research. This paper presents a novel approach to the automatic and quantitative screening of ASD as well as symptom severity prediction in preschool children. We develop a novel computational pipeline that extracts learned features from a dynamic visual stimulus to classify ASD children and predict the level of ASD-related symptoms. Experimental results demonstrate promising performance that is superior to using handcrafted features and machine learning algorithms, in terms of evaluation metrics used in diagnostic tests. Using a leave-one-out cross-validation approach, we obtained an accuracy of 94.59\%, a sensitivity of 100\%, a specificity of 76.47\% and an area under the receiver operating characteristic curve (AUC) of 96\% for ASD classification. In addition, we obtained an accuracy of 94.74\%, a sensitivity of 87.50\%, a specificity of 100\% and an AUC of 99\% for ASD symptom severity prediction.},
	language = {eng},
	number = {2},
	journal = {PloS One},
	author = {de Belen, Ryan Anthony J. and Eapen, Valsamma and Bednarz, Tomasz and Sowmya, Arcot},
	year = {2024},
	pmid = {38346053},
	pmcid = {PMC10861059},
	keywords = {Autism Spectrum Disorder, Humans, Algorithms, Child, Preschool, Machine Learning, ROC Curve, Videotape Recording},
	pages = {e0282818},
	file = {2024_de Belen et al._Using visual attention estimation on videos for automated prediction of autism spectrum disorder and.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_de Belen et al._Using visual attention estimation on videos for automated prediction of autism spectrum disorder and.pdf:application/pdf},
}

@inproceedings{wu2021,
	title = {Machine {Learning} {Based} {Autism} {Spectrum} {Disorder} {Detection} from {Videos}},
	url = {https://ieeexplore.ieee.org/document/9398924},
	doi = {10.1109/HEALTHCOM49281.2021.9398924},
	abstract = {Early diagnosis of Autism Spectrum Disorder (ASD) is crucial for best outcomes to interventions. In this paper, we present a machine learning (ML) approach to ASD diagnosis based on identifying specific behaviors from videos of infants of ages 6 through 36 months. The behaviors of interest include directed gaze towards faces or objects of interest, positive affect, and vocalization. The dataset consists of 2000 videos of 3-minute duration with these behaviors manually coded by expert raters. Moreover, the dataset has statistical features including duration and frequency of the above mentioned behaviors in the video collection as well as independent ASD diagnosis by clinicians. We tackle the ML problem in a two-stage approach. Firstly, we develop deep learning models for automatic identification of clinically relevant behaviors exhibited by infants in a one-on-one interaction setting with parents or expert clinicians. We report baseline results of behavior classification using two methods: (1) image based model (2) facial behavior features based model. We achieve 70\% accuracy for smile, 68\% accuracy for look face, 67\% for look object and 53\% accuracy for vocalization. Secondly, we focus on ASD diagnosis prediction by applying a feature selection process to identify the most significant statistical behavioral features and a over and under sampling process to mitigate the class imbalance, followed by developing a baseline ML classifier to achieve an accuracy of 82\% for ASD diagnosis.},
	urldate = {2025-02-08},
	booktitle = {2020 {IEEE} {International} {Conference} on {E}-health {Networking}, {Application} \& {Services} ({HEALTHCOM})},
	author = {Wu, Chongruo and Liaqat, Sidrah and Helvaci, Halil and Chcung, Sen-ching Samson and Chuah, Chen-Nee and Ozonoff, Sally and Young, Gregory},
	month = mar,
	year = {2021},
	keywords = {Autism Spectrum Disorder, Autism, Faces, Training, Machine Learning, Videos, Feature extraction, Neural networks, Facial Keypoint Detection, Human Behavior Detection, Manuals},
	pages = {1--6},
	file = {2021_Wu et al._Machine Learning Based Autism Spectrum Disorder Detection from Videos.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2021_Wu et al._Machine Learning Based Autism Spectrum Disorder Detection from Videos.pdf:application/pdf},
}

@article{varma2022,
	title = {Identification of {Social} {Engagement} {Indicators} {Associated} {With} {Autism} {Spectrum} {Disorder} {Using} a {Game}-{Based} {Mobile} {App}: {Comparative} {Study} of {Gaze} {Fixation} and {Visual} {Scanning} {Methods}},
	volume = {24},
	issn = {1438-8871},
	shorttitle = {Identification of {Social} {Engagement} {Indicators} {Associated} {With} {Autism} {Spectrum} {Disorder} {Using} a {Game}-{Based} {Mobile} {App}},
	doi = {10.2196/31830},
	abstract = {BACKGROUND: Autism spectrum disorder (ASD) is a widespread neurodevelopmental condition with a range of potential causes and symptoms. Standard diagnostic mechanisms for ASD, which involve lengthy parent questionnaires and clinical observation, often result in long waiting times for results. Recent advances in computer vision and mobile technology hold potential for speeding up the diagnostic process by enabling computational analysis of behavioral and social impairments from home videos. Such techniques can improve objectivity and contribute quantitatively to the diagnostic process.
OBJECTIVE: In this work, we evaluate whether home videos collected from a game-based mobile app can be used to provide diagnostic insights into ASD. To the best of our knowledge, this is the first study attempting to identify potential social indicators of ASD from mobile phone videos without the use of eye-tracking hardware, manual annotations, and structured scenarios or clinical environments.
METHODS: Here, we used a mobile health app to collect over 11 hours of video footage depicting 95 children engaged in gameplay in a natural home environment. We used automated data set annotations to analyze two social indicators that have previously been shown to differ between children with ASD and their neurotypical (NT) peers: (1) gaze fixation patterns, which represent regions of an individual's visual focus and (2) visual scanning methods, which refer to the ways in which individuals scan their surrounding environment. We compared the gaze fixation and visual scanning methods used by children during a 90-second gameplay video to identify statistically significant differences between the 2 cohorts; we then trained a long short-term memory (LSTM) neural network to determine if gaze indicators could be predictive of ASD.
RESULTS: Our results show that gaze fixation patterns differ between the 2 cohorts; specifically, we could identify 1 statistically significant region of fixation (P{\textless}.001). In addition, we also demonstrate that there are unique visual scanning patterns that exist for individuals with ASD when compared to NT children (P{\textless}.001). A deep learning model trained on coarse gaze fixation annotations demonstrates mild predictive power in identifying ASD.
CONCLUSIONS: Ultimately, our study demonstrates that heterogeneous video data sets collected from mobile devices hold potential for quantifying visual patterns and providing insights into ASD. We show the importance of automated labeling techniques in generating large-scale data sets while simultaneously preserving the privacy of participants, and we demonstrate that specific social engagement indicators associated with ASD can be identified and characterized using such data.},
	language = {eng},
	number = {2},
	journal = {Journal of Medical Internet Research},
	author = {Varma, Maya and Washington, Peter and Chrisman, Brianna and Kline, Aaron and Leblanc, Emilie and Paskov, Kelley and Stockham, Nate and Jung, Jae-Yoon and Sun, Min Woo and Wall, Dennis P.},
	month = feb,
	year = {2022},
	pmid = {35166683},
	pmcid = {PMC8889483},
	keywords = {Autism Spectrum Disorder, autism, gaze, Child, Humans, autism spectrum disorder, app, computer vision, Computers, Handheld, diagnostic, engagement, Fixation, Ocular, gaming, insight, Mobile Applications, mobile diagnostics, mobile health, pattern, pattern recognition, Social Participation, social phenotyping, video, vision},
	pages = {e31830},
	file = {2022_Varma et al._Identification of Social Engagement Indicators Associated With Autism Spectrum Disorder Using a Game.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2022_Varma et al._Identification of Social Engagement Indicators Associated With Autism Spectrum Disorder Using a Game.pdf:application/pdf},
}

@article{washington2022,
	title = {Crowd annotations can approximate clinical autism impressions from short home videos with privacy protections},
	volume = {6},
	issn = {2666-5212},
	url = {https://www.sciencedirect.com/science/article/pii/S2666521222000096},
	doi = {10.1016/j.ibmed.2022.100056},
	abstract = {Artificial Intelligence (A.I.) solutions are increasingly considered for telemedicine. For these methods to serve children and their families in home settings, it is crucial to ensure the privacy of the child and parent or caregiver. To address this challenge, we explore the potential for global image transformations to provide privacy while preserving the quality of behavioral annotations. Crowd workers have previously been shown to reliably annotate behavioral features in unstructured home videos, allowing machine learning classifiers to detect autism using the annotations as input. We evaluate this method with videos altered via pixelation, dense optical flow, and Gaussian blurring. On a balanced test set of 30 videos of children with autism and 30 neurotypical controls, we find that the visual privacy alterations do not drastically alter any individual behavioral annotation at the item level. The AUROC on the evaluation set was 90.0\% ±7.5\% for unaltered videos, 85.0\% ±9.0\% for pixelation, 85.0\% ±9.0\% for optical flow, and 83.3\% ±9.3\% for blurring, demonstrating that an aggregation of small changes across behavioral questions can collectively result in increased misdiagnosis rates. We also compare crowd answers against clinicians who provided the same annotations for the same videos as crowd workers, and we find that clinicians have higher sensitivity in their recognition of autism-related symptoms. We also find that there is a linear correlation (r = 0.75, p {\textless} 0.0001) between the mean Clinical Global Impression (CGI) score provided by professional clinicians and the corresponding score emitted by a previously validated autism classifier with crowd inputs, indicating that the classifier's output probability is a reliable estimate of the clinical impression of autism. A significant correlation is maintained with privacy alterations, indicating that crowd annotations can approximate clinician-provided autism impression from home videos in a privacy-preserved manner.},
	urldate = {2025-02-08},
	journal = {Intelligence-Based Medicine},
	author = {Washington, Peter and Chrisman, Brianna and Leblanc, Emilie and Dunlap, Kaitlyn and Kline, Aaron and Mutlu, Cezmi and Stockham, Nate and Paskov, Kelley and Wall, Dennis Paul},
	month = jan,
	year = {2022},
	pages = {100056},
	file = {2022_Washington et al._Crowd annotations can approximate clinical autism impressions from short home videos with privacy pr.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2022_Washington et al._Crowd annotations can approximate clinical autism impressions from short home videos with privacy pr.pdf:application/pdf},
}

@article{megerian2022b,
	title = {Evaluation of an artificial intelligence-based medical device for diagnosis of autism spectrum disorder},
	volume = {5},
	copyright = {2022 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-022-00598-6},
	doi = {10.1038/s41746-022-00598-6},
	abstract = {Autism spectrum disorder (ASD) can be reliably diagnosed at 18 months, yet significant diagnostic delays persist in the United States. This double-blinded, multi-site, prospective, active comparator cohort study tested the accuracy of an artificial intelligence-based Software as a Medical Device designed to aid primary care healthcare providers (HCPs) in diagnosing ASD. The Device combines behavioral features from three distinct inputs (a caregiver questionnaire, analysis of two short home videos, and an HCP questionnaire) in a gradient boosted decision tree machine learning algorithm to produce either an ASD positive, ASD negative, or indeterminate output. This study compared Device outputs to diagnostic agreement by two or more independent specialists in a cohort of 18–72-month-olds with developmental delay concerns (425 study completers, 36\% female, 29\% ASD prevalence). Device output PPV for all study completers was 80.8\% (95\% confidence intervals (CI), 70.3\%–88.8\%) and NPV was 98.3\% (90.6\%–100\%). For the 31.8\% of participants who received a determinate output (ASD positive or negative) Device sensitivity was 98.4\% (91.6\%–100\%) and specificity was 78.9\% (67.6\%–87.7\%). The Device’s indeterminate output acts as a risk control measure when inputs are insufficiently granular to make a determinate recommendation with confidence. If this risk control measure were removed, the sensitivity for all study completers would fall to 51.6\% (63/122) (95\% CI 42.4\%, 60.8\%), and specificity would fall to 18.5\% (56/303) (95\% CI 14.3\%, 23.3\%). Among participants for whom the Device abstained from providing a result, specialists identified that 91\% had one or more complex neurodevelopmental disorders. No significant differences in Device performance were found across participants’ sex, race/ethnicity, income, or education level. For nearly a third of this primary care sample, the Device enabled timely diagnostic evaluation with a high degree of accuracy. The Device shows promise to significantly increase the number of children able to be diagnosed with ASD in a primary care setting, potentially facilitating earlier intervention and more efficient use of specialist resources.},
	language = {en},
	number = {1},
	urldate = {2025-02-08},
	journal = {npj Digital Medicine},
	author = {Megerian, Jonathan T. and Dey, Sangeeta and Melmed, Raun D. and Coury, Daniel L. and Lerner, Marc and Nicholls, Christopher J. and Sohl, Kristin and Rouhbakhsh, Rambod and Narasimhan, Anandhi and Romain, Jonathan and Golla, Sailaja and Shareef, Safiullah and Ostrovsky, Andrey and Shannon, Jennifer and Kraft, Colleen and Liu-Mayo, Stuart and Abbas, Halim and Gal-Szabo, Diana E. and Wall, Dennis P. and Taraman, Sharief},
	month = may,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Machine learning, Autism spectrum disorders, Diagnosis, Paediatric research},
	pages = {1--11},
	file = {2022_Megerian et al._Evaluation of an artificial intelligence-based medical device for diagnosis of autism spectrum disor.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2022_Megerian et al._Evaluation of an artificial intelligence-based medical device for diagnosis of autism spectrum disor.pdf:application/pdf},
}

@article{wu2021a,
	title = {Machine {Learning} {Based} {Autism} {Spectrum} {Disorder} {Detection} from {Videos}},
	volume = {2020},
	doi = {10.1109/healthcom49281.2021.9398924},
	abstract = {Early diagnosis of Autism Spectrum Disorder (ASD) is crucial for best outcomes to interventions. In this paper, we present a machine learning (ML) approach to ASD diagnosis based on identifying specific behaviors from videos of infants of ages 6 through 36 months. The behaviors of interest include directed gaze towards faces or objects of interest, positive affect, and vocalization. The dataset consists of 2000 videos of 3-minute duration with these behaviors manually coded by expert raters. Moreover, the dataset has statistical features including duration and frequency of the above mentioned behaviors in the video collection as well as independent ASD diagnosis by clinicians. We tackle the ML problem in a two-stage approach. Firstly, we develop deep learning models for automatic identification of clinically relevant behaviors exhibited by infants in a one-on-one interaction setting with parents or expert clinicians. We report baseline results of behavior classification using two methods: (1) image based model (2) facial behavior features based model. We achieve 70\% accuracy for smile, 68\% accuracy for look face, 67\% for look object and 53\% accuracy for vocalization. Secondly, we focus on ASD diagnosis prediction by applying a feature selection process to identify the most significant statistical behavioral features and a over and under sampling process to mitigate the class imbalance, followed by developing a baseline ML classifier to achieve an accuracy of 82\% for ASD diagnosis.},
	language = {eng},
	journal = {Healthcom. International Conference on E-Health Networking, Applications and Services},
	author = {Wu, Chongruo and Liaqat, Sidrah and Helvaci, Halil and Cheung, Sen-Ching Samson and Chuah, Chen-Nee and Ozonoff, Sally and Young, Gregory},
	month = mar,
	year = {2021},
	pmid = {34693405},
	pmcid = {PMC8528233},
	keywords = {Autism Spectrum Disorder, Machine Learning, Facial Keypoint Detection, Human Behavior Detection},
	file = {2021_Wu et al._Machine Learning Based Autism Spectrum Disorder Detection from Videos.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2021_Wu et al._Machine Learning Based Autism Spectrum Disorder Detection from Videos.pdf:application/pdf},
}

@article{kojovic2021,
	title = {Using {2D} video-based pose estimation for automated prediction of autism spectrum disorders in young children},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-94378-z},
	doi = {10.1038/s41598-021-94378-z},
	abstract = {Clinical research in autism has recently witnessed promising digital phenotyping results, mainly focused on single feature extraction, such as gaze, head turn on name-calling or visual tracking of the moving object. The main drawback of these studies is the focus on relatively isolated behaviors elicited by largely controlled prompts. We recognize that while the diagnosis process understands the indexing of the specific behaviors, ASD also comes with broad impairments that often transcend single behavioral acts. For instance, the atypical nonverbal behaviors manifest through global patterns of atypical postures and movements, fewer gestures used and often decoupled from visual contact, facial affect, speech. Here, we tested the hypothesis that a deep neural network trained on the non-verbal aspects of social interaction can effectively differentiate between children with ASD and their typically developing peers. Our model achieves an accuracy of 80.9\% (F1 score: 0.818; precision: 0.784; recall: 0.854) with the prediction probability positively correlated to the overall level of symptoms of autism in social affect and repetitive and restricted behaviors domain. Provided the non-invasive and affordable nature of computer vision, our approach carries reasonable promises that a reliable machine-learning-based ASD screening may become a reality not too far in the future.},
	language = {en},
	number = {1},
	urldate = {2025-02-08},
	journal = {Scientific Reports},
	author = {Kojovic, Nada and Natraj, Shreyasvi and Mohanty, Sharada Prasanna and Maillart, Thomas and Schaer, Marie},
	month = jul,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Diagnostic markers, Human behaviour, Social behaviour},
	pages = {15069},
	file = {2021_Kojovic et al._Using 2D video-based pose estimation for automated prediction of autism spectrum disorders in young.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2021_Kojovic et al._Using 2D video-based pose estimation for automated prediction of autism spectrum disorders in young.pdf:application/pdf},
}

@article{nabil2021,
	title = {Applying machine learning on home videos for remote autism diagnosis: {Further} study and analysis},
	volume = {27},
	issn = {1741-2811},
	shorttitle = {Applying machine learning on home videos for remote autism diagnosis},
	doi = {10.1177/1460458221991882},
	abstract = {Autism Spectrum Disorder (Autism) is a developmental disorder that impedes the social and communication capabilities of a person through out his life. Early detection of autism is critical in contributing to better prognosis. In this study, the use of home videos to provide accessible diagnosis is investigated. A machine learning approach is adopted to detect autism from home videos. Feature selection and state-of-the-art classification methods are applied to provide a sound diagnosis based on home video ratings obtained from non-clinicians feedback. Our models results indicate that home videos can effectively detect autistic group with True Positive Rate reaching 94.05\% using Support Vector Machines and backwards feature selection. In this study, human-interpretable models are presented to elucidate the reasoning behind the classification process and its subsequent decision. In addition, the prime features that need to be monitored for early autism detection are revealed.},
	language = {eng},
	number = {1},
	journal = {Health Informatics Journal},
	author = {Nabil, Mohamed A. and Akram, Ansam and Fathalla, Karma M.},
	year = {2021},
	pmid = {33583277},
	keywords = {Autism Spectrum Disorder, autism, screening, machine learning, Autistic Disorder, Humans, Machine Learning, diagnosis, Early Diagnosis, feature selection, Support Vector Machine},
	pages = {1460458221991882},
	file = {2021_Nabil et al._Applying machine learning on home videos for remote autism diagnosis Further study and analysis.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2021_Nabil et al._Applying machine learning on home videos for remote autism diagnosis Further study and analysis.pdf:application/pdf},
}

@article{leblanc2020,
	title = {Feature replacement methods enable reliable home video analysis for machine learning detection of autism},
	volume = {10},
	copyright = {2020 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-020-76874-w},
	doi = {10.1038/s41598-020-76874-w},
	abstract = {Autism Spectrum Disorder is a neuropsychiatric condition affecting 53 million children worldwide and for which early diagnosis is critical to the outcome of behavior therapies. Machine learning applied to features manually extracted from readily accessible videos (e.g., from smartphones) has the potential to scale this diagnostic process. However, nearly unavoidable variability in video quality can lead to missing features that degrade algorithm performance. To manage this uncertainty, we evaluated the impact of missing values and feature imputation methods on two previously published autism detection classifiers, trained on standard-of-care instrument scoresheets and tested on ratings of 140 children videos from YouTube. We compare the baseline method of listwise deletion to classic univariate and multivariate techniques. We also introduce a feature replacement method that, based on a score, selects a feature from an expanded dataset to fill-in the missing value. The replacement feature selected can be identical for all records (general) or automatically adjusted to the record considered (dynamic). Our results show that general and dynamic feature replacement methods achieve a higher performance than classic univariate and multivariate methods, supporting the hypothesis that algorithmic management can maintain the fidelity of video-based diagnostics in the face of missing values and variable video quality.},
	language = {en},
	number = {1},
	urldate = {2025-02-08},
	journal = {Scientific Reports},
	author = {Leblanc, Emilie and Washington, Peter and Varma, Maya and Dunlap, Kaitlyn and Penev, Yordan and Kline, Aaron and Wall, Dennis P.},
	month = dec,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	keywords = {Machine learning, Autism spectrum disorders, Paediatric research},
	pages = {21245},
	file = {2020_Leblanc et al._Feature replacement methods enable reliable home video analysis for machine learning detection of au.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2020_Leblanc et al._Feature replacement methods enable reliable home video analysis for machine learning detection of au.pdf:application/pdf},
}

@article{ardalan2019,
	title = {Whole-{Body} {Movement} during {Videogame} {Play} {Distinguishes} {Youth} with {Autism} from {Youth} with {Typical} {Development}},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-56362-6},
	doi = {10.1038/s41598-019-56362-6},
	abstract = {Individuals with autism spectrum disorder struggle with motor difficulties throughout the life span, and these motor difficulties may affect independent living skills and quality of life. Yet, we know little about how whole-body movement may distinguish individuals with autism spectrum disorder from individuals with typical development. In this study, kinematic and postural sway data were collected during multiple sessions of videogame play in 39 youth with autism spectrum disorder and 23 age-matched youth with typical development (ages 7–17 years). The youth on the autism spectrum exhibited more variability and more entropy in their movements. Machine learning analysis of the youths’ motor patterns distinguished between the autism spectrum and typically developing groups with high aggregate accuracy (up to 89\%), with no single region of the body seeming to drive group differences. Moreover, the machine learning results corresponded to individual differences in performance on standardized motor tasks and measures of autism symptom severity. The machine learning algorithm was also sensitive to age, suggesting that motor challenges in autism may be best characterized as a developmental motor delay rather than an autism-distinct motor profile. Overall, these results reveal that whole-body movement is a distinguishing feature in autism spectrum disorder and that movement atypicalities in autism are present across the body.},
	language = {en},
	number = {1},
	urldate = {2025-02-08},
	journal = {Scientific Reports},
	author = {Ardalan, Adel and Assadi, Amir H. and Surgent, Olivia J. and Travers, Brittany G.},
	month = dec,
	year = {2019},
	note = {Publisher: Nature Publishing Group},
	keywords = {Autism spectrum disorders, Human behaviour, Motor control},
	pages = {20094},
	file = {2019_Ardalan et al._Whole-Body Movement during Videogame Play Distinguishes Youth with Autism from Youth with Typical De.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2019_Ardalan et al._Whole-Body Movement during Videogame Play Distinguishes Youth with Autism from Youth with Typical De.pdf:application/pdf},
}

@article{ganai2025,
	title = {Early detection of autism spectrum disorder: gait deviations and machine learning},
	volume = {15},
	copyright = {2025 The Author(s)},
	issn = {2045-2322},
	shorttitle = {Early detection of autism spectrum disorder},
	url = {https://www.nature.com/articles/s41598-025-85348-w},
	doi = {10.1038/s41598-025-85348-w},
	abstract = {Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder diagnosed by clinicians and experts through questionnaires, observations, and interviews. Current diagnostic practices focus on social and communication impairments, which often emerge later in life. This delay in detection results in missed opportunities for early intervention. Gait, a motor behavior, has been previously shown to be aberrant in children with ASD and may be a biomarker for early detection and diagnosis of ASD. The current study assessed gait in children with ASD using a single RGB camera-based pose estimation method by MediaPipe (MP). Data from 32 children with ASD and 29 typically developing (TD) children were collected. The ASD group exhibited significantly reduced step length and right elbow° and increased right shoulder° relative to TD children. Four machine learning (ML) algorithms were employed to classify the ASD and TD children based on the statistically significant gait parameters. The binomial logistic regression (Logit) performed the best, with an accuracy of 0.82, in classifying the ASD and TD children. The present study demonstrates the use of gait analysis and ML techniques for the early detection of ASD.},
	language = {en},
	number = {1},
	urldate = {2025-02-08},
	journal = {Scientific Reports},
	author = {Ganai, Umer Jon and Ratne, Aditya and Bhushan, Braj and Venkatesh, K. S.},
	month = jan,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Psychology},
	pages = {873},
	file = {2025_Ganai et al._Early detection of autism spectrum disorder gait deviations and machine learning.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2025_Ganai et al._Early detection of autism spectrum disorder gait deviations and machine learning.pdf:application/pdf},
}

@article{dong2024,
	title = {An enhanced real-time human pose estimation method based on modified {YOLOv8} framework},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-58146-z},
	doi = {10.1038/s41598-024-58146-z},
	abstract = {The objective of human pose estimation (HPE) derived from deep learning aims to accurately estimate and predict the human body posture in images or videos via the utilization of deep neural networks. However, the accuracy of real-time HPE tasks is still to be improved due to factors such as partial occlusion of body parts and limited receptive field of the model. To alleviate the accuracy loss caused by these issues, this paper proposes a real-time HPE model called \$\$\{{\textbackslash}textbf \{CCAM-Person\}\}\$\$based on the YOLOv8 framework. Specifically, we have improved the backbone and neck of the YOLOv8x-pose real-time HPE model to alleviate the feature loss and receptive field constraints. Secondly, we introduce the context coordinate attention module (CCAM) to augment the model’s focus on salient features, reduce background noise interference, alleviate key point regression failure caused by limb occlusion, and improve the accuracy of pose estimation. Our approach attains competitive results on multiple metrics of two open-source datasets, MS COCO 2017 and CrowdPose. Compared with the baseline model YOLOv8x-pose, CCAM-Person improves the average precision by 2.8\% and 3.5\% on the two datasets, respectively.},
	language = {en},
	number = {1},
	urldate = {2025-02-09},
	journal = {Scientific Reports},
	author = {Dong, Chengang and Du, Guodong},
	month = apr,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Software, Computational science, Imaging and sensing},
	pages = {8012},
	file = {2024_Dong and Du_An enhanced real-time human pose estimation method based on modified YOLOv8 framework.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Body-Tracking\\2024_Dong and Du_An enhanced real-time human pose estimation method based on modified YOLOv8 framework.pdf:application/pdf},
}

@misc{xu2023,
	title = {{ViTPose}++: {Vision} {Transformer} for {Generic} {Body} {Pose} {Estimation}},
	shorttitle = {{ViTPose}++},
	url = {http://arxiv.org/abs/2212.04246},
	doi = {10.48550/arXiv.2212.04246},
	abstract = {In this paper, we show the surprisingly good properties of plain vision transformers for body pose estimation from various aspects, namely simplicity in model structure, scalability in model size, flexibility in training paradigm, and transferability of knowledge between models, through a simple baseline model dubbed ViTPose. Specifically, ViTPose employs the plain and non-hierarchical vision transformer as an encoder to encode features and a lightweight decoder to decode body keypoints in either a top-down or a bottom-up manner. It can be scaled up from about 20M to 1B parameters by taking advantage of the scalable model capacity and high parallelism of the vision transformer, setting a new Pareto front for throughput and performance. Besides, ViTPose is very flexible regarding the attention type, input resolution, and pre-training and fine-tuning strategy. Based on the flexibility, a novel ViTPose+ model is proposed to deal with heterogeneous body keypoint categories in different types of body pose estimation tasks via knowledge factorization, i.e., adopting task-agnostic and task-specific feed-forward networks in the transformer. We also empirically demonstrate that the knowledge of large ViTPose models can be easily transferred to small ones via a simple knowledge token. Experimental results show that our ViTPose model outperforms representative methods on the challenging MS COCO Human Keypoint Detection benchmark at both top-down and bottom-up settings. Furthermore, our ViTPose+ model achieves state-of-the-art performance simultaneously on a series of body pose estimation tasks, including MS COCO, AI Challenger, OCHuman, MPII for human keypoint detection, COCO-Wholebody for whole-body keypoint detection, as well as AP-10K and APT-36K for animal keypoint detection, without sacrificing inference speed.},
	urldate = {2025-02-09},
	publisher = {arXiv},
	author = {Xu, Yufei and Zhang, Jing and Zhang, Qiming and Tao, Dacheng},
	month = dec,
	year = {2023},
	note = {arXiv:2212.04246 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {2023_Xu et al._ViTPose++ Vision Transformer for Generic Body Pose Estimation.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Body-Tracking\\2023_Xu et al._ViTPose++ Vision Transformer for Generic Body Pose Estimation.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\GHUPTNHE\\2212.html:text/html},
}

@article{bucci2024,
	title = {Eye {Movements} and {Postural} {Control} in {Children}; {Biomarkers} of {Neurodevelopmental} {Disorders}: {Evidences} {Toward} {New} {Forms} of {Therapeutic} {Intervention}?},
	volume = {10},
	issn = {2199-2673},
	shorttitle = {Eye {Movements} and {Postural} {Control} in {Children}; {Biomarkers} of {Neurodevelopmental} {Disorders}},
	url = {https://doi.org/10.1007/s40817-024-00172-w},
	doi = {10.1007/s40817-024-00172-w},
	abstract = {Autism (ASD), attention deficit/hyperactive disorders (ADHD), and dyslexia (DYS) are defined in DSM-5 as neurodevelopment disorders (NDDs) and several times they are comorbid disorders. Several studies have reported poor oculomotor performance and postural instability in this population of children confirming the cerebellar deficit hypothesis in subjects with NDDs. In this paper, we summarized oculomotor and posture findings collected over the last decade, in order to find out biomarkers for children with NDDs; To our knowledge, such issue has never been studied before; the present study is designed to fill this gap. Oculomotor parameters (the number of express and anticipate saccades, the number of saccades during fixations and the number of catch-up saccades during pursuits), and the postural instability index measured in two viewing conditions (eyes open and eyes closed, on stable and unstable platform) were compared in four groups of 40 children. One-way ANOVA reported significant differences in oculomotor parameters between the three groups of children with NDDs with respect to a typical developing (TD) children group: the number of anticipatory saccades was found to be significantly (p {\textless} 0.001) higher in the ADHD (32.4 ± 5.9) and the DYS (34.7 ± 5.3) groups; the occurrence of express saccades and catch-up saccades was significantly (p {\textless} 0.001) more frequent in the ASD group (26.8 ± 1.8 and 55.3 ± 4.2, respectively); unwanted saccades occurred more in ADHD (8.5 ± 0.7) and DYS (7.3 ± 1.1) groups. In the fixation with distractor task, the number of saccades was significantly (p {\textless} 0.001) higher in the ADHD group (36.3 ± 3.9). In addition, the mixed repeated ANOVA on the postural instability index (measured under eyes open and eyes closed conditions, on stable and unstable platforms) reported a significant difference (p {\textless} 0.001) in such parameters in children with NDDs (ASD: 2.8 ± 0.6; DYS: 2.6 ± 0.5; ADHD: 2.7 ± 0.5, respectively) with respect to the TD group (1.9 ± 0.3); however, the postural instability index failed to distinguish between the different neurodevelopmental deficits. Such abnormal oculomotor and postural performances are in line with poor cerebellar activity; oculomotor measures only could be used as phenotype biomarkers for children with NDDs. Finally, these results could be useful to clinicians looking to develop specific oculomotor and/or postural training programs based on visual fixations and body stability exercises designed to reinforce motor ability in children with NDDs.},
	language = {en},
	number = {3},
	urldate = {2025-02-09},
	journal = {Journal of Pediatric Neuropsychology},
	author = {Bucci, Maria Pia and Moscoso, Ana and Acquaviva, Eric and Humeau, Elise and Delorme, Richard},
	month = sep,
	year = {2024},
	keywords = {Children, Neurodevelopmental Disorders, Cerebellum, Eye movements, Frontal eye filed, Neurodevelopment disorders, Postural control},
	pages = {231--242},
	file = {2024_Bucci et al._Eye Movements and Postural Control in Children\; Biomarkers of Neurodevelopmental Disorders Evidence.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Bucci et al._Eye Movements and Postural Control in Children\; Biomarkers of Neurodevelopmental Disorders Evidence.pdf:application/pdf},
}

@article{coffman2023,
	title = {Relationship between quantitative digital behavioral features and clinical profiles in young autistic children},
	volume = {16},
	copyright = {© 2023 International Society for Autism Research and Wiley Periodicals LLC.},
	issn = {1939-3806},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aur.2955},
	doi = {10.1002/aur.2955},
	abstract = {Early behavioral markers for autism include differences in social attention and orienting in response to one's name when called, and differences in body movements and motor abilities. More efficient, scalable, objective, and reliable measures of these behaviors could improve early screening for autism. This study evaluated whether objective and quantitative measures of autism-related behaviors elicited from an app (SenseToKnow) administered on a smartphone or tablet and measured via computer vision analysis (CVA) are correlated with standardized caregiver-report and clinician administered measures of autism-related behaviors and cognitive, language, and motor abilities. This is an essential step in establishing the concurrent validity of a digital phenotyping approach. In a sample of 485 toddlers, 43 of whom were diagnosed with autism, we found that CVA-based gaze variables related to social attention were associated with the level of autism-related behaviors. Two language-related behaviors measured via the app, attention to people during a conversation and responding to one's name being called, were associated with children's language skills. Finally, performance during a bubble popping game was associated with fine motor skills. These findings provide initial support for the concurrent validity of the SenseToKnow app and its potential utility in identifying clinical profiles associated with autism. Future research is needed to determine whether the app can be used as an autism screening tool, can reliably stratify autism-related behaviors, and measure changes in autism-related behaviors over time.},
	language = {en},
	number = {7},
	urldate = {2025-02-10},
	journal = {Autism Research},
	author = {Coffman, Marika and Di Martino, J. Matias and Aiello, Rachel and Carpenter, Kimberly L. H. and Chang, Zhuoqing and Compton, Scott and Eichner, Brian and Espinosa, Steve and Flowers, Jacqueline and Franz, Lauren and Perochon, Sam and Krishnappa Babu, Pradeep Raj and Sapiro, Guillermo and Dawson, Geraldine},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/aur.2955},
	keywords = {autism, screening, computer vision, digital phenotyping},
	pages = {1360--1374},
	file = {2023_Coffman et al._Relationship between quantitative digital behavioral features and clinical profiles in young autisti.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2023_Coffman et al._Relationship between quantitative digital behavioral features and clinical profiles in young autisti.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\JFLZNG6W\\aur.html:text/html},
}

@article{davis2025,
	title = {Behavioral characteristics of toddlers later identified with an autism diagnosis, {ADHD} symptoms, or combined autism and {ADHD} symptoms},
	volume = {66},
	copyright = {© 2024 Association for Child and Adolescent Mental Health.},
	issn = {1469-7610},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jcpp.14050},
	doi = {10.1111/jcpp.14050},
	abstract = {Background Autism commonly co-occurs with attention-deficit/hyperactivity disorder (ADHD), but less is known regarding how ADHD symptoms impact the early presentation of autism. This study examined early behavioral characteristics of a community sample of toddlers later identified with autism diagnosis, ADHD symptoms, combined autism and ADHD symptoms, or neither condition. Methods Participants were 506 toddlers who were part of a longitudinal study of children's behavioral development. Parents completed questionnaires about their children's behavior at two time points. Four groups were identified based on study measures or medical record: autism diagnosis (n = 45), elevated ADHD symptoms (n = 70), autism and ADHD symptoms (n = 30), or neurotypical development (n = 361). Relationships between early parent report of autism- and ADHD-related behaviors, social–emotional and behavioral functioning, and caregiver experience and subsequent group designation were evaluated with adjusted linear regression models controlling for sex. Results Significant group differences were found in measures of autism-related behaviors, ADHD-related behaviors, externalizing and internalizing behaviors, and parent support needs (p {\textless} .0001). Pairwise comparisons indicated toddlers later identified with combined autism diagnosis and ADHD symptoms had higher levels of autism-related behaviors, externalizing and internalizing behaviors, and autism-related parent support needs compared to the other groups. Toddlers with subsequent elevated ADHD symptoms or combined autism diagnosis and ADHD symptoms exhibited similar levels of ADHD-related behaviors, while both groups displayed more ADHD-related behaviors than toddlers subsequently identified with autism or those with neither condition. Conclusions In this community sample, toddlers for whom combined autism diagnosis and ADHD symptoms were subsequently identified showed a distinct presentation characterized by higher early autism-related behaviors, broader behavioral concerns, and higher parent support needs. Presence of ADHD symptoms (alone or in combination with autism) was associated with higher parent-reported ADHD-related behaviors during toddlerhood. Results indicate that ADHD-related behaviors are manifest by toddlerhood, supporting screening for both autism and ADHD during early childhood.},
	language = {en},
	number = {2},
	urldate = {2025-02-10},
	journal = {Journal of Child Psychology and Psychiatry},
	author = {Davis, Naomi O. and Lerebours, Reginald and Aiello, Rachel E. and Carpenter, Kimberly L.H. and Compton, Scott and Franz, Lauren and Kollins, Scott H. and Sabatos-DeVito, Maura and Spanos, Marina and Dawson, Geraldine},
	year = {2025},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jcpp.14050},
	keywords = {Autism, ADHD, screening, preschool children},
	pages = {214--224},
	file = {2025_Davis et al._Behavioral characteristics of toddlers later identified with an autism diagnosis, ADHD symptoms, or.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2025_Davis et al._Behavioral characteristics of toddlers later identified with an autism diagnosis, ADHD symptoms, or.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\AU5X5JTW\\jcpp.html:text/html},
}

@article{griffin2025,
	title = {Spatiotemporal {Eye} {Movement} {Dynamics} {Reveal} {Altered} {Face} {Prioritization} in {Early} {Visual} {Processing} {Among} {Autistic} {Children}},
	volume = {10},
	issn = {2451-9022},
	url = {https://www.sciencedirect.com/science/article/pii/S2451902224002520},
	doi = {10.1016/j.bpsc.2024.08.017},
	abstract = {Background
Reduced social attention—looking at faces—is one of the most common manifestations of social difficulty in autism that is central to social development. Although reduced social attention is well characterized in autism, qualitative differences in how social attention unfolds across time remains unknown.
Methods
We used a computational modeling (i.e., hidden Markov modeling) approach to assess and compare the spatiotemporal dynamics of social attention in a large, well-characterized sample of children with autism (n = 280) and neurotypical children (n = 119) (ages 6–11) who completed 3 social eye-tracking assays at 3 longitudinal time points (baseline, 6 weeks, 24 weeks).
Results
Our analysis supported the existence of 2 common eye movement patterns that emerged across 3 eye-tracking assays. A focused pattern was characterized by small face regions of interest, which had high a probability of capturing fixations early in visual processing. In contrast, an exploratory pattern was characterized by larger face regions of interest, with a lower initial probability of fixation and more nonsocial regions of interest. In the context of social perception, children with autism showed significantly more exploratory eye movement patterns than neurotypical children across all social perception assays and all 3 longitudinal time points. Eye movement patterns were associated with clinical features of autism, including adaptive function, face recognition, and autism symptom severity.
Conclusions
Decreased likelihood of precisely looking at faces early in social visual processing may be an important feature of autism that is associated with autism-related symptomology and may reflect less visual sensitivity to face information.},
	number = {1},
	urldate = {2025-02-10},
	journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
	author = {Griffin, Jason W. and Naples, Adam and Bernier, Raphael and Chawarska, Katarzyna and Dawson, Geraldine and Dziura, James and Faja, Susan and Jeste, Shafali and Kleinhans, Natalia and Sugar, Catherine and Webb, Sara Jane and Shic, Frederick and McPartland, James C.},
	month = jan,
	year = {2025},
	keywords = {Eye tracking, Autism spectrum disorder (ASD), HMM, Markov models, Visual attention, Visual search},
	pages = {45--57},
	file = {2025_Griffin et al._Spatiotemporal Eye Movement Dynamics Reveal Altered Face Prioritization in Early Visual Processing A.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2025_Griffin et al._Spatiotemporal Eye Movement Dynamics Reveal Altered Face Prioritization in Early Visual Processing A.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\CQAFEVDX\\S2451902224002520.html:text/html},
}

@article{kwan2024,
	title = {Constrained {Multivariate} {Functional} {Principal} {Components} {Analysis} for {Novel} {Outcomes} in {Eye}-{Tracking} {Experiments}},
	volume = {16},
	issn = {1867-1772},
	url = {https://doi.org/10.1007/s12561-023-09399-1},
	doi = {10.1007/s12561-023-09399-1},
	abstract = {Individuals with autism spectrum disorder (ASD) tend to experience greater difficulties with social communication and sensory information processing. Of particular interest in ASD biomarker research is the study of visual attention, effectively quantified in eye tracking (ET) experiments. Eye tracking offers a powerful, safe, and feasible platform for gaining insights into attentional processes by measuring moment-by-moment gaze patterns in response to stimuli. Even though recording is done with millisecond granularity, analyses commonly collapse data across trials into variables such as proportion time spent looking at a region of interest (ROI). In addition, looking times in different ROIs are typically analyzed separately. We propose a novel multivariate functional outcome that carries proportion looking time information from multiple regions of interest jointly as a function of trial type, along with a novel constrained multivariate functional principal components analysis procedure to capture the variation in this outcome. The method incorporates the natural constraint that the proportion looking times from the multiple regions of interest must sum up to one. Our approach is motivated by the Activity Monitoring task, a social-attentional assay within the ET battery of the Autism Biomarkers Consortium for Clinical Trials (ABC-CT). Application of our methods to the ABC-CT data yields new insights into dominant modes of variation of proportion looking times from multiple regions of interest for school-age children with ASD and their typically developing (TD) peers, as well as richer analysis of diagnostic group differences in social attention.},
	language = {en},
	number = {3},
	urldate = {2025-02-10},
	journal = {Statistics in Biosciences},
	author = {Kwan, Brian and Sugar, Catherine A. and Qian, Qi and Shic, Frederick and Naples, Adam and Johnson, Scott P. and Webb, Sara J. and Jeste, Shafali and Faja, Susan and Levin, April R. and Dawson, Geraldine and McPartland, James C. and Şentürk, Damla},
	month = dec,
	year = {2024},
	keywords = {Eye tracking, Autism spectrum disorder, Functional data analysis, Functional principal components analysis, Multivariate functional principal component analysis},
	pages = {578--603},
	file = {2024_Kwan et al._Constrained Multivariate Functional Principal Components Analysis for Novel Outcomes in Eye-Tracking.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Kwan et al._Constrained Multivariate Functional Principal Components Analysis for Novel Outcomes in Eye-Tracking.pdf:application/pdf},
}

@inproceedings{aikat2024,
	address = {New York, NY, USA},
	series = {{UIST} {Adjunct} '24},
	title = {Digital {Phenotyping} based on a {Mobile} {App} {Identifies} {Distinct} and {Overlapping} {Features} in {Children} {Diagnosed} with {Autism} versus {ADHD}},
	isbn = {979-8-4007-0718-6},
	url = {https://dl.acm.org/doi/10.1145/3672539.3686323},
	doi = {10.1145/3672539.3686323},
	abstract = {The high prevalence of autism calls for accessible and scalable technology-assisted screening tools. This will aid in early detection allowing timely access to services and supports. SenseToKnow, a mobile digital phenotyping app, showed potential in eliciting autism-related behaviors that can be automatically captured via computer vision analysis (CVA) in toddlers. Here, we present the capability of SenseToKnow in characterizing autism in school age children and showcase the robustness of the CVA features in interpreting distinct and overlapping behaviors with attention-deficit/hyperactive disorder (ADHD).},
	urldate = {2025-02-10},
	booktitle = {Adjunct {Proceedings} of the 37th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Aikat, Vikram and Krishnappa Babu, Pradeep Raj and Carpenter, Kimberly L.H. and Di Martino, J. Matias and Espinosa, Steven and Davis, Naomi and Franz, Lauren and Spanos, Marina and Dawson, Geraldine and Sapiro, Guillermo},
	month = oct,
	year = {2024},
	pages = {1--4},
	file = {2024_Aikat et al._Digital Phenotyping based on a Mobile App Identifies Distinct and Overlapping Features in Children D.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Aikat et al._Digital Phenotyping based on a Mobile App Identifies Distinct and Overlapping Features in Children D.pdf:application/pdf},
}

@article{bey2024,
	title = {Automated {Video} {Tracking} of {Autistic} {Children}’s {Movement} {During} {Caregiver}-{Child} {Interaction}: {An} {Exploratory} {Study}},
	volume = {54},
	issn = {1573-3432},
	shorttitle = {Automated {Video} {Tracking} of {Autistic} {Children}’s {Movement} {During} {Caregiver}-{Child} {Interaction}},
	url = {https://doi.org/10.1007/s10803-023-06107-2},
	doi = {10.1007/s10803-023-06107-2},
	abstract = {Objective, quantitative measures of caregiver-child interaction during play are needed to complement caregiver or examiner ratings for clinical assessment and tracking intervention responses. In this exploratory study, we examined the feasibility of using automated video tracking, Noldus EthoVision XT, to measure 159 2-to-7-year-old autistic children’s patterns of movement during play-based, caregiver-child interactions and examined their associations with standard clinical measures and human observational coding of caregiver-child joint engagement. Results revealed that autistic children who exhibited higher durations and velocity of movement were, on average, younger, had lower cognitive abilities, greater autism-related features, spent less time attending to the caregiver, and showed lower levels of joint engagement. After adjusting for age and nonverbal cognitive abilities, we found that children who remained in close proximity to their caregiver were more likely to engage in joint engagement that required support from the caregiver. These findings suggest that video tracking offers promise as a scalable, quantitative, and relevant measure of autism-related behaviors.},
	language = {en},
	number = {10},
	urldate = {2025-02-10},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Bey, Alexandra L. and Sabatos-DeVito, Maura and Carpenter, Kimberly L.H. and Franz, Lauren and Howard, Jill and Vermeer, Saritha and Simmons, Ryan and Troy, Jesse D. and Dawson, Geraldine},
	month = oct,
	year = {2024},
	keywords = {Autism, Neurodevelopmental Disorders, Caregiver-child, Coding, Dyadic interaction, Free play, Movement},
	pages = {3706--3718},
	file = {2024_Bey et al._Automated Video Tracking of Autistic Children’s Movement During Caregiver-Child Interaction An Expl.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Bey et al._Automated Video Tracking of Autistic Children’s Movement During Caregiver-Child Interaction An Expl.pdf:application/pdf},
}

@article{azu2024,
	title = {Clinician–caregiver informant discrepancy is associated with sex, diagnosis age, and intervention use among autistic children},
	issn = {1362-3613},
	url = {https://doi.org/10.1177/13623613241279999},
	doi = {10.1177/13623613241279999},
	abstract = {Clinician and caregiver reports of autism features are both integral to receiving an autism diagnosis and appropriate intervention, yet informant discrepancies are present in clinical practice and may differ by demographic characteristics of the child and family. The present study examined how clinician–caregiver discrepancies in ratings of a child’s autism-related behaviors relate to a child’s sex at birth, age at first diagnosis, and amount of intervention received. Participants were 280 children (76.8\% male, 67.9\% White), 6–11 years old (M = 8.5 ± 1.6), with a diagnosis of autism spectrum disorder. Variable-centered and person-centered approaches were used to examine relationships between standardized clinician–caregiver discrepancy and participant characteristics. Both analytic approaches indicated that clinicians rated autism-related behaviors lower than caregivers for females and higher than caregivers for males. In addition, lower clinician ratings of autism features, relative to caregiver ratings, were associated with older age at diagnosis and fewer hours of intervention. Findings underscore the importance of incorporating multiple informants, especially caregivers, in the diagnostic process and developing diagnostic procedures sensitive to the female autism phenotype to facilitate diagnosis, intervention, and subsequent development.
Lay abstract
In some cases, a clinician’s perceptions of a child’s autism-related behaviors are not the same as the child’s caregiver’s perceptions. Identifying how these discrepancies relate to the characteristics of the child is critical for ensuring that diagnosis procedures are unbiased and suitable for all children. This study examined whether discrepancies between clinician and caregiver reports of autism features related to the child’s sex at birth. We also explored how the discrepancies related to the age at which the child received their autism diagnosis and how much intervention they received. We found that clinicians rated autism features higher than caregivers for boys and rated autism features lower than caregivers for girls. In addition, lower clinician relative to parent ratings was related to being diagnosed at an older age and receiving less intervention. These findings suggest that there is more to learn about the presentation of autism-related behaviors in girls. When caregiver and clinician ratings of autism features do not align, it may be important to consider caregivers’ ratings to obtain a more accurate picture of the child’s autism features and the support they may need.},
	language = {en},
	urldate = {2025-02-10},
	journal = {Autism},
	author = {Azu, Margaret A. and Han, Gloria T. and Wolf, Julie M. and Naples, Adam J. and Chawarska, Katarzyna and Dawson, Geraldine and Bernier, Raphael A. and Jeste, Shafali S. and Dziura, James D. and Webb, Sara J. and Sugar, Catherine A. and Shic, Frederick and McPartland, James C.},
	month = sep,
	year = {2024},
	note = {Publisher: SAGE Publications Ltd},
	pages = {13623613241279999},
	file = {2024_Azu et al._Clinician–caregiver informant discrepancy is associated with sex, diagnosis age, and intervention us.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Azu et al._Clinician–caregiver informant discrepancy is associated with sex, diagnosis age, and intervention us.pdf:application/pdf},
}

@article{isaev2024,
	title = {Computer {Vision} {Analysis} of {Caregiver}–{Child} {Interactions} in {Children} with {Neurodevelopmental} {Disorders}: {A} {Preliminary} {Report}},
	volume = {54},
	issn = {1573-3432},
	shorttitle = {Computer {Vision} {Analysis} of {Caregiver}–{Child} {Interactions} in {Children} with {Neurodevelopmental} {Disorders}},
	url = {https://doi.org/10.1007/s10803-023-05973-0},
	doi = {10.1007/s10803-023-05973-0},
	abstract = {We report preliminary results of computer vision analysis of caregiver–child interactions during free play with children diagnosed with autism (N = 29, 41–91 months), attention-deficit/hyperactivity disorder (ADHD, N = 22, 48–100 months), or combined autism + ADHD (N = 20, 56–98 months), and neurotypical children (NT, N = 7, 55–95 months). We conducted micro-analytic analysis of ‘reaching to a toy,’ as a proxy for initiating or responding to a toy play bout. Dyadic analysis revealed two clusters of interaction patterns, which differed in frequency of ‘reaching to a toy’ and caregivers’ contingent responding to the child’s reach for a toy by also reaching for a toy. Children in dyads with higher caregiver responsiveness had less developed language, communication, and socialization skills. Clusters were not associated with diagnostic groups. These results hold promise for automated methods of characterizing caregiver responsiveness in dyadic interactions for assessment and outcome monitoring in clinical trials.},
	language = {en},
	number = {6},
	urldate = {2025-02-10},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Isaev, Dmitry Yu. and Sabatos-DeVito, Maura and Di Martino, J. Matias and Carpenter, Kimberly and Aiello, Rachel and Compton, Scott and Davis, Naomi and Franz, Lauren and Sullivan, Connor and Dawson, Geraldine and Sapiro, Guillermo},
	month = jun,
	year = {2024},
	keywords = {Computer vision, Autism, ADHD, Neurodevelopmental Disorders, Caregiver–child interaction, Dyadic data analysis, Micro-analytic coding},
	pages = {2286--2297},
	file = {2024_Isaev et al._Computer Vision Analysis of Caregiver–Child Interactions in Children with Neurodevelopmental Disorde.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Isaev et al._Computer Vision Analysis of Caregiver–Child Interactions in Children with Neurodevelopmental Disorde.pdf:application/pdf},
}

@inproceedings{krishnappababu2024,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '24},
	title = {Large-scale {Validation} of a {Scalable} and {Portable} {Behavioral} {Digital} {Screening} {Tool} for {Autism} at {Home}},
	isbn = {979-8-4007-0331-7},
	url = {https://dl.acm.org/doi/10.1145/3613905.3650995},
	doi = {10.1145/3613905.3650995},
	abstract = {Autism, characterized by challenges in socialization and communication, benefits from early detection for prompt and timely intervention. Traditional autism screening questionnaires often exhibit reduced accuracy in primary care settings and significantly underperform underprivileged populations. We present findings on the effectiveness of an autism screening digital application (app) that can be administered at primary care clinics and also by caregivers at home. A large-scale validation was conducted with 1052 toddlers aged 16–40 months. Among them, 223 were subsequently diagnosed with autism. The age-appropriate interactive app utilized strategically designed stimuli, presented on the screen of the iPhone or iPad, to evoke behaviors related to social attention, facial expressions, head movements, blinking rate, and motor responses, which can be detected with the device's sensors and automatically quantified through computer vision (CV) and machine learning. The algorithm, combining various digital biomarkers, demonstrated strong accuracy: Area under the receiver operating characteristic curve (AUC) = 0.93, sensitivity = 86.0\%, specificity = 91.0\%, and precision = 71\%, for distinguishing autistic versus non-autistic toddlers, marking a strong foundation as a digital phenotyping tool in the autism research, notably without any costly equipment like eye tracking devices and at home administered by caregivers.},
	urldate = {2025-02-10},
	booktitle = {Extended {Abstracts} of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Krishnappa Babu, Pradeep Raj and Di Martino, J. Matias and Carpenter, Kimberly L.H. and Compton, Scott and Davis, Naomi and Eichner, Brian and Espinosa, Steven and Franz, Lauren and Perochon, Sam and Dawson, Geraldine and Sapiro, Guillermo},
	month = may,
	year = {2024},
	pages = {1--7},
	file = {2024_Krishnappa Babu et al._Large-scale Validation of a Scalable and Portable Behavioral Digital Screening Tool for Autism at Ho.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Krishnappa Babu et al._Large-scale Validation of a Scalable and Portable Behavioral Digital Screening Tool for Autism at Ho.pdf:application/pdf},
}

@article{chetcuti2024,
	title = {Feasibility of a 2-minute eye-tracking protocol to support the early identification of autism},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-55643-z},
	doi = {10.1038/s41598-024-55643-z},
	abstract = {We tested the potential for Gazefinder eye-tracking to support early autism identification, including feasible use with infants, and preliminary concurrent validity of trial-level gaze data against clinical assessment scores. We embedded the {\textasciitilde} 2-min ‘Scene 1S4’ protocol within a comprehensive clinical assessment for 54 consecutively-referred, clinically-indicated infants (prematurity-corrected age 9–14 months). Alongside \% tracking rate as a broad indicator of feasible assessment/data capture, we report infant gaze data to pre-specified regions of interest (ROI) across four trial types and associations with scores on established clinical/behavioural tools. Most infants tolerated Gazefinder eye-tracking well, returning high overall \% tracking rate. As a group, infants directed more gaze towards social vs. non-social (or more vs. less socially-salient) ROIs within trials. Behavioural autism features were correlated with increased gaze towards non-social/geometry (vs. social/people) scenes. No associations were found for gaze directed to ROIs within other stimulus types. Notably, there were no associations between developmental/cognitive ability or adaptive behaviour with gaze towards any ROI. Gazefinder assessment seems highly feasible with clinically-indicated infants, and the people vs. geometry stimuli show concurrent predictive validity for behavioural autism features. Aggregating data across the {\textasciitilde} 2-min autism identification protocol might plausibly offer greater utility than stimulus-level analysis alone.},
	language = {en},
	number = {1},
	urldate = {2025-02-10},
	journal = {Scientific Reports},
	author = {Chetcuti, Lacey and Varcin, Kandice J. and Boutrus, Maryam and Smith, Jodie and Bent, Catherine A. and Whitehouse, Andrew J. O. and Hudry, Kristelle},
	month = mar,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Disability, Risk factors},
	pages = {5117},
	file = {2024_Chetcuti et al._Feasibility of a 2-minute eye-tracking protocol to support the early identification of autism.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Eye-Tracking\\2024_Chetcuti et al._Feasibility of a 2-minute eye-tracking protocol to support the early identification of autism.pdf:application/pdf},
}

@article{zaharia2024,
	title = {Examining the {Link} {Between} {Social} {Affect} and {Visual} {Exploration} of {Cute} {Stimuli} in {Autistic} {Children}},
	issn = {1573-3432},
	url = {https://doi.org/10.1007/s10803-024-06504-1},
	doi = {10.1007/s10803-024-06504-1},
	abstract = {Baby schema refers to physical features perceived as cute, known to trigger attention, induce positive emotions, and prompt social interactions. Given the reduced visual attention to social stimuli observed in individuals on the autism spectrum, the current study examines whether the sensitivity to baby schema is also affected. We expected that the looking time towards cute-featured stimuli would vary with symptom severity levels and would be associated with social affect. Ninety-four children (31 typically developing; 63 diagnosed with autism spectrum disorder - ASD) aged 20–83 months (M = 49.63, SD = 13.59) completed an eye-tracking visual exploration task. Autistic participants were separated into two groups based on symptom severity: children with high autism severity symptoms (HS ASD; N = 23) and low-moderate autism symptoms (LMS ASD; N = 40). Animals and neutral objects were simultaneously presented on the screen along with either human babies (condition 1) or adults (condition 2). The results indicated that visual attention oriented to cute-featured stimuli varied with autism symptom severity: only LMS and TD groups spend more time looking at cute-featured stimuli (babies; animals) than neutral objects. Moreover, children with higher severity in the social affect domain spent less time on the stimuli depicting cute than non-cute stimuli. These findings suggest that autism symptom severity and social skills are linked to variations in visual attention to cute stimuli. Implications of baby schema sensitivity are discussed in relation to the development of social competencies and play, responsiveness to robot-based interventions, as well as appraised relevance in autistic children.},
	language = {en},
	urldate = {2025-02-11},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Zaharia, Alexandra and Kojovic, Nada and Rojanawisut, Tara and Sander, David and Schaer, Marie and Samson, Andrea C.},
	month = aug,
	year = {2024},
	keywords = {Eye-tracking, Autism spectrum disorder, Neurodevelopmental Disorders, Baby schema, Cuteness, Social affect},
	file = {2024_Zaharia et al._Examining the Link Between Social Affect and Visual Exploration of Cute Stimuli in Autistic Children.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Zaharia et al._Examining the Link Between Social Affect and Visual Exploration of Cute Stimuli in Autistic Children.pdf:application/pdf},
}

@article{journal2024,
	title = {Phenotyping variability in early socio-communicative skills in young children with autism and its influence on later development},
	volume = {17},
	copyright = {© 2024 The Author(s). Autism Research published by International Society for Autism Research and Wiley Periodicals LLC.},
	issn = {1939-3806},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aur.3188},
	doi = {10.1002/aur.3188},
	abstract = {Children with autism spectrum disorder (ASD) often face challenges in early social communication skills, prompting the need for a detailed exploration of specific behaviors and their impact on cognitive and adaptive functioning. This study aims to address this gap by examining the developmental trajectories of early social communication skills in preschoolers with ASD aged 18–60 months, comparing them to age-matched typically developing (TD) children. Utilizing the early social communication scales (ESCS), the research employs a longitudinal design to capture changes over time. We apply a principal component analysis (PCA) to ESCS variables to identify underlying components, and cluster analysis to identify subgroups based on preverbal communication profiles. The results reveal consistent differences in early social communication skills between ASD and TD children, with ASD children exhibiting reduced skills. PCA identifies two components, distinguishing objects-directed behaviors and social interaction-directed behaviors. Cluster analysis identifies three subgroups of autistic children, each displaying specific communication profiles associated with distinct cognitive and adaptive functioning trajectories. In conclusion, this study provides a nuanced understanding of early social communication development in ASD, emphasizing the importance of low-level behaviors. The identification of subgroups and their unique trajectories contributes to a more comprehensive understanding of ASD heterogeneity. These findings underscore the significance of early diagnosis, focusing on specific behaviors predicting cognitive and adaptive functioning outcomes. The study encourages further research to explore the sequential development of these skills, offering valuable insights for interventions and support strategies.},
	language = {en},
	number = {10},
	urldate = {2025-02-11},
	journal = {Autism Research},
	author = {Journal, Fiona and Franchini, Martina and Godel, Michel and Kojovic, Nada and Latrèche, Kenza and Solazzo, Stefania and Schneider, Maude and Schaer, Marie},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/aur.3188},
	keywords = {autism spectrum disorders, joint attention, cluster analysis, deep phenotyping, nonverbal communication, phenotype, trajectories},
	pages = {2030--2044},
	file = {2024_Journal et al._Phenotyping variability in early socio-communicative skills in young children with autism and its in.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Schaer\\2024_Journal et al._Phenotyping variability in early socio-communicative skills in young children with autism and its in.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\LWBS6HFM\\aur.html:text/html},
}

@article{kojovic2024,
	title = {Unraveling the developmental dynamic of visual exploration of social interactions in autism},
	volume = {13},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.85623},
	doi = {10.7554/eLife.85623},
	abstract = {Atypical deployment of social gaze is present early on in toddlers with autism spectrum disorders (ASDs). Yet, studies characterizing the developmental dynamic behind it are scarce. Here, we used a data-driven method to delineate the developmental change in visual exploration of social interaction over childhood years in autism. Longitudinal eye-tracking data were acquired as children with ASD and their typically developing (TD) peers freely explored a short cartoon movie. We found divergent moment-to-moment gaze patterns in children with ASD compared to their TD peers. This divergence was particularly evident in sequences that displayed social interactions between characters and even more so in children with lower developmental and functional levels. The basic visual properties of the animated scene did not account for the enhanced divergence. Over childhood years, these differences dramatically increased to become more idiosyncratic. These findings suggest that social attention should be targeted early in clinical treatments.},
	urldate = {2025-02-11},
	journal = {eLife},
	author = {Kojovic, Nada and Cekic, Sezen and Castañón, Santiago Herce and Franchini, Martina and Sperdin, Holger Franz and Sandini, Corrado and Jan, Reem Kais and Zöller, Daniela and Ben Hadid, Lylia and Bavelier, Daphné and Schaer, Marie},
	editor = {Büchel, Christian and Adolphs, Ralph},
	month = jan,
	year = {2024},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {eye-tracking, autism spectrum disorders, developmental trajectories, divergence, longitudinal, prediction of symptomatology},
	pages = {e85623},
	file = {2024_Kojovic et al._Unraveling the developmental dynamic of visual exploration of social interactions in autism.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Schaer\\2024_Kojovic et al._Unraveling the developmental dynamic of visual exploration of social interactions in autism.pdf:application/pdf},
}

@misc{farkhondeh2024,
	title = {{ChildPlay}-{Hand}: {A} {Dataset} of {Hand} {Manipulations} in the {Wild}},
	shorttitle = {{ChildPlay}-{Hand}},
	url = {http://arxiv.org/abs/2409.09319},
	doi = {10.48550/arXiv.2409.09319},
	abstract = {Hand-Object Interaction (HOI) is gaining significant attention, particularly with the creation of numerous egocentric datasets driven by AR/VR applications. However, third-person view HOI has received less attention, especially in terms of datasets. Most third-person view datasets are curated for action recognition tasks and feature pre-segmented clips of high-level daily activities, leaving a gap for in-the-wild datasets. To address this gap, we propose ChildPlay-Hand, a novel dataset that includes person and object bounding boxes, as well as manipulation actions. ChildPlay-Hand is unique in: (1) providing per-hand annotations; (2) featuring videos in uncontrolled settings with natural interactions, involving both adults and children; (3) including gaze labels from the ChildPlay-Gaze dataset for joint modeling of manipulations and gaze. The manipulation actions cover the main stages of an HOI cycle, such as grasping, holding or operating, and different types of releasing. To illustrate the interest of the dataset, we study two tasks: object in hand detection (OiH), i.e. if a person has an object in their hand, and manipulation stages (ManiS), which is more fine-grained and targets the main stages of manipulation. We benchmark various spatio-temporal and segmentation networks, exploring body vs. hand-region information and comparing pose and RGB modalities. Our findings suggest that ChildPlay-Hand is a challenging new benchmark for modeling HOI in the wild.},
	urldate = {2025-02-11},
	publisher = {arXiv},
	author = {Farkhondeh, Arya and Tafasca, Samy and Odobez, Jean-Marc},
	month = sep,
	year = {2024},
	note = {arXiv:2409.09319 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {2024_Farkhondeh et al._ChildPlay-Hand A Dataset of Hand Manipulations in the Wild.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\IDIAP\\2024_Farkhondeh et al._ChildPlay-Hand A Dataset of Hand Manipulations in the Wild.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\9VG388WB\\2409.html:text/html},
}

@inproceedings{gupta2024a,
	title = {A {Unified} {Model} for {Gaze} {Following} and {Social} {Gaze} {Prediction}},
	url = {https://ieeexplore.ieee.org/abstract/document/10581955},
	doi = {10.1109/FG59268.2024.10581955},
	abstract = {Human gaze plays a crucial role in communication and social interaction. Many recent studies have focused on predicting the 2D pixel location of a person's gaze target in an image. However, this approach has limitations when it comes to studying gaze for downstream applications that require analysis of higher-level social gaze behaviors. Previous works have post-processed the predicted 2D gaze target for social gaze prediction, however, we show that this approach is insufficient. Our proposed method jointly predicts the gaze target and social gaze behaviour, explicitly incorporating people interaction for state of the art results on three social gaze tasks - looking at heads, mutual gaze and shared attention. Additionally, we introduce evaluation protocols for these tasks, presenting a promising avenue for future research in gaze behavior analysis.},
	urldate = {2025-02-11},
	booktitle = {2024 {IEEE} 18th {International} {Conference} on {Automatic} {Face} and {Gesture} {Recognition} ({FG})},
	author = {Gupta, Anshul and Tafasca, Samy and Chutisilp, Naravich and Odobez, Jean-Marc},
	month = may,
	year = {2024},
	note = {ISSN: 2770-8330},
	keywords = {Face recognition, Training, Protocols, Focusing, Gesture recognition, Head, Semantics},
	pages = {1--9},
	file = {2024_Gupta et al._A Unified Model for Gaze Following and Social Gaze Prediction.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\IDIAP\\2024_Gupta et al._A Unified Model for Gaze Following and Social Gaze Prediction.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\4S2UMC9U\\10581955.html:text/html},
}

@misc{gupta2024b,
	title = {Exploring the {Zero}-{Shot} {Capabilities} of {Vision}-{Language} {Models} for {Improving} {Gaze} {Following}},
	url = {http://arxiv.org/abs/2406.03907},
	doi = {10.48550/arXiv.2406.03907},
	abstract = {Contextual cues related to a person's pose and interactions with objects and other people in the scene can provide valuable information for gaze following. While existing methods have focused on dedicated cue extraction methods, in this work we investigate the zero-shot capabilities of Vision-Language Models (VLMs) for extracting a wide array of contextual cues to improve gaze following performance. We first evaluate various VLMs, prompting strategies, and in-context learning (ICL) techniques for zero-shot cue recognition performance. We then use these insights to extract contextual cues for gaze following, and investigate their impact when incorporated into a state of the art model for the task. Our analysis indicates that BLIP-2 is the overall top performing VLM and that ICL can improve performance. We also observe that VLMs are sensitive to the choice of the text prompt although ensembling over multiple text prompts can provide more robust performance. Additionally, we discover that using the entire image along with an ellipse drawn around the target person is the most effective strategy for visual prompting. For gaze following, incorporating the extracted cues results in better generalization performance, especially when considering a larger set of cues, highlighting the potential of this approach.},
	urldate = {2025-02-11},
	publisher = {arXiv},
	author = {Gupta, Anshul and Vuillecard, Pierre and Farkhondeh, Arya and Odobez, Jean-Marc},
	month = jun,
	year = {2024},
	note = {arXiv:2406.03907 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {2024_Gupta et al._Exploring the Zero-Shot Capabilities of Vision-Language Models for Improving Gaze Following.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\IDIAP\\2024_Gupta et al._Exploring the Zero-Shot Capabilities of Vision-Language Models for Improving Gaze Following.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\HDZWQZ3M\\2406.html:text/html},
}

@misc{tafasca2023,
	title = {{ChildPlay}: {A} {New} {Benchmark} for {Understanding} {Children}'s {Gaze} {Behaviour}},
	shorttitle = {{ChildPlay}},
	url = {http://arxiv.org/abs/2307.01630},
	doi = {10.48550/arXiv.2307.01630},
	abstract = {Gaze behaviors such as eye-contact or shared attention are important markers for diagnosing developmental disorders in children. While previous studies have looked at some of these elements, the analysis is usually performed on private datasets and is restricted to lab settings. Furthermore, all publicly available gaze target prediction benchmarks mostly contain instances of adults, which makes models trained on them less applicable to scenarios with young children. In this paper, we propose the first study for predicting the gaze target of children and interacting adults. To this end, we introduce the ChildPlay dataset: a curated collection of short video clips featuring children playing and interacting with adults in uncontrolled environments (e.g. kindergarten, therapy centers, preschools etc.), which we annotate with rich gaze information. We further propose a new model for gaze target prediction that is geometrically grounded by explicitly identifying the scene parts in the 3D field of view (3DFoV) of the person, leveraging recent geometry preserving depth inference methods. Our model achieves state of the art results on benchmark datasets and ChildPlay. Furthermore, results show that looking at faces prediction performance on children is much worse than on adults, and can be significantly improved by fine-tuning models using child gaze annotations. Our dataset and models will be made publicly available.},
	urldate = {2025-02-11},
	publisher = {arXiv},
	author = {Tafasca, Samy and Gupta, Anshul and Odobez, Jean-Marc},
	month = jul,
	year = {2023},
	note = {arXiv:2307.01630 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {2023_Tafasca et al._ChildPlay A New Benchmark for Understanding Children's Gaze Behaviour.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\IDIAP\\2023_Tafasca et al._ChildPlay A New Benchmark for Understanding Children's Gaze Behaviour.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\CZ458CWM\\2307.html:text/html},
}

@inproceedings{tafasca2023a,
	address = {New York, NY, USA},
	series = {{ICMI} '23 {Companion}},
	title = {The {AI4Autism} {Project}: {A} {Multimodal} and {Interdisciplinary} {Approach} to {Autism} {Diagnosis} and {Stratification}},
	isbn = {979-8-4007-0321-8},
	shorttitle = {The {AI4Autism} {Project}},
	url = {https://dl.acm.org/doi/10.1145/3610661.3616239},
	doi = {10.1145/3610661.3616239},
	abstract = {Nowadays, 1 in 36 children is diagnosed with autism spectrum disorder (ASD) according to the Centers for Disease Control and Prevention (CDC) [52], which makes this condition one of the most prevalent neurodevelopmental disorders. For children on the autism spectrum who face substantial developmental delays, the trajectory of their cognitive growth can be markedly improved by interventions if the condition is identified early. Therefore, there is a critical need for more scalable screening and diagnostic tools, as well as the need to improve phenotyping to refine estimates of ASD symptoms in children. Here, we introduce AI4Autism: a 4-year project funded by the Swiss National Science Foundation, which aims to address the needs outlined above. In this project, we examine the potential of digital sensing to provide automated measures of the extended autism phenotype. This is accomplished using multimodal techniques based on computer vision and Internet of Things sensing, for the purpose of stratifying autism subtypes in ways that would allow for precision medicine. We present an overview of our main results so far, introducing datasets and annotations that we intend to make publicly available, as well as methods and algorithms for analyzing children’s behaviors and producing an ASD diagnosis.},
	urldate = {2025-02-11},
	booktitle = {Companion {Publication} of the 25th {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Tafasca, Samy and Gupta, Anshul and Kojovic, Nada and Gelsomini, Mirko and Maillart, Thomas and Papandrea, Michela and Schaer, Marie and Odobez, Jean-Marc},
	month = oct,
	year = {2023},
	pages = {414--425},
	file = {2023_Tafasca et al._The AI4Autism Project A Multimodal and Interdisciplinary Approach to Autism Diagnosis and Stratific.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Schaer\\2023_Tafasca et al._The AI4Autism Project A Multimodal and Interdisciplinary Approach to Autism Diagnosis and Stratific.pdf:application/pdf},
}

@article{keehn2024,
	title = {Eye-{Tracking} {Biomarkers} and {Autism} {Diagnosis} in {Primary} {Care}},
	volume = {7},
	issn = {2574-3805},
	url = {https://doi.org/10.1001/jamanetworkopen.2024.11190},
	doi = {10.1001/jamanetworkopen.2024.11190},
	abstract = {Finding effective and scalable solutions to address diagnostic delays and disparities in autism is a public health imperative. Approaches that integrate eye-tracking biomarkers into tiered community-based models of autism evaluation hold promise for addressing this problem.To determine whether a battery of eye-tracking biomarkers can reliably differentiate young children with and without autism in a community-referred sample collected during clinical evaluation in the primary care setting and to evaluate whether combining eye-tracking biomarkers with primary care practitioner (PCP) diagnosis and diagnostic certainty is associated with diagnostic outcome.Early Autism Evaluation (EAE) Hub system PCPs referred a consecutive sample of children to this prospective diagnostic study for blinded eye-tracking index test and follow-up expert evaluation from June 7, 2019, to September 23, 2022. Participants included 146 children (aged 14-48 months) consecutively referred by 7 EAE Hubs. Of 154 children enrolled, 146 provided usable data for at least 1 eye-tracking measure.The primary outcomes were sensitivity and specificity of a composite eye-tracking (ie, index) test, which was a consolidated measure based on significant eye-tracking indices, compared with reference standard expert clinical autism diagnosis. Secondary outcome measures were sensitivity and specificity of an integrated approach using an index test and PCP diagnosis and certainty.Among 146 children (mean [SD] age, 2.6 [0.6] years; 104 [71\%] male; 21 [14\%] Hispanic or Latine and 96 [66\%] non-Latine White; 102 [70\%] with a reference standard autism diagnosis), 113 (77\%) had concordant autism outcomes between the index (composite biomarker) and reference outcomes, with 77.5\% sensitivity (95\% CI, 68.4\%-84.5\%) and 77.3\% specificity (95\% CI, 63.0\%-87.2\%). When index diagnosis was based on the combination of a composite biomarker, PCP diagnosis, and diagnostic certainty, outcomes were concordant with reference standard for 114 of 127 cases (90\%) with a sensitivity of 90.7\% (95\% CI, 83.3\%-95.0\%) and a specificity of 86.7\% (95\% CI, 70.3\%-94.7\%).In this prospective diagnostic study, a composite eye-tracking biomarker was associated with a best-estimate clinical diagnosis of autism, and an integrated diagnostic model including PCP diagnosis and diagnostic certainty demonstrated improved sensitivity and specificity. These findings suggest that equipping PCPs with a multimethod diagnostic approach has the potential to substantially improve access to timely, accurate diagnosis in local communities.},
	number = {5},
	urldate = {2025-02-13},
	journal = {JAMA Network Open},
	author = {Keehn, Brandon and Monahan, Patrick and Enneking, Brett and Ryan, Tybytha and Swigonski, Nancy and McNally Keehn, Rebecca},
	month = may,
	year = {2024},
	keywords = {ASD, EyeTracking},
	pages = {e2411190},
	file = {2024_Keehn et al._Eye-Tracking Biomarkers and Autism Diagnosis in Primary Care.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\autism\\2024_Keehn et al._Eye-Tracking Biomarkers and Autism Diagnosis in Primary Care.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\5Y3G7N75\\2818669.html:text/html},
}

@article{hull2020,
	title = {The {Female} {Autism} {Phenotype} and {Camouflaging}: a {Narrative} {Review}},
	volume = {7},
	issn = {2195-7185},
	shorttitle = {The {Female} {Autism} {Phenotype} and {Camouflaging}},
	url = {https://doi.org/10.1007/s40489-020-00197-9},
	doi = {10.1007/s40489-020-00197-9},
	abstract = {Autism is more commonly diagnosed in males than females. One explanation is the ‘female protective effect’: there is something inherent in being female which reduces the likelihood of developing autism. However, evidence suggests that the condition is underdiagnosed in females, perhaps because females express their autism in ways which do not meet current diagnostic criteria. This review explores evidence for a female-typical autism presentation, the Female Autism Phenotype (FAP) and the component of camouflaging (compensating for and masking autistic characteristics) in particular. The evidence so far supports the existence of a female-typical autism presentation, although further examination of the characteristics and their impact across all genders and ages is needed.},
	language = {en},
	number = {4},
	urldate = {2025-02-13},
	journal = {Review Journal of Autism and Developmental Disorders},
	author = {Hull, Laura and Petrides, K. V. and Mandy, William},
	month = dec,
	year = {2020},
	keywords = {Autism, Female autism phenotype, Camouflaging, Gender, Sex, Female protective effect},
	pages = {306--317},
	file = {2020_Hull et al._The Female Autism Phenotype and Camouflaging a Narrative Review.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\Female\\2020_Hull et al._The Female Autism Phenotype and Camouflaging a Narrative Review.pdf:application/pdf},
}

@article{tubio-fungueirino2021,
	title = {Social {Camouflaging} in {Females} with {Autism} {Spectrum} {Disorder}: {A} {Systematic} {Review}},
	volume = {51},
	issn = {1573-3432},
	shorttitle = {Social {Camouflaging} in {Females} with {Autism} {Spectrum} {Disorder}},
	url = {https://doi.org/10.1007/s10803-020-04695-x},
	doi = {10.1007/s10803-020-04695-x},
	abstract = {Autism spectrum disorder (ASD) is a neurodevelopmental disorder with increasing prevalence, and a male-to-female ratio of 4:1. Research has been suggesting that discrepancy in prevalence may be due to the fact that females camouflage their symptoms. In this study, we aimed to systematically review evidence on the camouflage effect in females with ASD. Following the PRISMA guidelines, we reviewed empirical research published from January 2009 to September 2019 on PubMed, Web of Science, PsychInfo and Scopus databases. Thirteen empirical articles were included in this review. Overall, evidence supports that camouflaging seems to be an adaptive mechanism for females with ASD, despite the negative implications of these behaviours in their daily life.},
	language = {en},
	number = {7},
	urldate = {2025-02-13},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Tubío-Fungueiriño, María and Cruz, Sara and Sampaio, Adriana and Carracedo, Angel and Fernández-Prieto, Montse},
	month = jul,
	year = {2021},
	keywords = {Autism, Systematic review, Camouflage, Females},
	pages = {2190--2199},
	file = {2021_Tubío-Fungueiriño et al._Social Camouflaging in Females with Autism Spectrum Disorder A Systematic Review.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\Female\\2021_Tubío-Fungueiriño et al._Social Camouflaging in Females with Autism Spectrum Disorder A Systematic Review.pdf:application/pdf},
}

@article{altin2025,
	title = {Examining {Virtual} {Reality} {Interventions} for {Social} {Skills} in {Children} with {Autism} {Spectrum} {Disorder}: {A} {Systematic} {Review}},
	issn = {1573-3432},
	shorttitle = {Examining {Virtual} {Reality} {Interventions} for {Social} {Skills} in {Children} with {Autism} {Spectrum} {Disorder}},
	url = {https://doi.org/10.1007/s10803-025-06741-y},
	doi = {10.1007/s10803-025-06741-y},
	abstract = {Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder that is characterized by limitations in social communication and interaction, self-repetitive behaviors, and the presence of limited interests. The prevalence of ASD, which typically emerges in the first years of life, is increasing at an alarming rate due to multiple factors, including the broadening of diagnostic criteria, heightened public awareness, and more frequent diagnoses among women and adults. Over the years, experts have invested considerable time and effort in developing educational scenarios for children with ASD. However, they have faced challenges replicating certain scenarios—such as emergencies, crowded public transportation, or restaurant environments—because recreating these exact conditions in real-world settings is difficult or cost-prohibitive. This has consequently compelled experts to seek out supplementary intervention methods that are more suitable and accessible. Virtual reality (VR), which has the capacity to integrate the physical and virtual realms, represents one such alternative intervention method. In this study, a systematic review of studies employing VR technology in social skills interventions for individuals with ASD was conducted, and 31 studies were included. The findings indicate the potential benefits of VR applications focusing on the social skills of individuals with ASD. Additionally, this research elucidates the limitations of the studies and offers suggestions for future research.},
	language = {en},
	urldate = {2025-02-14},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Altın, Yücel and Boşnak, Özge and Turhan, Ceyda},
	month = feb,
	year = {2025},
	keywords = {Virtual reality, Autism spectrum disorder, Intervention, Social skills},
	file = {2025_Altın et al._Examining Virtual Reality Interventions for Social Skills in Children with Autism Spectrum Disorder.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\NeuroDiverse\\2025_Altın et al._Examining Virtual Reality Interventions for Social Skills in Children with Autism Spectrum Disorder.pdf:application/pdf},
}

@article{kim2024,
	title = {Promoting {Self}-{Efficacy} of {Individuals} {With} {Autism} in {Practicing} {Social} {Skills} in the {Workplace} {Using} {Virtual} {Reality} and {Physiological} {Sensors}: {Mixed} {Methods} {Study}},
	volume = {8},
	copyright = {Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work ("first published in the Journal of Medical Internet Research...") is properly cited with original URL and bibliographic citation information. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included.},
	shorttitle = {Promoting {Self}-{Efficacy} of {Individuals} {With} {Autism} in {Practicing} {Social} {Skills} in the {Workplace} {Using} {Virtual} {Reality} and {Physiological} {Sensors}},
	url = {https://formative.jmir.org/2024/1/e52157},
	doi = {10.2196/52157},
	abstract = {Background: Individuals with autism often experience heightened anxiety in workplace environments because of challenges in communication and sensory overload. As these experiences can result in negative self-image, promoting their self-efficacy in the workplace is crucial. Virtual reality (VR) systems have emerged as promising tools for enhancing the self-efficacy of individuals with autism in navigating social scenarios, aiding in the identification of anxiety-inducing situations, and preparing for real-world interactions. However, there is limited research exploring the potential of VR to enhance self-efficacy by facilitating an understanding of emotional and physiological states during social skills practice. Objective: This study aims to develop and evaluate a VR system that enabled users to experience simulated work-related social scenarios and reflect on their behavioral and physiological data through data visualizations. We intended to investigate how these data, combined with the simulations, can support individuals with autism in building their self-efficacy in social skills. Methods: We developed WorkplaceVR, a comprehensive VR system designed for engagement in simulated work-related social scenarios, supplemented with data-driven reflections of users’ behavioral and physiological responses. A within-subject deployment study was subsequently conducted with 14 young adults with autism to examine WorkplaceVR’s feasibility. A mixed methods approach was used, compassing pre- and postsystem use assessments of participants’ self-efficacy perceptions. Results: The study results revealed WorkplaceVR’s effectiveness in enhancing social skills and self-efficacy among individuals with autism. First, participants exhibited a statistically significant increase in perceived self-efficacy following their engagement with the VR system (P=.02). Second, thematic analysis of the interview data confirmed that the VR system and reflections on the data fostered increased self-awareness among participants about social situations that trigger their anxiety, as well as the behaviors they exhibit during anxious moments. This increased self-awareness prompted the participants to recollect their related experiences in the real world and articulate anxiety management strategies. Furthermore, the insights uncovered motivated participants to engage in self-advocacy, as they wanted to share the insights with others. Conclusions: This study highlights the potential of VR simulations enriched with physiological and behavioral sensing as a valuable tool for augmenting self-efficacy in workplace social interactions for individuals with autism. Data reflection facilitated by physiological sensors helped participants with autism become more self-aware of their emotions and behaviors, advocate for their characteristics, and develop positive self-beliefs.},
	language = {EN},
	number = {1},
	urldate = {2025-02-14},
	journal = {JMIR Formative Research},
	author = {Kim, Sung-In and Jang, So-youn and Kim, Taewan and Kim, Bogoan and Jeong, Dayoung and Noh, Taehyung and Jeong, Mingon and Hall, Kaely and Kim, Meelim and Yoo, Hee Jeong and Han, Kyungsik and Hong, Hwajung and Kim, Jennifer G.},
	month = jan,
	year = {2024},
	note = {Company: JMIR Formative Research
Distributor: JMIR Formative Research
Institution: JMIR Formative Research
Label: JMIR Formative Research
Publisher: JMIR Publications Inc., Toronto, Canada},
	pages = {e52157},
	file = {2024_Kim et al._Promoting Self-Efficacy of Individuals With Autism in Practicing Social Skills in the Workplace Usin.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\2024_Kim et al._Promoting Self-Efficacy of Individuals With Autism in Practicing Social Skills in the Workplace Usin.pdf:application/pdf},
}

@article{moon2024a,
	title = {Learning experience design of verbal prompts in virtual reality-based training for autistic children},
	volume = {32},
	copyright = {Copyright (c) 2024 Jewoong Moon},
	issn = {2156-7077},
	url = {https://journal.alt.ac.uk/index.php/rlt/article/view/3129},
	doi = {10.25304/rlt.v32.3129},
	abstract = {This study aimed to explore the design and development of verbal prompts in virtual reality (VR)-based social skills training for autistic children. Autism indicates a category with neurodiversity that influences individuals’ capability to engage in social and cognitive tasks. This complex neurodevelopmental condition manifests in a wide array of patterns, featuring unique experiences of each individual. This study explored both advantages and challenges encountered when autistic children interact with verbal prompts in multi-user, desktop VR-based social skills training. Our explanatory case study involved VR-based learning experiences of four autistic children. We used a qualitative thematic analysis to analyse the study participants’ interaction patterns with verbal prompts in the VR-based training. Our research can contribute to both theoretical knowledge and practical design guidelines for the creation of verbal prompts in desktop VR-based training programmes tailored for autistic children.},
	language = {en},
	urldate = {2025-02-14},
	journal = {Research in Learning Technology},
	author = {Moon, Jewoong},
	month = mar,
	year = {2024},
	keywords = {social skills, autism spectrum disorder, learning experience design, verbal prompts, virtual world},
	file = {2024_Moon_Learning experience design of verbal prompts in virtual reality-based training for autistic children.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\2024_Moon_Learning experience design of verbal prompts in virtual reality-based training for autistic children.pdf:application/pdf},
}

@article{soltiyeva2023,
	title = {My {Lovely} {Granny}’s {Farm}: {An} immersive virtual reality training system for children with autism spectrum disorder},
	volume = {28},
	issn = {1573-7608},
	shorttitle = {My {Lovely} {Granny}’s {Farm}},
	url = {https://doi.org/10.1007/s10639-023-11862-x},
	doi = {10.1007/s10639-023-11862-x},
	abstract = {One of the biggest difficulties faced by children with Autism Spectrum Disorder during their learning process and general life, is communication and social interaction. In recent years, researchers and practitioners have invested in different approaches to improving aspects of their communication and learning. However, there is still no consolidated approach and the community is still looking for new approaches that can meet this need. Addressing this challenge, in this article we propose a novelty approach (i.e., an Adaptive Immersive Virtual Reality Training System), aiming to enrich social interaction and communication skills for children with Autism Spectrum Disorder. In this adaptive system (called My Lovely Granny’s Farm), the behavior of the virtual trainer changes depending on the mood and actions of the users (i.e., patients/learners). Additionally, we conducted an initial observational study by monitoring the behavior of children with autism in a virtual environment. In the initial study, the system was offered to users with a high degree of interactivity so that they might practice various social situations in a safe and controlled environment. The results demonstrate that the use of the system can allow patients who needed treatment to receive therapy without leaving home. Our approach is the first experience of treating children with autism in Kazakhstan and can contribute to improving the communication and social interaction of children with Autism Spectrum Disorder. We contribute to the community of educational technologies and mental health by providing a system that can improve communication among children with autism and providing insights on how to design this kind of system.},
	language = {en},
	number = {12},
	urldate = {2025-02-14},
	journal = {Education and Information Technologies},
	author = {Soltiyeva, Aiganym and Oliveira, Wilk and Madina, Alimanova and Adilkhan, Shyngys and Urmanov, Marat and Hamari, Juho},
	month = dec,
	year = {2023},
	keywords = {Virtual reality, Autism spectrum disorder, Communicational skills, Digital Education and Educational Technology, Immersive systems, Social interaction},
	pages = {16887--16907},
	file = {2023_Soltiyeva et al._My Lovely Granny’s Farm An immersive virtual reality training system for children with autism spect.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\2023_Soltiyeva et al._My Lovely Granny’s Farm An immersive virtual reality training system for children with autism spect.pdf:application/pdf},
}

@article{moon2024b,
	title = {Effects of {Adaptive} {Prompts} in {Virtual} {Reality}-{Based} {Social} {Skills} {Training} for {Children} with {Autism}},
	volume = {54},
	issn = {1573-3432},
	url = {https://doi.org/10.1007/s10803-023-06021-7},
	doi = {10.1007/s10803-023-06021-7},
	abstract = {The purpose of this single-case experimental design (SCED) study is to investigate how adaptive prompts in virtual reality (VR)-based social skills training affect the social skills performance of autistic children. Adaptive prompts are driven by autistic children’s emotional states. To integrate adaptive prompts in VR-based training, we conducted speech data mining and endorsed micro-adaptivity design. We recruited four autistic children (12–13 years) for the SCED study. We carried out alternating treatments design to evaluate the impacts of adaptive and non-adaptive prompting conditions throughout a series of VR-based social skills training sessions. Using mixed-method data collection and analyses, we found that adaptive prompts can foster autistic children’s desirable social skills performance in VR-based training. Based on the study findings, we also describe design implications and limitations for future research.},
	language = {en},
	number = {8},
	urldate = {2025-02-14},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Moon, Jewoong and Ke, Fengfeng},
	month = aug,
	year = {2024},
	keywords = {Autism spectrum disorder, Adaptivity design, Educational data mining, Learning analytics, Verbal prompt, Virtual reality-based training},
	pages = {2826--2846},
	file = {2024_Moon and Ke_Effects of Adaptive Prompts in Virtual Reality-Based Social Skills Training for Children with Autism.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\2024_Moon and Ke_Effects of Adaptive Prompts in Virtual Reality-Based Social Skills Training for Children with Autism.pdf:application/pdf},
}

@article{kourtesis2023a,
	title = {Virtual {Reality} {Training} of {Social} {Skills} in {Adults} with {Autism} {Spectrum} {Disorder}: {An} {Examination} of {Acceptability}, {Usability}, {User} {Experience}, {Social} {Skills}, and {Executive} {Functions}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-328X},
	shorttitle = {Virtual {Reality} {Training} of {Social} {Skills} in {Adults} with {Autism} {Spectrum} {Disorder}},
	url = {https://www.mdpi.com/2076-328X/13/4/336},
	doi = {10.3390/bs13040336},
	abstract = {Poor social skills in autism spectrum disorder (ASD) are associated with reduced independence in daily life. Current interventions for improving the social skills of individuals with ASD fail to represent the complexity of real-life social settings and situations. Virtual reality (VR) may facilitate social skills training in social environments and situations similar to those in real life; however, more research is needed to elucidate aspects such as the acceptability, usability, and user experience of VR systems in ASD. Twenty-five participants with ASD attended a neuropsychological evaluation and three sessions of VR social skills training, which incorporated five social scenarios with three difficulty levels. Participants reported high acceptability, system usability, and user experience. Significant correlations were observed between performance in social scenarios, self-reports, and executive functions. Working memory and planning ability were significant predictors of the functionality level in ASD and the VR system’s perceived usability, respectively. Yet, performance in social scenarios was the best predictor of usability, acceptability, and functionality level. Planning ability substantially predicted performance in social scenarios, suggesting an implication in social skills. Immersive VR social skills training in individuals with ASD appears to be an appropriate service, but an errorless approach that is adaptive to the individual’s needs should be preferred.},
	language = {en},
	number = {4},
	urldate = {2025-02-14},
	journal = {Behavioral Sciences},
	author = {Kourtesis, Panagiotis and Kouklari, Evangelia-Chrysanthi and Roussos, Petros and Mantas, Vasileios and Papanikolaou, Katerina and Skaloumbakas, Christos and Pehlivanidis, Artemios},
	month = apr,
	year = {2023},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {virtual reality, training, autism, acceptability, executive functions, prompts, social cognition, social skills, usability, user experience},
	pages = {336},
	file = {2023_Kourtesis et al._Virtual Reality Training of Social Skills in Adults with Autism Spectrum Disorder An Examination of.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\2023_Kourtesis et al._Virtual Reality Training of Social Skills in Adults with Autism Spectrum Disorder An Examination of.pdf:application/pdf},
}

@article{manju2023,
	title = {Increasing the {Social} {Interaction} of {Autism} {Child} using {Virtual} {Reality} {Intervention} ({VRI})},
	issn = {2375-4699},
	url = {https://dl.acm.org/doi/10.1145/3592855},
	doi = {10.1145/3592855},
	abstract = {Nowadays there is an increase in number of autisms, a neuro-developmental disorder across the world. The level of autism varies with the symptoms such as inattention, interaction, social communication, repetitive behaviors, irritability and the like. Early recovery of a child from autism is necessary to live in a normal socio-communicable life. To measure the inattention of the autism child by enhancing the visual perception through virtual environment. The proposed Virtual Reality Intervention (VRI) enhances visual perception, learning, and social interaction. The proposed method observes the attention level of the autism child through eye tracking or eye movements who interacts with virtual world using eye tracking methodology. As eye tracking is the major component to measure the reduced looking time of objects and subjects which considered being the earliest signs of autism spectrum disorder (ASD). For observing the attention of kids during testing, Eye movements and gestures plays a major role. Using head position and eye pupil direction, attention has been analyzed. Quantitative and Qualitative findings conclude that the inattention of autism child can gradually be reduced by iterating the virtual therapy through eye ball tracking technique. Qualitative finding is done using Aberrant Behavior Checklist (ABC) and quantitative using eye-pupil and head position. Autism affected children can easily recovered from this inattention symptom by continuous iterations on virtual therapy. Similar virtual therapies can also be provided to address other symptoms of autism.},
	urldate = {2025-02-14},
	journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
	author = {Manju, T. and Magesh and Padmavathi, S. and Durairaj},
	month = apr,
	year = {2023},
	note = {Just Accepted},
	file = {2023_Manju et al._Increasing the Social Interaction of Autism Child using Virtual Reality Intervention (VRI).pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\2023_Manju et al._Increasing the Social Interaction of Autism Child using Virtual Reality Intervention (VRI).pdf:application/pdf},
}

@article{elkin2022,
	title = {Gaze {Fixation} and {Visual} {Searching} {Behaviors} during an {Immersive} {Virtual} {Reality} {Social} {Skills} {Training} {Experience} for {Children} and {Youth} with {Autism} {Spectrum} {Disorder}: {A} {Pilot} {Study}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3425},
	shorttitle = {Gaze {Fixation} and {Visual} {Searching} {Behaviors} during an {Immersive} {Virtual} {Reality} {Social} {Skills} {Training} {Experience} for {Children} and {Youth} with {Autism} {Spectrum} {Disorder}},
	url = {https://www.mdpi.com/2076-3425/12/11/1568},
	doi = {10.3390/brainsci12111568},
	abstract = {Children and youth with Autism Spectrum Disorder (ASD) display difficulties recognizing and interacting with behavioral expressions of emotion, a deficit that makes social interaction problematic. Social skills training is foundational to the treatment of ASD, yet this intervention is costly, time-consuming, lacks objectivity, and is difficult to deliver in real-world settings. This pilot project investigated the use of an immersive virtual reality (IVR) headset to simulate real-world social interactions for children/youth with ASD. The primary objective was to describe gaze fixation and visual search behaviors during the simulated activity. Ten participants were enrolled and completed one social-skills training session in the IVR. The results demonstrate differential patterns between participants with mild, moderate, and severe ASD in the location and duration of gaze fixation as well as the patterns of visual searching. Although the results are preliminary, these differences may shed light on phenotypes within the continuum of ASD. Additionally, there may be value in quantifying gaze and visual search behaviors as an objective metric of interventional effectiveness for social-skills training therapy.},
	language = {en},
	number = {11},
	urldate = {2025-02-14},
	journal = {Brain Sciences},
	author = {Elkin, Thomas David and Zhang, Yunxi and Reneker, Jennifer C.},
	month = nov,
	year = {2022},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {virtual reality, eye tracking, social skills, autism spectrum disorder},
	pages = {1568},
	file = {2022_Elkin et al._Gaze Fixation and Visual Searching Behaviors during an Immersive Virtual Reality Social Skills Train.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\2022_Elkin et al._Gaze Fixation and Visual Searching Behaviors during an Immersive Virtual Reality Social Skills Train.pdf:application/pdf},
}

@article{ip2022,
	title = {Enhance affective expression and social reciprocity for children with autism spectrum disorder: using virtual reality headsets at schools},
	volume = {32},
	issn = {1049-4820},
	shorttitle = {Enhance affective expression and social reciprocity for children with autism spectrum disorder},
	url = {https://doi.org/10.1080/10494820.2022.2107681},
	doi = {10.1080/10494820.2022.2107681},
	abstract = {Social-emotional deficits in school-aged children diagnosed with autism spectrum disorder (ASD) greatly hinder these children from fully participating in various school activities in the inclusive education setting. Previous studies have demonstrated evidence regarding the effectiveness of using virtual reality (VR) for enhancing the children’s affective expression and social reciprocity. However, considering the technical and logistical complexity of the enabling hardware and software systems, how such approaches can be effectively and sustainably delivered in the school setting remains underexplored. This paper presents a study that utilised VR headsets to enhance affective expression and social reciprocity for children with ASD and explored how the approach could be effectively and sustainably delivered at schools. A total of eight VR learning scenarios were designed based on Kolb’s experiential learning framework. 176 children aged 6–12 with a clinical diagnosis of ASD participated in the study. The statistical analyses showed that the participants who received the intervention significantly improved in affective expression and social reciprocity, compared to those who were in the control group. Moreover, the approaches to enhance long-term sustainability have also been presented and discussed in this paper.},
	number = {3},
	urldate = {2025-02-14},
	journal = {Interactive Learning Environments},
	author = {Ip, Horace H. S. and Wong, Simpson W. L. and Chan, Dorothy F. Y. and Li, Chen and Kon, Lo Lo and Ma, Po Ke and Lau, Kate S. Y. and Byrne, Julia},
	year = {2022},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/10494820.2022.2107681},
	keywords = {Virtual reality, autism spectrum disorder, affective expression, experiential learning, social reciprocity},
	pages = {1012--1035},
	file = {2024_Ip et al._Enhance affective expression and social reciprocity for children with autism spectrum disorder usin.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\2024_Ip et al._Enhance affective expression and social reciprocity for children with autism spectrum disorder usin.pdf:application/pdf},
}

@article{ke2022,
	title = {Designing and deploying a virtual social sandbox for autistic children},
	volume = {19},
	issn = {1748-3107},
	url = {https://doi.org/10.1080/17483107.2022.2156630},
	doi = {10.1080/17483107.2022.2156630},
	abstract = {This exploratory study was intended to investigate the design and feasibility of using a web virtual reality based social learning space for autistic children at home. The researchers of the current study developed and implemented an open-source, web virtual reality based learning program for children with autism. Endorsing mixed-method convergent parallel design, we collected both qualitative and quantitative data from four autistic children, including repeated measures of social skills performance, self- and parent-reported social and communication competence, observation notes, and individual interviews. The study found preliminary evidence for a positive impact of deploying a virtual reality-based social sandbox on the practice and development of complex social skills for autistic children. All participants showed significant reduced social communication impairments from the pre- to the post-intervention phases. Nevertheless, participants’ social skills performance in the virtual world was mediated by two social task design features—external goal structure and individualization. Play- and design-oriented social tasks in the three-dimensional virtual world framed meaningful social experiences or the naturalistic intervention for social skills development. Positive impacts of using a virtual reality-based social sandbox on complex social skills development for autistic children.Social task design features mediate social skills performance of autistic children.Purposeful environment arrangement creates a naturalistic intervention for autism. Positive impacts of using a virtual reality-based social sandbox on complex social skills development for autistic children. Social task design features mediate social skills performance of autistic children. Purposeful environment arrangement creates a naturalistic intervention for autism.},
	number = {4},
	urldate = {2025-02-14},
	journal = {Disability and Rehabilitation: Assistive Technology},
	author = {Ke, Fengfeng and Moon, Jewoong and Sokolikj, Zlatko},
	month = may,
	year = {2022},
	pmid = {36524469},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/17483107.2022.2156630},
	keywords = {Autism spectrum disorder, virtual world, collaborative virtual reality, naturalistic intervention, social skills training},
	pages = {1178--1209},
	file = {2024_Ke et al._Designing and deploying a virtual social sandbox for autistic children.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\2024_Ke et al._Designing and deploying a virtual social sandbox for autistic children.pdf:application/pdf},
}

@article{ke2022a,
	title = {Virtual {Reality}–{Based} {Social} {Skills} {Training} for {Children} {With} {Autism} {Spectrum} {Disorder}},
	volume = {37},
	issn = {0162-6434},
	url = {https://doi.org/10.1177/0162643420945603},
	doi = {10.1177/0162643420945603},
	abstract = {In this study, the researchers explored the usage of a virtual reality (VR)–based social skills learning environment for children with autism spectrum disorder (ASD). Using OpenSimulator, the researchers constructed a desktop VR-based learning environment that supports social-oriented role-play, gaming, and design by children with ASD. Seven 10–14 years old children with ASD participated in this VR-based social skills program for 20+ hr on average. Data were collected via screen recording and observation of play- and design-oriented social skills enactment and pre- and postintervention Social Communication and Skills Questionnaires. Participants demonstrated an increased level of successful social skills performance from the baseline to the intervention phase. The findings provided preliminary evidence for the usage of a VR-based social skills learning environment for children with ASD.},
	language = {en},
	number = {1},
	urldate = {2025-02-14},
	journal = {Journal of Special Education Technology},
	author = {Ke, Fengfeng and Moon, Jewoong and Sokolikj, Zlatko},
	month = mar,
	year = {2022},
	note = {Publisher: SAGE Publications Inc},
	pages = {49--62},
	file = {2022_Ke et al._Virtual Reality–Based Social Skills Training for Children With Autism Spectrum Disorder.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\2022_Ke et al._Virtual Reality–Based Social Skills Training for Children With Autism Spectrum Disorder.pdf:application/pdf},
}

@article{meng2021,
	title = {Exploring the {Social} {Interaction} of {Autistic} {Students} of {Elementary} and {Junior} {High} {School} {Students} {Through} the {Teaching} of {Social} {Skills} and {Learning} {Process} in {Virtual} {Reality}},
	volume = {3},
	issn = {2661-8907},
	url = {https://doi.org/10.1007/s42979-021-00914-z},
	doi = {10.1007/s42979-021-00914-z},
	abstract = {With the development of global technology and teaching trends, digital technology teaching has become a common phenomenon today. This study uses the virtual reality social skills course to teach ten autistic students for Elementary and Junior High School, with the theme of “One Art Tour with Students” and four key points in the 12-year National Basic Education Curriculum. In virtual reality situational teaching, through t test analysis, it is found that inappropriate social behavior is obviously improved, and environmental adaptability is also greatly improved. Through interviews, parents and teachers believe that children’s social skills have improved. In addition, the research found that some problems must be noticed in the actual teaching process, including (1) black picture in virtual reality; (2) students control the handle controller in the virtual reality; and (3) definition of virtual space. The situations mentioned above should be handled with caution, they may affect the effectiveness of students' learning and involve the safety of the teaching process. These findings are suitable for virtual reality teaching and operation of students with autism, so opinions are provided to the field scholars as a reference.},
	language = {en},
	number = {1},
	urldate = {2025-02-14},
	journal = {SN Computer Science},
	author = {Meng, Ying-Ru and Yeh, Chia-Chi},
	month = nov,
	year = {2021},
	keywords = {Virtual reality, Social skills, High functional autism},
	pages = {55},
	file = {2021_Meng and Yeh_Exploring the Social Interaction of Autistic Students of Elementary and Junior High School Students.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\2021_Meng and Yeh_Exploring the Social Interaction of Autistic Students of Elementary and Junior High School Students.pdf:application/pdf},
}

@article{vanpelt2022,
	title = {Dynamic {Interactive} {Social} {Cognition} {Training} in {Virtual} {Reality} ({DiSCoVR}) for adults with {Autism} {Spectrum} {Disorder}: {A} feasibility study},
	volume = {96},
	issn = {1750-9467},
	shorttitle = {Dynamic {Interactive} {Social} {Cognition} {Training} in {Virtual} {Reality} ({DiSCoVR}) for adults with {Autism} {Spectrum} {Disorder}},
	url = {https://www.sciencedirect.com/science/article/pii/S1750946722000903},
	doi = {10.1016/j.rasd.2022.102003},
	abstract = {Background
Social cognitive difficulties in Autism Spectrum Disorder (ASD) can affect the daily lives of people with ASD profoundly, impacting the development and maintenance of meaningful social relations. Social cognition training (SCT) is commonly used for improving social functioning, but lacks ecological validity and the ability to effectively mimic social situations. Development of virtual reality (VR) interventions, focusing on enhancing social cognition, could add to the effectiveness of SCT within ASD care, by offering a safe, interactive and practical training setting, where generalization of knowledge and skills to the real-world are promoted. In this paper, our primary aim is to evaluate the feasibility and acceptance by participants and therapists of the Dynamic Interactive Social Cognition
Method
Training in Virtual Reality (DiSCoVR) protocol as developed for adults with schizophrenic spectrum disorder (SSD), adapted for ASD (DiSCoVR-A). 26 participants, aged 18–63, took part in a pilot study. 22 participants completed baseline and post-assessment, including primary outcome evaluation assessment through a semi-structured interview. Secondary measures focused on social cognition, emotion recognition, mental flexibility, social anxiety, empathy and social responsiveness and were assessed at baseline (T0), post-treatment (T1), and at follow-up (T2) sixteen weeks after completion of the intervention.
Results
Our results show that the majority of participant and therapists found the VR intervention acceptable and feasible, as reported in evaluation questionnaires and interviews.
Conclusion
These preliminary findings are promising; however, controlled research is needed to further investigate the effectiveness of VR within social cognition training for adults with ASD.},
	urldate = {2025-02-14},
	journal = {Research in Autism Spectrum Disorders},
	author = {van Pelt, B. J. and Nijman, S. A. and van Haren, N. E. M. and Veling, W. and Pijnenborg, G. H. M. and van Balkom, I. D. C. and Landlust, A. M. and Greaves-Lord, K.},
	month = aug,
	year = {2022},
	keywords = {Virtual reality, Autism spectrum disorder, Emotion perception, Social cognition training, Social functioning, Theory of mind},
	pages = {102003},
	file = {2022_van Pelt et al._Dynamic Interactive Social Cognition Training in Virtual Reality (DiSCoVR) for adults with Autism Sp.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\2022_van Pelt et al._Dynamic Interactive Social Cognition Training in Virtual Reality (DiSCoVR) for adults with Autism Sp.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\JIQLQ2IP\\S1750946722000903.html:text/html},
}

@article{zhao2022,
	title = {Virtual reality technology enhances the cognitive and social communication of children with autism spectrum disorder},
	volume = {10},
	issn = {2296-2565},
	url = {https://www.frontiersin.org/journals/public-health/articles/10.3389/fpubh.2022.1029392/full},
	doi = {10.3389/fpubh.2022.1029392},
	abstract = {{\textless}sec{\textgreater}{\textless}title{\textgreater}Objective{\textless}/title{\textgreater}{\textless}p{\textgreater}We aimed to explore the impact of using virtual reality technology to intervene in and encourage the developmental behavior areas of cognition, imitation, and social interaction in children with autism spectrum disorder.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Methods{\textless}/title{\textgreater}{\textless}p{\textgreater}Forty-four children with autism spectrum disorder were divided randomly into an intervention group and a control group, with each group consisting of 22 participants. Incorporating conventional rehabilitation strategies, virtual reality technology was used with the intervention group to conduct rehabilitation training in areas including cognition, imitation, and social interaction. The control group was provided conventional/routine clinical rehabilitation training. The children's cognitive development was evaluated before and 3 months after intervention.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Results{\textless}/title{\textgreater}{\textless}p{\textgreater}After intervention, the developmental abilities of both groups of children in the areas of cognition, imitation, and social interaction were improved over their abilities measured before the intervention ({\textless}italic{\textgreater}P{\textless}/italic{\textgreater} \&lt; 0.05). However, post-intervention score differences between the two groups demonstrated that the intervention group levels were better than the control group levels only in the areas of cognition and social interaction ({\textless}italic{\textgreater}P{\textless}/italic{\textgreater} \&lt; 0.05).{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Conclusion{\textless}/title{\textgreater}{\textless}p{\textgreater}Combining virtual reality with conventional rehabilitation training improved the cognitive and social development of children with autism spectrum disorder and supported the goal of improving the rehabilitation effect.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}},
	language = {English},
	urldate = {2025-02-14},
	journal = {Frontiers in Public Health},
	author = {Zhao, Junqiang and Zhang, Xinxin and Lu, Yi and Wu, Xingyang and Zhou, Fujun and Yang, Shichang and Wang, Luping and Wu, Xiaoyan and Fei, Fangrong},
	month = oct,
	year = {2022},
	note = {Publisher: Frontiers},
	keywords = {virtual reality, Autism Spectrum Disorder, Cognition, Developmental ability, Nursing information, social communication},
	file = {2022_Zhao et al._Virtual reality technology enhances the cognitive and social communication of children with autism s.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\2022_Zhao et al._Virtual reality technology enhances the cognitive and social communication of children with autism s.pdf:application/pdf},
}

@article{yang2025,
	title = {Effectiveness of {Virtual} {Reality} {Technology} {Interventions} in {Improving} the {Social} {Skills} of {Children} and {Adolescents} {With} {Autism}: {Systematic} {Review}},
	volume = {27},
	shorttitle = {Effectiveness of {Virtual} {Reality} {Technology} {Interventions} in {Improving} the {Social} {Skills} of {Children} and {Adolescents} {With} {Autism}},
	url = {https://www.jmir.org/2025/1/e60845},
	doi = {10.2196/60845},
	abstract = {Background: Virtual reality (VR) technology has shown significant potential in improving the social skills of children and adolescents with autism spectrum disorder (ASD). Objective: This study aimed to systematically review the evidence supporting the effectiveness of VR technology in improving the social skills of children and adolescents with ASD. Methods: The search for eligible studies encompassed 4 databases: PubMed, Web of Science, IEEE, and Scopus. Two (XY and JW) researchers independently assessed the extracted studies according to predefined criteria for inclusion and exclusion. These researchers also independently extracted information regarding gathered data on the sources, samples, measurement methods, primary results, and data related to the main results of the studies that met the inclusion criteria. The quality of the studies was further evaluated using the Physiotherapy Evidence Database scale. Results: This review analyzed 14 studies on using VR technology interventions to improve social skills in children and adolescents with ASD. Our findings indicate that VR interventions have a positive effect on improving social skills in children and adolescents with ASD. Compared with individuals with low-functioning autism (LFA), those with high-functioning autism (HFA) benefited more from the intervention. The duration and frequency of the intervention may also influence its effectiveness. In addition, immersive VR is more suitable for training complex skills in individuals with HFA. At the same time, nonimmersive VR stands out in terms of lower cost and flexibility, making it more appropriate for basic skill interventions for people with LFA. Finally, while VR technology positively enhances social skills, some studies have reported potential adverse side effects. According to the quality assessment using the Physiotherapy Evidence Database scale, of the 14 studies, 6 (43\%) were classified as high quality, 4 (29\%) as moderate quality, and 4 (29\%) as low quality. Conclusions: This systematic review found that VR technology interventions positively impact social skills in children and adolescents with ASD, with particularly significant effects on the enhancement of complex social skills in individuals with HFA. For children and adolescents with LFA, progress was mainly observed in basic skills. Immersive VR interventions are more suitable for the development of complex skills. At the same time, nonimmersive VR, due to its lower cost and greater flexibility, also holds potential for application in specific contexts. However, the use of VR technology may lead to side effects such as dizziness, eye fatigue, and sensory overload, particularly in immersive settings. These potential issues should be carefully addressed in intervention designs to ensure user comfort and safety. Future research should focus on optimizing individualized interventions and further exploring the long-term effects of VR interventions. Clinical Trial: International Platform of Registered Systematic Review and Meta-analysis Protocols INPLASY202420079U1; https://inplasy.com/inplasy-2024-2-0079/},
	language = {EN},
	number = {1},
	urldate = {2025-02-14},
	journal = {Journal of Medical Internet Research},
	author = {Yang, Xipeng and Wu, Jinlong and Ma, Yudan and Yu, Jingxuan and Cao, Hong and Zeng, Aihua and Fu, Rui and Tang, Yucheng and Ren, Zhanbing},
	month = feb,
	year = {2025},
	note = {Company: Journal of Medical Internet Research
Distributor: Journal of Medical Internet Research
Institution: Journal of Medical Internet Research
Label: Journal of Medical Internet Research
Publisher: JMIR Publications Inc., Toronto, Canada},
	pages = {e60845},
	file = {2025_Yang et al._Effectiveness of Virtual Reality Technology Interventions in Improving the Social Skills of Children.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\2025_Yang et al._Effectiveness of Virtual Reality Technology Interventions in Improving the Social Skills of Children.pdf:application/pdf},
}

@article{poglitsch2024,
	title = {{XR} technologies to enhance the emotional skills of people with autism spectrum disorder: {A} systematic review},
	volume = {121},
	issn = {0097-8493},
	shorttitle = {{XR} technologies to enhance the emotional skills of people with autism spectrum disorder},
	url = {https://www.sciencedirect.com/science/article/pii/S0097849324000773},
	doi = {10.1016/j.cag.2024.103942},
	abstract = {In this paper, we present a systematic review of the applications of (1) Extended Reality (XR), (2) Augmented Reality (AR), and (3) Virtual Reality (VR) technologies to enhance emotion recognition and emotion expression in people with Autism Spectrum Disorder (ASD). ASD can affect various abilities, and poses challenges to the recognition of emotions in others, which is often referred to as “social blindness”. Treating this condition typically requires intensive one-on-one or small-group therapy sessions, which can be costly and limited in terms of availability. With the growing number of diagnoses of ASD, concerns have risen regarding a potential “lost generation” that may face difficulties in fulfilling its potential. Through this comprehensive review, we aim to provide an overview of innovative approaches that use XR technologies to improve the learning experience of individuals with ASD.},
	urldate = {2025-02-14},
	journal = {Computers \& Graphics},
	author = {Poglitsch, Christian and Safikhani, Saeed and List, Erin and Pirker, Johanna},
	month = jun,
	year = {2024},
	keywords = {Virtual reality (VR), Emotion recognition, Autism spectrum disorder, Augmented reality (AR), Emotional skills, Extended reality (XR)},
	pages = {103942},
	file = {2024_Poglitsch et al._XR technologies to enhance the emotional skills of people with autism spectrum disorder A systemati.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\2024_Poglitsch et al._XR technologies to enhance the emotional skills of people with autism spectrum disorder A systemati.pdf:application/pdf},
}

@article{zotero-5033,
	title = {{PSYCHIATRIA} {DANUBINA}},
	volume = {36},
	abstract = {Background: Autism spectrum disorder (ASD) is a complex neurodevelopmental disorder that affects a significant proportion of the world's population, particularly children and adolescents. The sensory processing issues can be an evidence-based target for therapeutic/corrective interventions by controlling the intensity and targeted replacement of maladaptive sensory stimuli with neutral stimuli using virtual reality or augmented reality. Subjects and methods: We searched for articles on Pubmed. The search query included ((VR or virtual reality) or (AR or augmented reality)) and (children or adolescents) and (ASD or autism spectrum disorder or autism).
Results: Our criteria were met by 25 articles. 19 articles used VR, 5 articles used AR and 1 article used MR. Most interventions offer children and adolescents with ASD individualized tasks. Immersive VR games developed collaborative skills. Other systems encourage and teach directed facial gaze. Evaluation of the effectiveness of learning in VR/AR environment is carried out by means of different scales, qualitative analysis of surveys, questionnaires and interviews, studying the number and duration of eye contacts between the participant and the avatar. It should be noted that almost all studies were conducted on small samples, so their results allow us to draw only preliminary conclusions about the effectiveness of VR /AR.
Conclusions: The following key areas of VR/AR technologies for children and adolescents with high-functioning ASD can be identified: communicating with an avatar, including answering its questions, tracking the child's gaze and encouraging the child to look at the face, placing it in social situations close to real life, practicing common everyday situations, learning to recognize emotions. A VR/AR-based therapy approach may help children with autism spectrum disorder without cognitive impairment to develop higher levels of adaptation in terms of social and communication skills. However, more research is needed to evaluate the effectiveness of different methods.},
	language = {en},
	journal = {VIRTUAL REALITY},
	file = {PSYCHIATRIA DANUBINA.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\PSYCHIATRIA DANUBINA.pdf:application/pdf},
}

@article{dechsling2022,
	title = {Virtual and {Augmented} {Reality} in {Social} {Skills} {Interventions} for {Individuals} with {Autism} {Spectrum} {Disorder}: {A} {Scoping} {Review}},
	volume = {52},
	issn = {1573-3432},
	shorttitle = {Virtual and {Augmented} {Reality} in {Social} {Skills} {Interventions} for {Individuals} with {Autism} {Spectrum} {Disorder}},
	url = {https://doi.org/10.1007/s10803-021-05338-5},
	doi = {10.1007/s10803-021-05338-5},
	abstract = {In the last decade, there has been an increase in publications on technology-based interventions for autism spectrum disorder (ASD). Virtual reality based assessments and intervention tools are promising and have shown to be acceptable amongst individuals with ASD. This scoping review reports on 49 studies utilizing virtual reality and augmented reality technology in social skills interventions for individuals with ASD. The included studies mostly targeted children and adolescents, but few targeted very young children or adults. Our findings show that the mode number of participants with ASD is low, and that female participants are underrepresented. Our review suggests that there is need for studies that apply virtual and augmented realty with more rigorous designs involving established and evidenced-based intervention strategies.},
	language = {en},
	number = {11},
	urldate = {2025-02-14},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Dechsling, Anders and Orm, Stian and Kalandadze, Tamara and Sütterlin, Stefan and Øien, Roald A. and Shic, Frederick and Nordahl-Hansen, Anders},
	month = nov,
	year = {2022},
	keywords = {Augmented reality, Virtual reality, Autism spectrum disorder, Social skills},
	pages = {4692--4707},
	file = {2022_Dechsling et al._Virtual and Augmented Reality in Social Skills Interventions for Individuals with Autism Spectrum Di.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\orif\\2022_Dechsling et al._Virtual and Augmented Reality in Social Skills Interventions for Individuals with Autism Spectrum Di.pdf:application/pdf},
}

@article{bourson2024,
	title = {Characteristics of restricted interests in girls with {ASD} compared to boys: a systematic review of the literature},
	volume = {33},
	issn = {1435-165X},
	shorttitle = {Characteristics of restricted interests in girls with {ASD} compared to boys},
	url = {https://doi.org/10.1007/s00787-022-01998-5},
	doi = {10.1007/s00787-022-01998-5},
	abstract = {The existence of a female phenotype profile in autistic spectrum disorder is one of the current hypotheses to explain the diagnostic discrepancy between men and women. In this context, an international literature review was carried out to evidence and describe the characteristics of restricted interests found in girls with autistic spectrum disorder. A documentary search was conducted on PubMed and a systematic literature review was carried out based on the PRISMA methodology. We selected studies with a population of boys and girls diagnosed as autistic according to the DSM-IV or the DSM-5, in which quantitative and descriptive comparisons of restricted interests, according to gender were carried out. Nineteen studies were found to be relevant. Fifteen enabled a refining of the characteristics of restricted interests among females: fewer restricted interests were identified in comparison with boys, and the autistic girls’ interests seem to be closer to those of neurotypical girls than to those of autistic boys, which thus led to more complex screening. Age and Intelligence quotient seem to be two factors that trigger variations in restricted interests differently according to gender. Representations among professionals also have an impact on diagnoses among girls. For future research, one of the perspectives could be a comparison between girls with autism and neurotypical girls to limit gender bias. The present results contribute to potentially extending knowledge of a female phenotypical profile in autism and show the need to improve the general population’s awareness, to improve health professionals’ training and possibly to revise the diagnostic tools.},
	language = {en},
	number = {4},
	urldate = {2025-02-14},
	journal = {European Child \& Adolescent Psychiatry},
	author = {Bourson, Lise and Prevost, Camille},
	month = apr,
	year = {2024},
	keywords = {Diagnosis, Autistic spectrum disorder, Circumscribed interests, Girl, Restricted interests, Women},
	pages = {987--1004},
	file = {Full Text PDF:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2024_Bourson and Prevost_Characteristics of restricted interests in girls with ASD compared to boys a systematic review of t.pdf:application/pdf},
}

@article{young2018,
	title = {Clinical characteristics and problems diagnosing autism spectrum disorder in girls},
	volume = {25},
	issn = {0929-693X},
	url = {https://www.sciencedirect.com/science/article/pii/S0929693X18301477},
	doi = {10.1016/j.arcped.2018.06.008},
	abstract = {Background
Autism is a neurodevelopmental disorder with various clinical presentations. It has been historically considered a male disorder. An increasing number of authors stress the existence of sex/gender bias in prevalence and the need to define sex/gender differences in the clinical presentation.
Review
Recently, an increasing number of authors have studied the impact of sex/gender on autism's clinical presentation. The sex ratio of four boys to one girl commonly reported in literature is questioned. Sociocultural and familial influences can impact female clinical presentation as well as the way the difficulties of girls with autism are perceived. Issues of autism diagnostic instruments such as sex/gender bias are also studied since they have an impact on the access to diagnosis for girls. Clinical variability is a part of autism spectrum disorder, but some traits appear to be more specific of the female phenotype: existence of a “camouflage” phenomenon and less unusual play or restricted interests.
Discussion
Better understanding and diagnosis of females with autism is required to ensure the access to the support and treatment they need. Professionals must apprehend the sex/gender clinical differences to prevent the frequent misdiagnosis or missed diagnosis of females with autism.
Conclusion
Pursuing research on sex/gender differences seems necessary to ensure appropriate support and diagnosis of undiagnosed females.},
	number = {6},
	urldate = {2025-02-14},
	journal = {Archives de Pédiatrie},
	author = {Young, H. and Oreve, M. -J. and Speranza, M.},
	month = aug,
	year = {2018},
	keywords = {Autism spectrum disorder, Diagnosis, Gender, Sex, Prevalence},
	pages = {399--403},
	file = {2018_Young et al._Clinical characteristics and problems diagnosing autism spectrum disorder in girls.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2018_Young et al._Clinical characteristics and problems diagnosing autism spectrum disorder in girls.pdf:application/pdf},
}

@article{cruz2024,
	title = {Is {There} a {Bias} {Towards} {Males} in the {Diagnosis} of {Autism}? {A} {Systematic} {Review} and {Meta}-{Analysis}},
	issn = {1573-6660},
	shorttitle = {Is {There} a {Bias} {Towards} {Males} in the {Diagnosis} of {Autism}?},
	url = {https://doi.org/10.1007/s11065-023-09630-2},
	doi = {10.1007/s11065-023-09630-2},
	abstract = {Autism is more frequently diagnosed in males, with evidence suggesting that females are more likely to be misdiagnosed or underdiagnosed. Possibly, the male/female ratio imbalance relates to phenotypic and camouflaging differences between genders. Here, we performed a comprehensive approach to phenotypic and camouflaging research in autism addressed in two studies. First (Study 1 – Phenotypic Differences in Autism), we conducted a systematic review and meta-analysis of gender differences in autism phenotype. The electronic datasets Pubmed, Scopus, Web of Science, and PsychInfo were searched. We included 67 articles that compared females and males in autism core symptoms, and in cognitive, socioemotional, and behavioural phenotypes. Autistic males exhibited more severe symptoms and social interaction difficulties on standard clinical measures than females, who, in turn, exhibited more cognitive and behavioural difficulties. Considering the hypothesis of camouflaging possibly underlying these differences, we then conducted a meta-analysis of gender differences in camouflaging (Study 2 – Camouflaging Differences in Autism). The same datasets as the first study were searched. Ten studies were included. Females used more compensation and masking camouflage strategies than males. The results support the argument of a bias in clinical procedures towards males and the importance of considering a ‘female autism phenotype’—potentially involving camouflaging—in the diagnostic process.},
	language = {en},
	urldate = {2025-02-14},
	journal = {Neuropsychology Review},
	author = {Cruz, Sara and Zubizarreta, Sabela Conde-Pumpido and Costa, Ana Daniela and Araújo, Rita and Martinho, Júlia and Tubío-Fungueiriño, María and Sampaio, Adriana and Cruz, Raquel and Carracedo, Angel and Fernández-Prieto, Montse},
	month = jan,
	year = {2024},
	keywords = {Autism, Meta-analysis, Systematic review, Camouflaging, Gender differences},
	file = {2024_Cruz et al._Is There a Bias Towards Males in the Diagnosis of Autism A Systematic Review and Meta-Analysis.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2024_Cruz et al._Is There a Bias Towards Males in the Diagnosis of Autism A Systematic Review and Meta-Analysis.pdf:application/pdf},
}

@article{natoli2023,
	title = {No sex differences in core autism features, social functioning, cognition or co-occurring conditions in young autistic children: {A} systematic review and meta-analysis},
	volume = {107},
	issn = {1750-9467},
	shorttitle = {No sex differences in core autism features, social functioning, cognition or co-occurring conditions in young autistic children},
	url = {https://www.sciencedirect.com/science/article/pii/S1750946723001071},
	doi = {10.1016/j.rasd.2023.102207},
	abstract = {Autism Spectrum Disorder (ASD) is a neurodevelopmental condition with more males than females diagnosed, and researchers have considered whether the existence of a female-specific ASD phenotype may contribute to differences in rates of diagnosis. We sought to inform this issue through a systematic review and meta-analysis of potential sex differences specifically in young autistic children across a range of domains including core ASD features, social functioning, cognition, and co-occurring internalising and/or externalising conditions. The systematic review identified 35 studies examining sex differences in young autistic children. Conflicting results were evident across studies, with some reporting small sex differences and others reporting no sex differences. Meta-analysis revealed no overarching significant sex differences in the domains investigated. However, the meta-analytic effect for the RRB domain approached significance, with females demonstrating fewer RRBs than males. Many of the primary studies included here utilised data from standardised diagnostic instruments to measure autism features, so while this study suggests non-significant sex differences in early childhood ASD, it remains possible that current tools are insufficiently sensitive to detect differences in ASD presentation by sex at this age. It is also possible that the diagnostic criteria may reflect a predominately ‘male phenotype’ and this may obscure the detection of genuine sex differences in young autistic children.},
	urldate = {2025-02-14},
	journal = {Research in Autism Spectrum Disorders},
	author = {Natoli, Katherine and Brown, Amy and Bent, Catherine A. and Luu, Jenny and Hudry, Kristelle},
	month = sep,
	year = {2023},
	keywords = {Autism, Sex differences, Core Autism features, Early Childhood, Restricted and Repetitive Behaviours},
	pages = {102207},
	file = {2023_Natoli et al._No sex differences in core autism features, social functioning, cognition or co-occurring conditions.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2023_Natoli et al._No sex differences in core autism features, social functioning, cognition or co-occurring conditions.pdf:application/pdf},
}

@article{lockwoodestrin2021,
	title = {Barriers to {Autism} {Spectrum} {Disorder} {Diagnosis} for {Young} {Women} and {Girls}: a {Systematic} {Review}},
	volume = {8},
	issn = {2195-7185},
	shorttitle = {Barriers to {Autism} {Spectrum} {Disorder} {Diagnosis} for {Young} {Women} and {Girls}},
	url = {https://doi.org/10.1007/s40489-020-00225-8},
	doi = {10.1007/s40489-020-00225-8},
	abstract = {There is increased recognition that women and girls with autism spectrum disorders (ASD) are underserved by the clinical criteria and processes required to receive a diagnosis. This mixed-methods systematic review aimed to identify key barriers to obtaining an ASD diagnosis in girls and young women under 21 years. Six themes were identified that focused on perceived gendered symptoms, namely behavioural problems, social and communication abilities, language, relationships, additional diagnoses/difficulties and restricted and repetitive behaviours and interests. Five themes were identified as (parental) perceived barriers to diagnosis, namely compensatory behaviours, parental concerns, others’ perceptions, lack of information/resources and clinician bias. This review highlights the importance of enhancing widespread understanding and recognition of ASD presentation in females across development. PROSPERO Centre for Reviews and Dissemination (ID 2018 CRD42018087235)},
	language = {en},
	number = {4},
	urldate = {2025-02-14},
	journal = {Review Journal of Autism and Developmental Disorders},
	author = {Lockwood Estrin, Georgia and Milner, Victoria and Spain, Debbie and Happé, Francesca and Colvert, Emma},
	month = dec,
	year = {2021},
	keywords = {Autism, Diagnosis, Female, Gender differences, Barriers},
	pages = {454--470},
	file = {2021_Lockwood Estrin et al._Barriers to Autism Spectrum Disorder Diagnosis for Young Women and Girls a Systematic Review.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2021_Lockwood Estrin et al._Barriers to Autism Spectrum Disorder Diagnosis for Young Women and Girls a Systematic Review.pdf:application/pdf},
}

@article{edwards2024a,
	title = {Research {Review}: {A} systematic review and meta-analysis of sex differences in narrow constructs of restricted and repetitive behaviours and interests in autistic children, adolescents, and adults},
	volume = {65},
	copyright = {© 2023 The Authors. Journal of Child Psychology and Psychiatry published by John Wiley \& Sons Ltd on behalf of Association for Child and Adolescent Mental Health.},
	issn = {1469-7610},
	shorttitle = {Research {Review}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jcpp.13855},
	doi = {10.1111/jcpp.13855},
	abstract = {Background Evidence that autism often manifests differently between males and females is growing, particularly in terms of social interaction and communication, but it is unclear if there are sex differences in restricted and repetitive behaviours and interests (RRBIs) when rigorously focusing on the narrow construct level (i.e., stereotyped behaviour, restricted interests, insistence on sameness, and/or sensory experiences). Methods We conducted a systematic review and four random effects meta-analyses investigating sex differences in narrow construct measures of RRBIs in autistic children, adolescents, and adults (Prospero registration ID: CRD42021254221). Study quality was appraised using the Newcastle-Ottawa Quality Assessment Scale. Results Forty-six studies were narratively synthesised and 25 of these were included in four random effects meta-analyses. Results found that autistic males had significantly higher levels of stereotyped behaviours (SMD = 0.21, 95\% confidence interval (CI) [0.09, 0.33], p {\textless} .001) and restricted interests (SMD = 0.18, 95\% CI [0.07, 0.29], p {\textless} .001) compared to autistic females. In contrast, there were no significant sex differences for sensory experiences (SMD = −0.09, 95\% CI [−0.27, 0.09], p = .32) and insistence on sameness (SMD = 0.01, 95\% CI [−0.03, 0.05], p = .68). The findings from the narrative synthesis were generally consistent with those from the meta-analyses and also found qualitative sex differences in the way RRBIs manifest. Conclusions Our findings show significant differences in narrowly defined RRBIs in males and females. Practitioners need to be aware of such differences, which could be contributing to the under-recognition of autism in females and may not be captured by current diagnostic instruments.},
	language = {en},
	number = {1},
	urldate = {2025-02-14},
	journal = {Journal of Child Psychology and Psychiatry},
	author = {Edwards, Hannah and Wright, Sarah and Sargeant, Cora and Cortese, Samuele and Wood-Downie, Henry},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jcpp.13855},
	keywords = {Autism, gender differences, meta-analysis, restricted and repetitive behaviours and interests, sex differences, systematic review},
	pages = {4--17},
	file = {2024_Edwards et al._Research Review A systematic review and meta-analysis of sex differences in narrow constructs of re.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2024_Edwards et al._Research Review A systematic review and meta-analysis of sex differences in narrow constructs of re.pdf:application/pdf},
}

@article{napolitano2022,
	title = {Sex {Differences} in {Autism} {Spectrum} {Disorder}: {Diagnostic}, {Neurobiological}, and {Behavioral} {Features}},
	volume = {13},
	issn = {1664-0640},
	shorttitle = {Sex {Differences} in {Autism} {Spectrum} {Disorder}},
	url = {https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2022.889636/full},
	doi = {10.3389/fpsyt.2022.889636},
	abstract = {{\textless}p{\textgreater}Autism Spectrum Disorder (ASD) is a complex neurodevelopmental disorder with a worldwide prevalence of about 1\%, characterized by impairments in social interaction, communication, repetitive patterns of behaviors, and can be associated with hyper- or hypo-reactivity of sensory stimulation and cognitive disability. ASD comorbid features include internalizing and externalizing symptoms such as anxiety, depression, hyperactivity, and attention problems. The precise etiology of ASD is still unknown and it is undoubted that the disorder is linked to some extent to both genetic and environmental factors. It is also well-documented and known that one of the most striking and consistent finding in ASD is the higher prevalence in males compared to females, with around 70\% of ASD cases described being males. The present review looked into the most significant studies that attempted to investigate differences in ASD males and females thus trying to shade some light on the peculiar characteristics of this prevalence in terms of diagnosis, imaging, major autistic-like behavior and sex-dependent uniqueness. The study also discussed sex differences found in animal models of ASD, to provide a possible explanation of the neurological mechanisms underpinning the different presentation of autistic symptoms in males and females.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-02-14},
	journal = {Frontiers in Psychiatry},
	author = {Napolitano, Antonio and Schiavi, Sara and La Rosa, Piergiorgio and Rossi-Espagnet, Maria Camilla and Petrillo, Sara and Bottino, Francesca and Tagliente, Emanuela and Longo, Daniela and Lupi, Elisabetta and Casula, Laura and Valeri, Giovanni and Piemonte, Fiorella and Trezza, Viviana and Vicari, Stefano},
	month = may,
	year = {2022},
	note = {Publisher: Frontiers},
	keywords = {autism, Animal Models, gender, imaging, Neurobiological mechanism},
	file = {2022_Napolitano et al._Sex Differences in Autism Spectrum Disorder Diagnostic, Neurobiological, and Behavioral Features.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2022_Napolitano et al._Sex Differences in Autism Spectrum Disorder Diagnostic, Neurobiological, and Behavioral Features.pdf:application/pdf},
}

@article{duvekot2017,
	title = {Factors influencing the probability of a diagnosis of autism spectrum disorder in girls versus boys},
	volume = {21},
	issn = {1362-3613},
	url = {https://doi.org/10.1177/1362361316672178},
	doi = {10.1177/1362361316672178},
	abstract = {In order to shed more light on why referred girls are less likely to be diagnosed with autism spectrum disorder than boys, this study examined whether behavioral characteristics influence the probability of an autism spectrum disorder diagnosis differently in girls versus boys derived from a multicenter sample of consecutively referred children aged 2.5–10 years. Based on information from the short version of the Developmental, Dimensional and Diagnostic Interview and the Autism Diagnostic Observation Schedule, 130 children (106 boys and 24 girls) received a diagnosis of autism spectrum disorder according to Diagnostic and Statistical Manual of Mental Disorders (4th ed., text rev.) criteria and 101 children (61 boys and 40 girls) did not. Higher overall levels of parent-reported repetitive and restricted behavior symptoms were less predictive of an autism spectrum disorder diagnosis in girls than in boys (odds ratio interaction = 0.41, 95\% confidence interval = 0.18–0.92, p = 0.03). In contrast, higher overall levels of parent-reported emotional and behavioral problems increased the probability of an autism spectrum disorder diagnosis more in girls than in boys (odds ratio interaction = 2.44, 95\% confidence interval = 1.13–5.29, p = 0.02). No differences were found between girls and boys in the prediction of an autism spectrum disorder diagnosis by overall autistic impairment, sensory symptoms, and cognitive functioning. These findings provide insight into possible explanations for the assumed underidentification of autism spectrum disorder in girls in the clinic.},
	language = {en},
	number = {6},
	urldate = {2025-02-14},
	journal = {Autism},
	author = {Duvekot, Jorieke and van der Ende, Jan and Verhulst, Frank C and Slappendel, Geerte and van Daalen, Emma and Maras, Athanasios and Greaves-Lord, Kirstin},
	month = aug,
	year = {2017},
	note = {Publisher: SAGE Publications Ltd},
	pages = {646--658},
	file = {2017_Duvekot et al._Factors influencing the probability of a diagnosis of autism spectrum disorder in girls versus boys.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2017_Duvekot et al._Factors influencing the probability of a diagnosis of autism spectrum disorder in girls versus boys.pdf:application/pdf},
}

@article{green2019,
	title = {Women and {Autism} {Spectrum} {Disorder}: {Diagnosis} and {Implications} for {Treatment} of {Adolescents} and {Adults}},
	volume = {21},
	issn = {1535-1645},
	shorttitle = {Women and {Autism} {Spectrum} {Disorder}},
	url = {https://doi.org/10.1007/s11920-019-1006-3},
	doi = {10.1007/s11920-019-1006-3},
	abstract = {We review the recent literature regarding the implications of gender on the diagnosis and treatment of autism spectrum disorder (ASD) in women and adolescent females. We also discuss important clinical observations in treating this population.},
	language = {en},
	number = {4},
	urldate = {2025-02-14},
	journal = {Current Psychiatry Reports},
	author = {Green, Renée M. and Travers, Alyssa M. and Howe, Yamini and McDougle, Christopher J.},
	month = mar,
	year = {2019},
	keywords = {ASD, Autism spectrum disorder, Female, Women, Adults},
	pages = {22},
	file = {2019_Green et al._Women and Autism Spectrum Disorder Diagnosis and Implications for Treatment of Adolescents and Adul.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2019_Green et al._Women and Autism Spectrum Disorder Diagnosis and Implications for Treatment of Adolescents and Adul.pdf:application/pdf},
}

@article{lehnhardt2016,
	title = {Sex-{Related} {Cognitive} {Profile} in {Autism} {Spectrum} {Disorders} {Diagnosed} {Late} in {Life}: {Implications} for the {Female} {Autistic} {Phenotype}},
	volume = {46},
	issn = {1573-3432},
	shorttitle = {Sex-{Related} {Cognitive} {Profile} in {Autism} {Spectrum} {Disorders} {Diagnosed} {Late} in {Life}},
	url = {https://doi.org/10.1007/s10803-015-2558-7},
	doi = {10.1007/s10803-015-2558-7},
	abstract = {Females with high-functioning ASD are known to camouflage their autistic symptoms better than their male counterparts, making them prone to being under-ascertained and delayed in diagnostic assessment. Thus far the underlying cognitive processes that enable such successful socio-communicative adaptation are not well understood. The current results show sex-related differences in the cognitive profile of ASD individuals, which were diagnosed late in life exclusively. Higher verbal abilities were found in males (n = 69) as opposed to higher processing speed and better executive functions in females with ASD (n = 38). Since both sexes remained unidentified during childhood and adolescence, these results are suggestive for sex-distinctive cognitive strategies as an alternative to typically-developed reciprocal social behavior and social mimicry in high functioning ASD.},
	language = {en},
	number = {1},
	urldate = {2025-02-14},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Lehnhardt, Fritz-Georg and Falter, Christine Michaela and Gawronski, Astrid and Pfeiffer, Kathleen and Tepest, Ralf and Franklin, Jeremy and Vogeley, Kai},
	month = jan,
	year = {2016},
	keywords = {Autism spectrum disorder, Adulthood, Cognitive profile, Female autistic phenotype, Processing speed},
	pages = {139--154},
	file = {2016_Lehnhardt et al._Sex-Related Cognitive Profile in Autism Spectrum Disorders Diagnosed Late in Life Implications for.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2016_Lehnhardt et al._Sex-Related Cognitive Profile in Autism Spectrum Disorders Diagnosed Late in Life Implications for.pdf:application/pdf},
}

@article{bolte2011,
	title = {Autistic {Traits} and {Autism} {Spectrum} {Disorders}: {The} {Clinical} {Validity} of {Two} {Measures} {Presuming} a {Continuum} of {Social} {Communication} {Skills}},
	volume = {41},
	issn = {1573-3432},
	shorttitle = {Autistic {Traits} and {Autism} {Spectrum} {Disorders}},
	url = {https://doi.org/10.1007/s10803-010-1024-9},
	doi = {10.1007/s10803-010-1024-9},
	abstract = {Research indicates that autism is the extreme end of a continuously distributed trait. The Social Responsiveness Scale (SRS) and the Social and Communication Disorders Checklist (SCDC) aim to assess autistic traits. The objective of this study was to compare their clinical validity. The SRS showed sensitivities of .74 to .80 and specificities of .69 to 1.00 for autism. Sensitivities were .85 to .90 and specificities .28 to.82 for the SCDC. Correlations with the ADI-R, ADOS and SCQ were higher for the SRS than for the SCDC. The SCDC seems superior to the SRS to screen for unspecific social and communicative deficits including autism. The SRS appears more suitable than the SCDC in clinical settings and for specific autism screening.},
	language = {en},
	number = {1},
	urldate = {2025-02-14},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Bölte, Sven and Westerwald, Eva and Holtmann, Martin and Freitag, Christine and Poustka, Fritz},
	month = jan,
	year = {2011},
	keywords = {Psychometrics, Assessment, Diagnostics, PDD, Screening; Questionnaire},
	pages = {66--72},
	file = {2011_Bölte et al._Autistic Traits and Autism Spectrum Disorders The Clinical Validity of Two Measures Presuming a Con.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2011_Bölte et al._Autistic Traits and Autism Spectrum Disorders The Clinical Validity of Two Measures Presuming a Con.pdf:application/pdf},
}

@article{cook2021,
	title = {Camouflaging in autism: {A} systematic review},
	volume = {89},
	issn = {0272-7358},
	shorttitle = {Camouflaging in autism},
	url = {https://www.sciencedirect.com/science/article/pii/S0272735821001239},
	doi = {10.1016/j.cpr.2021.102080},
	abstract = {Some autistic people employ strategies and behaviours to cope with the everyday social world, thereby ‘camouflaging’ their autistic differences and difficulties. This review aimed to systematically appraise and synthesise the current evidence base pertaining to autistic camouflaging. Following a systematic search of eight databases, 29 studies quantifying camouflaging in children and adults with autism diagnoses or high levels of autistic traits were reviewed. The multiple methods used to measure camouflaging broadly fell under two different approaches: internal-external discrepancy or self-report. These approaches appear to relate to two distinct but potentially connected elements of camouflaging: observable behavioural presentations and self-perceived camouflaging efforts. While significant variation was noted across individual study findings, much of the existing literature supported three preliminary findings about the nature of autistic camouflaging: (1) adults with more self-reported autistic traits report greater engagement in camouflaging; (2) sex and gender differences exist in camouflaging; and (3) higher self-reported camouflaging is associated with worse mental health outcomes. However, the research base was limited regarding participant characterisation and representativeness, which suggests that conclusions cannot be applied to the autistic community as a whole. We propose priorities for future research in refining the current understanding of camouflaging and improving measurement methods.},
	urldate = {2025-02-14},
	journal = {Clinical Psychology Review},
	author = {Cook, Julia and Hull, Laura and Crane, Laura and Mandy, William},
	month = nov,
	year = {2021},
	keywords = {Mental health, Autism, Social behaviour, Camouflaging, Gender, Camouflage},
	pages = {102080},
	file = {2021_Cook et al._Camouflaging in autism A systematic review.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2021_Cook et al._Camouflaging in autism A systematic review.pdf:application/pdf},
}

@article{ochoa-lubinoff2023,
	series = {Neurological {Disorders} in {Women}: {From} {Epidemiology} to {Outcome}},
	title = {Autism in {Women}},
	volume = {41},
	issn = {0733-8619},
	url = {https://www.sciencedirect.com/science/article/pii/S0733861922000925},
	doi = {10.1016/j.ncl.2022.10.006},
	number = {2},
	urldate = {2025-02-14},
	journal = {Neurologic Clinics},
	author = {Ochoa-Lubinoff, Cesar and Makol, Bridget A. and Dillon, Emily F.},
	month = may,
	year = {2023},
	keywords = {Autism spectrum disorder, Male bias, Female autism, Sex differences},
	pages = {381--397},
	file = {2023_Ochoa-Lubinoff et al._Autism in Women.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2023_Ochoa-Lubinoff et al._Autism in Women.pdf:application/pdf},
}

@article{kok2016,
	title = {Self-{Reported} {Empathy} in {Adult} {Women} with {Autism} {Spectrum} {Disorders} – {A} {Systematic} {Mini} {Review}},
	volume = {11},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0151568},
	doi = {10.1371/journal.pone.0151568},
	abstract = {Introduction There is limited research on Autism Spectrum Disorders (ASD) in females. Although the empathy construct has been examined thoroughly in autism, little attention has been paid to empathy in adult women with this condition or to gender differences within the disorder. Objective Self-reported empathy in adult women with ASD was examined and compared to that of typically developed men and women as well as to men with this condition. Methods Online databases were searched for articles investigating self-reported empathy among adult women with ASD. Only six studies comparing women to men were identified. Results All studies found women with an ASD to report lower levels of empathy than typically developed women, and typically developed men, but similar levels to men with this condition. Conclusion The self-reported empathic ability of women diagnosed with ASD resembles that of their male counterparts most closely; they show a hypermasculinisation in empathy. This is particularly surprising considering the large gender difference in empathy in the general population. Discussion One of the limitations of this review is that the current diagnostic criteria for ASD are oriented towards male-specific behaviour and fail to integrate gender specific characteristics. Hence, women diagnosed with ASD are likely to be at the male end of the continuum. The suggested hypermasculinisation of women on the spectrum, as evident from this review, may therefore be exaggerated due to a selection bias.},
	language = {en},
	number = {3},
	urldate = {2025-02-14},
	journal = {PLOS ONE},
	author = {Kok, Francien M. and Groen, Yvonne and Becke, Miriam and Fuermaier, Anselm B. M. and Tucha, Oliver},
	month = mar,
	year = {2016},
	note = {Publisher: Public Library of Science},
	keywords = {Autism, Autism spectrum disorder, Adults, Database searching, Questionnaires, Research reporting guidelines, Social communication, Testosterone},
	pages = {e0151568},
	file = {2016_Kok et al._Self-Reported Empathy in Adult Women with Autism Spectrum Disorders – A Systematic Mini Review.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2016_Kok et al._Self-Reported Empathy in Adult Women with Autism Spectrum Disorders – A Systematic Mini Review.pdf:application/pdf},
}

@incollection{young2021,
	address = {Cham},
	title = {The {Measurement} of {Restricted} and {Repetitive} {Behaviors} in {Autism} {Spectrum} {Disorder}},
	isbn = {978-3-030-66445-9},
	url = {https://doi.org/10.1007/978-3-030-66445-9_8},
	abstract = {The image of the non-verbal child, flapping and spinning, totally absorbed in their own obsessive interests, as described by Kanner (Nerv Child 2:217–250, 1943), represents the more traditional view of Autism Spectrum Disorder (ASD) and the stereotypical behaviors that accompany this condition. However, with the revision of the Diagnostic and Statistical Manual of Mental Disorders (DSM) in 1994 (4th edition; DSM-IV; American Psychiatric Association. Diagnostic and statistical manual of mental disorders, 4th edn. Washington, DC: Author, 1994), the spectrum was expanded to include persons with a milder variant of the disorder (i.e. Asperger’s syndrome). With this came a broader interpretation of the presentation of these restricted and repetitive behaviors and interests (RRBI) in the current edition of the DSM, the DSM-5 (American Psychiatric Association. Diagnostic and statistical manual of mental disorders, 5th edn. Washington, DC: Author, 2013). As a result, the DSM-5 now includes heterogenic behaviors such as motor stereotypies, sensory-related behaviors, circumscribed interests, rituals, excessive sensitivity to change, and echolalic speech. While it is agreed that these behaviors are pervasive in this condition, and form part of the ASD diagnostic criteria, there remains a lack of consensus regarding a definition of RRBI (Leekam et al. Psychol Bull 137:562–593, 2011) and how pervasive these behaviors must be to be considered deviant and of diagnostic significance. This, therefore, creates challenges for researchers and clinicians in designing valid and reliable assessments of RRBI that are sensitive to this disorder, yet specific to ASD. The purpose of this chapter is to operationalize these behaviors and review the currently available tools so that we may determine whether these tools are valid measures of these behaviors.},
	language = {en},
	urldate = {2025-02-14},
	booktitle = {Repetitive and {Restricted} {Behaviors} and {Interests} in {Autism} {Spectrum} {Disorders}: {From} {Neurobiology} to {Behavior}},
	publisher = {Springer International Publishing},
	author = {Young, Robyn L. and Lim, Alliyza},
	editor = {Gal, Eynat and Yirmiya, Nurit},
	year = {2021},
	doi = {10.1007/978-3-030-66445-9_8},
	pages = {115--142},
}

@article{bouchouras2025,
	title = {Integrating {Artificial} {Intelligence}, {Internet} of {Things}, and {Sensor}-{Based} {Technologies}: {A} {Systematic} {Review} of {Methodologies} in {Autism} {Spectrum} {Disorder} {Detection}},
	volume = {18},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1999-4893},
	shorttitle = {Integrating {Artificial} {Intelligence}, {Internet} of {Things}, and {Sensor}-{Based} {Technologies}},
	url = {https://www.mdpi.com/1999-4893/18/1/34},
	doi = {10.3390/a18010034},
	abstract = {This paper presents a systematic review of the emerging applications of artificial intelligence (AI), Internet of Things (IoT), and sensor-based technologies in the diagnosis of autism spectrum disorder (ASD). The integration of these technologies has led to promising advances in identifying unique behavioral, physiological, and neuroanatomical markers associated with ASD. Through an examination of recent studies, we explore how technologies such as wearable sensors, eye-tracking systems, virtual reality environments, neuroimaging, and microbiome analysis contribute to a holistic approach to ASD diagnostics. The analysis reveals how these technologies facilitate non-invasive, real-time assessments across diverse settings, enhancing both diagnostic accuracy and accessibility. The findings underscore the transformative potential of AI, IoT, and sensor-based driven tools in providing personalized and continuous ASD detection, advocating for data-driven approaches that extend beyond traditional methodologies. Ultimately, this review emphasizes the role of technology in improving ASD diagnostic processes, paving the way for targeted and individualized assessments.},
	language = {en},
	number = {1},
	urldate = {2025-02-14},
	journal = {Algorithms},
	author = {Bouchouras, Georgios and Kotis, Konstantinos},
	month = jan,
	year = {2025},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial intelligence, autism spectrum disorder, Internet of Things, non-invasive diagnostics, sensor-based technologies},
	pages = {34},
	file = {2025_Bouchouras and Kotis_Integrating Artificial Intelligence, Internet of Things, and Sensor-Based Technologies A Systematic.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2025_Bouchouras and Kotis_Integrating Artificial Intelligence, Internet of Things, and Sensor-Based Technologies A Systematic.pdf:application/pdf},
}

@article{liu2017,
	title = {Technology-{Facilitated} {Diagnosis} and {Treatment} of {Individuals} with {Autism} {Spectrum} {Disorder}: {An} {Engineering} {Perspective}},
	volume = {7},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	shorttitle = {Technology-{Facilitated} {Diagnosis} and {Treatment} of {Individuals} with {Autism} {Spectrum} {Disorder}},
	url = {https://www.mdpi.com/2076-3417/7/10/1051},
	doi = {10.3390/app7101051},
	abstract = {The rapid development of computer and robotic technologies in the last decade is giving hope to perform earlier and more accurate diagnoses of the Autism Spectrum Disorder (ASD), and more effective, consistent, and cost-conscious treatment. Besides the reduced cost, the main benefit of using technology to facilitate treatment is that stimuli produced during each session of the treatment can be controlled, which not only guarantees consistency across different sessions, but also makes it possible to focus on a single phenomenon, which is difficult even for a trained professional to perform, and deliver the stimuli according to the treatment plan. In this article, we provide a comprehensive review of research on recent technology-facilitated diagnosis and treat of children and adults with ASD. Different from existing reviews on this topic, which predominantly concern clinical issues, we focus on the engineering perspective of autism studies. All technology facilitated systems used for autism studies can be modeled as human machine interactive systems where one or more participants would constitute as the human component, and a computer-based or a robotic-based system would be the machine component. Based on this model, we organize our review with the following questions: (1) What are presented to the participants in the studies and how are the content and delivery methods enabled by technologies? (2) How are the reactions/inputs collected from the participants in response to the stimuli in the studies? (3) Are the experimental procedure and programs presented to participants dynamically adjustable based on the responses from the participants, and if so, how? and (4) How are the programs assessed?},
	language = {en},
	number = {10},
	urldate = {2025-02-14},
	journal = {Applied Sciences},
	author = {Liu, Xiongyi and Wu, Qing and Zhao, Wenbing and Luo, Xiong},
	month = oct,
	year = {2017},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {virtual reality, emotion recognition, affective computing, autism spectrum disorder, avatars, depth sensors, joint attention, social robots},
	pages = {1051},
	file = {2017_Liu et al._Technology-Facilitated Diagnosis and Treatment of Individuals with Autism Spectrum Disorder An Engi.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2017_Liu et al._Technology-Facilitated Diagnosis and Treatment of Individuals with Autism Spectrum Disorder An Engi.pdf:application/pdf},
}

@article{rezayi2025,
	title = {Systematic {Review} and {Thematic} {Analysis} of {Digital} {Games} for {Cognitive} {Enhancement} in {Children} with {Autism} {Spectrum} {Disorder}: {Toward} a {Conceptual} {Framework}},
	volume = {17},
	issn = {1866-9964},
	shorttitle = {Systematic {Review} and {Thematic} {Analysis} of {Digital} {Games} for {Cognitive} {Enhancement} in {Children} with {Autism} {Spectrum} {Disorder}},
	url = {https://doi.org/10.1007/s12559-024-10395-w},
	doi = {10.1007/s12559-024-10395-w},
	abstract = {Compared to typically developing people, children with autism spectrum disorder (ASD) have distinct cognitive and intelligence profiles. Some of these children require cognitive rehabilitation. Through the use of cutting-edge therapy and cognitive empowerment methods, some cognitive skills in children with ASD can be strengthened by digital game-based tools. This study’s main purpose is to provide a systematic review and qualitative study about designing digital games for cognitive enhancement in autistic children and to determine the main design components of such digital games. The primary focus of this study is to explore the citations in which the technical and functional elements are provided thoroughly. Furthermore, a conceptual framework is elaborated for designing a digital game for autism. A thorough review of the literature was conducted in the databases of Medline (via PubMed), Web of Science (WOS), Scopus, and IEEE Xplore for English publications published before January 23, 2023. Of 976 papers, 34 studies were found to be eligible in this systematic review. The bulk of the studies were carried out in Asia and Europe. Three (8.8\%) studies used games that were built to be multilingual, while 22 (64.7\%) studies used games that were only created in English. Creating motivation through narratives, providing incentive systems, raising the complexity level, targeting main skills, and adjusting the choices are the principal components of digital game design. (1) Main cognitive rehabilitation domains in ASD; (2) game designing details: platforms and game genres, motivations, evaluations, game graphics designs, aesthetic mechanisms, incentive systems, and famous game development engines; and (3) mutual interaction between child, therapist, and parents are the crucial categories that are described to devise a conceptual framework in this qualitative study. Of the total number of included studies, 25 studies reported positive effects on autism cases, and in nine, there has not been any evaluation of real cases; however, only usability tests have been conducted. Children with autism may benefit from using appropriate digital game-based interventions to improve mental indices. According to a review, it can be stated that the suitable computerized and digital game-based solutions could enhance cognitive outcomes in children with autism spectrum disorder. However, more research is required to ascertain the true efficacy of these new technologies.},
	language = {en},
	number = {1},
	urldate = {2025-02-14},
	journal = {Cognitive Computation},
	author = {Rezayi, Sorayya and Shahmoradi, Leila and Tehrani-Doost, Mehdi},
	month = jan,
	year = {2025},
	keywords = {Autism spectrum disorder, Artificial Intelligence, Cognitive outcomes, Cognitive rehabilitation, Digital games},
	pages = {60},
	file = {2025_Rezayi et al._Systematic Review and Thematic Analysis of Digital Games for Cognitive Enhancement in Children with.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2025_Rezayi et al._Systematic Review and Thematic Analysis of Digital Games for Cognitive Enhancement in Children with.pdf:application/pdf},
}

@article{mukherjee2024,
	title = {Digital tools for direct assessment of autism risk during early childhood: {A} systematic review},
	volume = {28},
	issn = {1362-3613},
	shorttitle = {Digital tools for direct assessment of autism risk during early childhood},
	url = {https://doi.org/10.1177/13623613221133176},
	doi = {10.1177/13623613221133176},
	abstract = {Current challenges in early identification of autism spectrum disorder lead to significant delays in starting interventions, thereby compromising outcomes. Digital tools can potentially address this barrier as they are accessible, can measure autism-relevant phenotypes and can be administered in children’s natural environments by non-specialists. The purpose of this systematic review is to identify and characterise potentially scalable digital tools for direct assessment of autism spectrum disorder risk in early childhood. In total, 51,953 titles, 6884 abstracts and 567 full-text articles from four databases were screened using predefined criteria. Of these, 38 met inclusion criteria. Tasks are presented on both portable and non-portable technologies, typically by researchers in laboratory or clinic settings. Gamified tasks, virtual-reality platforms and automated analysis of video or audio recordings of children’s behaviours and speech are used to assess autism spectrum disorder risk. Tasks tapping social communication/interaction and motor domains most reliably discriminate between autism spectrum disorder and typically developing groups. Digital tools employing objective data collection and analysis methods hold immense potential for early identification of autism spectrum disorder risk. Next steps should be to further validate these tools, evaluate their generalisability outside laboratory or clinic settings, and standardise derived measures across tasks. Furthermore, stakeholders from underserved communities should be involved in the research and development process.
Lay abstract
The challenge of finding autistic children, and finding them early enough to make a difference for them and their families, becomes all the greater in parts of the world where human and material resources are in short supply. Poverty of resources delays interventions, translating into a poverty of outcomes. Digital tools carry potential to lessen this delay because they can be administered by non-specialists in children’s homes, schools or other everyday environments, they can measure a wide range of autistic behaviours objectively and they can automate analysis without requiring an expert in computers or statistics. This literature review aimed to identify and describe digital tools for screening children who may be at risk for autism. These tools are predominantly at the ‘proof-of-concept’ stage. Both portable (laptops, mobile phones, smart toys) and fixed (desktop computers, virtual-reality platforms) technologies are used to present computerised games, or to record children’s behaviours or speech. Computerised analysis of children’s interactions with these technologies differentiates children with and without autism, with promising results. Tasks assessing social responses and hand and body movements are the most reliable in distinguishing autistic from typically developing children. Such digital tools hold immense potential for early identification of autism spectrum disorder risk at a large scale. Next steps should be to further validate these tools and to evaluate their applicability in a variety of settings. Crucially, stakeholders from underserved communities globally must be involved in this research, lest it fail to capture the issues that these stakeholders are facing.},
	language = {en},
	number = {1},
	urldate = {2025-02-14},
	journal = {Autism},
	author = {Mukherjee, Debarati and Bhavnani, Supriya and Lockwood Estrin, Georgia and Rao, Vaisnavi and Dasgupta, Jayashree and Irfan, Hiba and Chakrabarti, Bhismadev and Patel, Vikram and Belmonte, Matthew K},
	month = jan,
	year = {2024},
	note = {Publisher: SAGE Publications Ltd},
	pages = {6--31},
	file = {2024_Mukherjee et al._Digital tools for direct assessment of autism risk during early childhood A systematic review.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2024_Mukherjee et al._Digital tools for direct assessment of autism risk during early childhood A systematic review.pdf:application/pdf},
}

@inproceedings{sharmin2018,
	address = {New York, NY, USA},
	series = {{CHI} '18},
	title = {From {Research} to {Practice}: {Informing} the {Design} of {Autism} {Support} {Smart} {Technology}},
	isbn = {978-1-4503-5620-6},
	shorttitle = {From {Research} to {Practice}},
	url = {https://dl.acm.org/doi/10.1145/3173574.3173676},
	doi = {10.1145/3173574.3173676},
	abstract = {Smart technologies (wearable and mobile devices) show tremendous potential in the detection, diagnosis, and management of Autism Spectrum Disorder (ASD) by enabling continuous real-time data collection, identifying effective treatment strategies, and supporting intervention design and delivery. Though promising, effective utilization of smart technology in aiding ASD is still limited. We propose a set of implications to guide the design of ASD-support technology by analyzing 149 peer-reviewed articles focused on children with autism from ACM Digital Library, IEEE Xplore, and PubMed. Our analysis reveals that technology should facilitate real-time detection and identification of points-of-interest, adapt its behavior driven by the real-time affective state of the user, utilize familiar and unfamiliar features depending on user-context, and aid in revealing even minuscule progress made by children with autism. Our findings indicate that such technology should strive to blend-in with everyday objects. Moreover, gradual exposure and desensitization may facilitate successful adaptation of novel technology.},
	urldate = {2025-02-14},
	booktitle = {Proceedings of the 2018 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sharmin, Moushumi and Hossain, Md Monsur and Saha, Abir and Das, Maitraye and Maxwell, Margot and Ahmed, Shameem},
	month = apr,
	year = {2018},
	pages = {1--16},
	file = {2018_Sharmin et al._From Research to Practice Informing the Design of Autism Support Smart Technology.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2018_Sharmin et al._From Research to Practice Informing the Design of Autism Support Smart Technology.pdf:application/pdf},
}

@article{robles2022,
	title = {A {Virtual} {Reality} {Based} {System} for the {Screening} and {Classification} of {Autism}},
	volume = {28},
	issn = {1941-0506},
	url = {https://ieeexplore.ieee.org/document/9714809},
	doi = {10.1109/TVCG.2022.3150489},
	abstract = {Autism - also known as Autism Spectrum Disorders or Autism Spectrum Conditions - is a neurodevelopmental condition characterized by repetitive behaviours and differences in communication and social interaction. As a consequence, many autistic individuals may struggle in everyday life, which sometimes manifests in depression, unemployment, or addiction. One crucial problem in patient support and treatment is the long waiting time to diagnosis, which was approximated to seven months on average. Yet, the earlier an intervention can take place the better the patient can be supported, which was identified as a crucial factor. We propose a system to support the screening of Autism Spectrum Disorders based on a virtual reality social interaction, namely a shopping experience, with an embodied agent. During this everyday interaction, behavioral responses are tracked and recorded. We analyze this behavior with machine learning approaches to classify participants from an autistic participant sample in comparison to a typically developed individuals control sample with high accuracy, demonstrating the feasibility of the approach. We believe that such tools can strongly impact the way mental disorders are assessed and may help to further find objective criteria and categorization.},
	number = {5},
	urldate = {2025-02-14},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Robles, Marta and Namdarian, Negar and Otto, Julia and Wassiljew, Evelyn and Navab, Nassir and Falter-Wagner, Christine M. and Roth, Daniel},
	month = may,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Virtual reality, Machine learning, Autism, autism, Three-dimensional displays, machine learning, Virtual environments, Avatars, diagnosis, Reliability, embodiment, agents, Tutorials},
	pages = {2168--2178},
	file = {2022_Robles et al._A Virtual Reality Based System for the Screening and Classification of Autism.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2022_Robles et al._A Virtual Reality Based System for the Screening and Classification of Autism.pdf:application/pdf},
}

@article{cabibihan2016,
	title = {Sensing {Technologies} for {Autism} {Spectrum} {Disorder} {Screening} and {Intervention}},
	volume = {17},
	copyright = {cc by},
	issn = {1424-8220},
	url = {https://europepmc.org/articles/PMC5298619},
	doi = {10.3390/s17010046},
	abstract = {This paper reviews the state-of-the-art in sensing technologies that are relevant for Autism Spectrum Disorder (ASD) screening and therapy. This disorder is characterized by difficulties in social communication, social interactions, and repetitive behaviors. It is diagnosed during the first three years of life. Early and intensive interventions have been shown to improve the developmental trajectory of the affected children. The earlier the diagnosis, the sooner the intervention therapy can begin, thus, making early diagnosis an important research goal. Technological innovations have tremendous potential to assist with early diagnosis and improve intervention programs. The need for careful and methodological evaluation of such emerging technologies becomes important in order to assist not only the therapists and clinicians in their selection of suitable tools, but to also guide the developers of the technologies in improving hardware and software. In this paper, we survey the literatures on sensing technologies for ASD and we categorize them into eye trackers, movement trackers, electrodermal activity monitors, tactile sensors, vocal prosody and speech detectors, and sleep quality assessment devices. We assess their effectiveness and study their limitations. We also examine the challenges faced by this growing field that need to be addressed before these technologies can perform up to their theoretical potential.},
	language = {eng},
	number = {1},
	urldate = {2025-02-14},
	journal = {Sensors (Basel, Switzerland)},
	author = {Cabibihan, John-John and Javed, Hifza and Aldosari, Mohammed and Frazier, Thomas W and Elbashir, Haitham},
	month = dec,
	year = {2016},
	pmid = {28036004},
	pmcid = {PMC5298619},
	keywords = {Autism Spectrum Disorder, Electrodermal Activity Monitors, Eye Trackers, Movement Trackers, Prosody And Speech Detectors, Sleep Quality Assessment, Social Robotics, Tactile Sensing},
	pages = {E46},
	file = {2016_Cabibihan et al._Sensing Technologies for Autism Spectrum Disorder Screening and Intervention.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2016_Cabibihan et al._Sensing Technologies for Autism Spectrum Disorder Screening and Intervention.pdf:application/pdf},
}

@inproceedings{hassan2024,
	title = {A {Machine} {Learning} {Approach} for {Early} {Prediction} of {Autism} {Spectrum} {Disorder} using {eXtreme} {Gradient} {Boosting} and {Logistic} {Regression} {Framework}},
	url = {https://ieeexplore.ieee.org/abstract/document/10796968},
	doi = {10.1109/SSITCON62437.2024.10796968},
	abstract = {Nowadays, Autism Spectrum Disorder (ASD) is a compound neurodevelopmental disorder that affects millions of persons universally, impacting their communication, communal interaction, and behavior. ASD is a diverse condition that exhibits differently in each individual, from trivial complications with social cues to severe challenges with parallel communication. ASD has become a demanding concern for healthcare professionals, and educationalists, highlighting the need for early detection, effective intervention, and wide-ranging support systems. As a result, an approach has proposed for predicting ASD at early stages, as the existing models have overfitting, class imbalance and more complexity problems. To overcome these issues a hybrid model of eXtreme Gradient Boosting and Logistic Regression (XGBoost-LR) has proposed, which involves with preprocessing of Synthetic Minority Oversampling Technique (SMOTE) and Standard Scalar Standardization to balance class and to improve interpretability. Next, feature extraction is performed with Recursive Feature Elimination (RFE) and cross validation using 5foldCV to reduce overfitting and improve generalizability. The XGBoost-LR model gives a precise rate in predicting and classifying the stages of ASD. The proposed XGBoost-LR model gives the better results than the other existing methods Support Vector Machine (SVM) in terms of metrices accuracy, precision, recall, specificity and f1 score respectively.},
	urldate = {2025-02-14},
	booktitle = {2024 {First} {International} {Conference} on {Software}, {Systems} and {Information} {Technology} ({SSITCON})},
	author = {Hassan, Muntather Muhsin and Laxmi, H. Bhagya and Sasikala, M. and Kumar, S. Senthil and Alamelu, M.},
	month = oct,
	year = {2024},
	keywords = {Autism, Standards, Feature extraction, Predictive models, Support vector machines, autism spectrum disorder, Biological system modeling, Boosting, early prediction, extreme gradient boosting-logistic regression, Logistic regression, Overfitting, recursive feature elimination, synthetic minority over-sampling technique, Transfer learning},
	pages = {1--5},
	file = {2024_Hassan et al._A Machine Learning Approach for Early Prediction of Autism Spectrum Disorder using eXtreme Gradient.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2024_Hassan et al._A Machine Learning Approach for Early Prediction of Autism Spectrum Disorder using eXtreme Gradient.pdf:application/pdf},
}

@article{milton2014,
	title = {Autistic expertise: {A} critical reflection on the production of knowledge in autism studies},
	volume = {18},
	issn = {1362-3613},
	shorttitle = {Autistic expertise},
	url = {https://doi.org/10.1177/1362361314525281},
	doi = {10.1177/1362361314525281},
	abstract = {The field of autism studies is a highly disputed territory within which competing contradictory discourses abound. In this field, it is the voices and claims of autistic people regarding their own expertise in knowledge production concerning autism that is most recent in the debate, and traditionally the least attended to. In this article, I utilise the theories of Harry Collins and colleagues in order to reflect upon and conceptualise the various claims to knowledge production and expertise within the field of autism studies, from the perspective of an author who has been diagnosed as being on the autism spectrum. The notion that autistic people lack sociality is problematised, with the suggestion that autistic people are not well described by notions such as the ‘social brain’, or as possessing ‘zero degrees of cognitive empathy’. I then argue, however, that there is a qualitative difference in autistic sociality, and question to what extent such differences are of a biological or cultural nature, and to what extent interactional expertise can be gained by both parties in interactions between autistic and non-autistic people. In conclusion, I argue that autistic people have often become distrustful of researchers and their aims, and are frequently frozen out of the processes of knowledge production. Such a context results in a negative feedback spiral with further damage to the growth of interactional expertise between researchers and autistic people, and a breakdown in trust and communication leading to an increase in tension between stakeholder groups. The involvement of autistic scholars in research and improvements in participatory methods can thus be seen as a requirement, if social research in the field of autism is to claim ethical and epistemological integrity.},
	language = {en},
	number = {7},
	urldate = {2025-02-15},
	journal = {Autism},
	author = {Milton, Damian EM},
	month = oct,
	year = {2014},
	note = {Publisher: SAGE Publications Ltd},
	pages = {794--802},
	file = {2014_Milton_Autistic expertise A critical reflection on the production of knowledge in autism studies.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\FNSAutism\\2014_Milton_Autistic expertise A critical reflection on the production of knowledge in autism studies.pdf:application/pdf},
}

@article{molloy2022,
	title = {Can stratification biomarkers address the heterogeneity of autism spectrum disorder?},
	volume = {39},
	issn = {0790-9667, 2051-6967},
	url = {https://www.cambridge.org/core/journals/irish-journal-of-psychological-medicine/article/can-stratification-biomarkers-address-the-heterogeneity-of-autism-spectrum-disorder/673518A0ACC4FB64B8B5248E2051673A},
	doi = {10.1017/ipm.2021.73},
	abstract = {The search for biomarkers for autism spectrum disorder (henceforth autism) has received a lot of attention due to their potential clinical relevance. The clinical and aetiological heterogeneity of autism suggests the presence of subgroups. The lack of identification of a valid diagnostic biomarker for autism, and the inconsistencies seen in studies assessing differences between autism and typically developing control groups, may be partially explained by the vast heterogeneity observed in autism. The focus now is to better understand the clinical and biological heterogeneity and identify stratification biomarkers, which are measures that describe subgroups of individuals with shared biology. Using stratification approaches to assess treatment within pre-defined subgroups could clarify who may benefit from different treatments and therapies, and ultimately lead to more effective individualised treatment plans.},
	language = {en},
	number = {3},
	urldate = {2025-02-15},
	journal = {Irish Journal of Psychological Medicine},
	author = {Molloy, C. J. and Gallagher, L.},
	month = sep,
	year = {2022},
	keywords = {autism, Autism spectrum disorder, biomarkers, stratification},
	pages = {305--311},
	file = {2022_Molloy and Gallagher_Can stratification biomarkers address the heterogeneity of autism spectrum disorder.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\2022_Molloy and Gallagher_Can stratification biomarkers address the heterogeneity of autism spectrum disorder.pdf:application/pdf},
}

@article{gallagher2022,
	title = {Autism spectrum disorders: current issues and future directions},
	volume = {39},
	issn = {0790-9667, 2051-6967},
	shorttitle = {Autism spectrum disorders},
	url = {https://www.cambridge.org/core/journals/irish-journal-of-psychological-medicine/article/autism-spectrum-disorders-current-issues-and-future-directions/ED00F8E8FA48910CD814968A9866DE46},
	doi = {10.1017/ipm.2022.34},
	abstract = {This edition of Irish Journal of Psychological Medicine is a Special Themed Issue on Autism Spectrum Disorders (ASD). Mental health services are not currently meeting the needs of autistic people across the lifespan. We have limited evidence based treatments for core symptoms and comorbidities and there is lack of awareness and under-recognition of ASD, particularly in adults and certain groups of individuals. The key themes in this edition focus on challenges with recognition and diagnosis and address these from both clinical and research perspectives. Co-occurring conditions also feature, which are also under-recognised and can contribute to less optimal outcomes. New and existing research developments in stratification for clinical trials and neuroimaging are also discussed. We hope this Issue highlights relevant current issues in ASD, and provides insights which can help address the challenges in providing evidence based pathways to better meet the needs of autistic people into the future.},
	language = {en},
	number = {3},
	urldate = {2025-02-15},
	journal = {Irish Journal of Psychological Medicine},
	author = {Gallagher, L. and McGrath, J.},
	month = sep,
	year = {2022},
	keywords = {Autism spectrum disorders, clinical services, mental health, research},
	pages = {237--239},
	file = {2022_Gallagher and McGrath_Autism spectrum disorders current issues and future directions.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2022_Gallagher and McGrath_Autism spectrum disorders current issues and future directions.pdf:application/pdf},
}

@article{wolfers2019,
	title = {From pattern classification to stratification: towards conceptualizing the heterogeneity of {Autism} {Spectrum} {Disorder}},
	volume = {104},
	issn = {0149-7634},
	shorttitle = {From pattern classification to stratification},
	url = {https://www.sciencedirect.com/science/article/pii/S0149763419303197},
	doi = {10.1016/j.neubiorev.2019.07.010},
	abstract = {Pattern classification and stratification approaches have increasingly been used in research on Autism Spectrum Disorder (ASD) over the last ten years with the goal of translation towards clinical applicability. Here, we present an extensive scoping literature review on those two approaches. We screened a total of 635 studies, of which 57 pattern classification and 19 stratification studies were included. We observed large variance across pattern classification studies in terms of predictive performance from about 60\% to 98\% accuracy, which is among other factors likely linked to sampling bias, different validation procedures across studies, the heterogeneity of ASD and differences in data quality. Stratification studies were less prevalent with only two studies reporting replications and just a few showing external validation. While some identified strata based on cognition and intelligence reappear across studies, biology as a stratification marker is clearly underexplored. In summary, mapping biological differences at the level of the individual with ASD is a major challenge for the field now. Conceptualizing those mappings and individual trajectories that lead to the diagnosis of ASD, will become a major challenge in the near future.},
	urldate = {2025-02-15},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Wolfers, Thomas and Floris, Dorothea L. and Dinga, Richard and van Rooij, Daan and Isakoglou, Christina and Kia, Seyed Mostafa and Zabihi, Mariam and Llera, Alberto and Chowdanayaka, Rajanikanth and Kumar, Vinod J. and Peng, Han and Laidi, Charles and Batalle, Dafnis and Dimitrova, Ralica and Charman, Tony and Loth, Eva and Lai, Meng-Chuan and Jones, Emily and Baumeister, Sarah and Moessnang, Carolin and Banaschewski, Tobias and Ecker, Christine and Dumas, Guillaume and O’Muircheartaigh, Jonathan and Murphy, Declan and Buitelaar, Jan K. and Marquand, Andre F. and Beckmann, Christian F.},
	month = sep,
	year = {2019},
	keywords = {Machine learning, Classification, Autism spectrum disorder, Precision medicine, Biotypes, Clustering, Pattern recognition, Stratification},
	pages = {240--254},
	file = {2019_Wolfers et al._From pattern classification to stratification towards conceptualizing the heterogeneity of Autism S.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2019_Wolfers et al._From pattern classification to stratification towards conceptualizing the heterogeneity of Autism S.pdf:application/pdf},
}

@incollection{meridianmcdonald2023,
	address = {Cham},
	title = {Autism and {Neurodiversity}},
	isbn = {978-3-031-42383-3},
	url = {https://doi.org/10.1007/978-3-031-42383-3_15},
	abstract = {Autism may be a single diagnosis, but the characteristics of autistic people are highly heterogeneous. These heterogeneous characteristics include phenotypic traits, predictive factors, and biomarkers that contribute to the neurodiversity of autism. This neurodiversity transcends biological diversity to include a variety of medical, social, ecological, and Indigenous models (and their combinations) of disability and difference. However, a unifying theoretical model of autism, the Broader Autism Phenotype Constellations-Disability Matrix Paradigm, may resolve tensions between these models of disability and controversies of autism. These challenges could also be supported by biological studies of autism that include broader perspectives and topics, as well as enhanced communication and resources.},
	language = {en},
	urldate = {2025-02-15},
	booktitle = {Neurobiology of {Autism} {Spectrum} {Disorders}},
	publisher = {Springer International Publishing},
	author = {Meridian McDonald, T. A.},
	editor = {El Idrissi, Abdeslem and McCloskey, Dan},
	year = {2023},
	doi = {10.1007/978-3-031-42383-3_15},
	pages = {313--332},
}

@article{loth2017,
	title = {The {EU}-{AIMS} {Longitudinal} {European} {Autism} {Project} ({LEAP}): design and methodologies to identify and validate stratification biomarkers for autism spectrum disorders},
	volume = {8},
	issn = {2040-2392},
	shorttitle = {The {EU}-{AIMS} {Longitudinal} {European} {Autism} {Project} ({LEAP})},
	url = {https://doi.org/10.1186/s13229-017-0146-8},
	doi = {10.1186/s13229-017-0146-8},
	abstract = {The tremendous clinical and aetiological diversity among individuals with autism spectrum disorder (ASD) has been a major obstacle to the development of new treatments, as many may only be effective in particular subgroups. Precision medicine approaches aim to overcome this challenge by combining pathophysiologically based treatments with stratification biomarkers that predict which treatment may be most beneficial for particular individuals. However, so far, we have no single validated stratification biomarker for ASD. This may be due to the fact that most research studies primarily have focused on the identification of mean case-control differences, rather than within-group variability, and included small samples that were underpowered for stratification approaches. The EU-AIMS Longitudinal European Autism Project (LEAP) is to date the largest multi-centre, multi-disciplinary observational study worldwide that aims to identify and validate stratification biomarkers for ASD.},
	language = {en},
	number = {1},
	urldate = {2025-02-15},
	journal = {Molecular Autism},
	author = {Loth, Eva and Charman, Tony and Mason, Luke and Tillmann, Julian and Jones, Emily J. H. and Wooldridge, Caroline and Ahmad, Jumana and Auyeung, Bonnie and Brogna, Claudia and Ambrosino, Sara and Banaschewski, Tobias and Baron-Cohen, Simon and Baumeister, Sarah and Beckmann, Christian and Brammer, Michael and Brandeis, Daniel and Bölte, Sven and Bourgeron, Thomas and Bours, Carsten and de Bruijn, Yvette and Chakrabarti, Bhismadev and Crawley, Daisy and Cornelissen, Ineke and Acqua, Flavio Dell’ and Dumas, Guillaume and Durston, Sarah and Ecker, Christine and Faulkner, Jessica and Frouin, Vincent and Garces, Pilar and Goyard, David and Hayward, Hannah and Ham, Lindsay M. and Hipp, Joerg and Holt, Rosemary J. and Johnson, Mark H. and Isaksson, Johan and Kundu, Prantik and Lai, Meng-Chuan and D’ardhuy, Xavier Liogier and Lombardo, Michael V. and Lythgoe, David J. and Mandl, René and Meyer-Lindenberg, Andreas and Moessnang, Carolin and Mueller, Nico and O’Dwyer, Laurence and Oldehinkel, Marianne and Oranje, Bob and Pandina, Gahan and Persico, Antonio M. and Ruigrok, Amber N. V. and Ruggeri, Barbara and Sabet, Jessica and Sacco, Roberto and Cáceres, Antonia San José and Simonoff, Emily and Toro, Roberto and Tost, Heike and Waldman, Jack and Williams, Steve C. R. and Zwiers, Marcel P. and Spooren, Will and Murphy, Declan G. M. and Buitelaar, Jan K.},
	month = jun,
	year = {2017},
	keywords = {Eye-tracking, EEG, Neuroimaging, MRI, Cognition, Biomarkers, stratification, Genetics},
	pages = {24},
	file = {2017_Loth et al._The EU-AIMS Longitudinal European Autism Project (LEAP) design and methodologies to identify and va.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2017_Loth et al._The EU-AIMS Longitudinal European Autism Project (LEAP) design and methodologies to identify and va.pdf:application/pdf},
}

@article{chantiluke2014,
	title = {Disorder-specific functional abnormalities during temporal discounting in youth with {Attention} {Deficit} {Hyperactivity} {Disorder} ({ADHD}), {Autism} and comorbid {ADHD} and {Autism}},
	volume = {223},
	issn = {0925-4927},
	url = {https://www.sciencedirect.com/science/article/pii/S0925492714000936},
	doi = {10.1016/j.pscychresns.2014.04.006},
	abstract = {Attention Deficit Hyperactivity Disorder (ADHD) and Autism Spectrum Disorder (ASD) are often comorbid and share cognitive abnormalities in temporal foresight. A key question is whether shared cognitive phenotypes are based on common or different underlying pathophysiologies and whether comorbid patients have additive neurofunctional deficits, resemble one of the disorders or have a different pathophysiology. We compared age- and IQ-matched boys with non-comorbid ADHD (18), non-comorbid ASD (15), comorbid ADHD and ASD (13) and healthy controls (18) using functional magnetic resonance imaging (fMRI) during a temporal discounting task. Only the ASD and the comorbid groups discounted delayed rewards more steeply. The fMRI data showed both shared and disorder-specific abnormalities in the three groups relative to controls in their brain-behaviour associations. The comorbid group showed both unique and more severe brain-discounting associations than controls and the non-comorbid patient groups in temporal discounting areas of ventromedial and lateral prefrontal cortex, ventral striatum and anterior cingulate, suggesting that comorbidity is neither an endophenocopy of the two pure disorders nor an additive pathology.},
	number = {2},
	urldate = {2025-02-15},
	journal = {Psychiatry Research: Neuroimaging},
	author = {Chantiluke, Kaylita and Christakou, Anastasia and Murphy, Clodagh M. and Giampietro, Vincent and Daly, Eileen M. and Ecker, Christina and Brammer, Michael and Murphy, Declan G. and Rubia, Katya},
	month = aug,
	year = {2014},
	keywords = {Autism, ASD, ADHD, fMRI, Temporal discounting},
	pages = {113--120},
	file = {2014_Chantiluke et al._Disorder-specific functional abnormalities during temporal discounting in youth with Attention Defic.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2014_Chantiluke et al._Disorder-specific functional abnormalities during temporal discounting in youth with Attention Defic.pdf:application/pdf},
}

@article{mihailov2020,
	title = {Cortical signatures in behaviorally clustered autistic traits subgroups: a population-based study},
	volume = {10},
	issn = {2158-3188},
	shorttitle = {Cortical signatures in behaviorally clustered autistic traits subgroups},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7320967/},
	doi = {10.1038/s41398-020-00894-3},
	abstract = {Extensive heterogeneity in autism spectrum disorder (ASD) has hindered the characterization of consistent biomarkers, which has led to widespread negative results. Isolating homogenized subtypes could provide insight into underlying biological mechanisms and an overall better understanding of ASD. A total of 1093 participants from the population-based “Healthy Brain Network” cohort (Child Mind Institute in the New York City area, USA) were selected based on score availability in behaviors relevant to ASD, aged 6–18 and IQ {\textgreater}= 70. All participants underwent an unsupervised clustering analysis on behavioral dimensions to reveal subgroups with ASD traits, identified by the presence of social deficits. Analysis revealed three socially impaired ASD traits subgroups: (1) high in emotionally dysfunctional traits, (2) high in ADHD-like traits, and (3) high in anxiety and depressive symptoms. 527 subjects had good quality structural MRI T1 data. Site effects on cortical features were adjusted using the ComBat method. Neuroimaging analyses compared cortical thickness, gyrification, and surface area, and were controlled for age, gender, and IQ, and corrected for multiple comparisons. Structural neuroimaging analyses contrasting one combined heterogeneous ASD traits group against controls did not yield any significant differences. Unique cortical signatures, however, were observed within each of the three individual ASD traits subgroups versus controls. These observations provide evidence of ASD traits subtypes, and confirm the necessity of applying dimensional approaches to extract meaningful differences, thus reducing heterogeneity and paving the way to better understanding ASD traits.},
	urldate = {2025-02-17},
	journal = {Translational Psychiatry},
	author = {Mihailov, Angeline and Philippe, Cathy and Gloaguen, Arnaud and Grigis, Antoine and Laidi, Charles and Piguet, Camille and Houenou, Josselin and Frouin, Vincent},
	month = jun,
	year = {2020},
	pmid = {32594096},
	pmcid = {PMC7320967},
	pages = {207},
	file = {2020_Mihailov et al._Cortical signatures in behaviorally clustered autistic traits subgroups a population-based study.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2020_Mihailov et al._Cortical signatures in behaviorally clustered autistic traits subgroups a population-based study.pdf:application/pdf},
}

@article{grzadzinski2016,
	title = {Measuring {Changes} in {Social} {Communication} {Behaviors}: {Preliminary} {Development} of the {Brief} {Observation} of {Social} {Communication} {Change} ({BOSCC})},
	volume = {46},
	issn = {1573-3432},
	shorttitle = {Measuring {Changes} in {Social} {Communication} {Behaviors}},
	url = {https://doi.org/10.1007/s10803-016-2782-9},
	doi = {10.1007/s10803-016-2782-9},
	abstract = {Psychometric properties and initial validity of the Brief Observation of Social Communication Change (BOSCC), a measure of treatment-response for social-communication behaviors, are described. The BOSCC coding scheme is applied to 177 video observations of 56 young children with ASD and minimal language abilities. The BOSCC has high to excellent inter-rater and test–retest reliability and shows convergent validity with measures of language and communication skills. The BOSCC Core total demonstrates statistically significant amounts of change over time compared to a no change alternative while the ADOS CSS over the same period of time did not. This work is a first step in the development of a novel outcome measure for social-communication behaviors with applications to clinical trials and longitudinal studies.},
	language = {en},
	number = {7},
	urldate = {2025-02-17},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Grzadzinski, Rebecca and Carr, Themba and Colombi, Costanza and McGuire, Kelly and Dufek, Sarah and Pickles, Andrew and Lord, Catherine},
	month = jul,
	year = {2016},
	keywords = {Autism, Autism spectrum disorder (ASD), Social communication, Autism Diagnostic Observation Schedule (ADOS), Brief Observation of Social Communication Change (BOSCC), Preschoolers, Restricted and Repetitive Behaviors and Interests (RRB), Toddlers},
	pages = {2464--2479},
	file = {2016_Grzadzinski et al._Measuring Changes in Social Communication Behaviors Preliminary Development of the Brief Observatio.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2016_Grzadzinski et al._Measuring Changes in Social Communication Behaviors Preliminary Development of the Brief Observatio.pdf:application/pdf},
}

@article{liu2024,
	title = {Self-{Guided} {DMT}: {Exploring} a {Novel} {Paradigm} of {Dance} {Movement} {Therapy} in {Mixed} {Reality} for {Children} with {ASD}},
	volume = {30},
	issn = {1941-0506},
	shorttitle = {Self-{Guided} {DMT}},
	url = {https://ieeexplore.ieee.org/abstract/document/10463763?casa_token=i8gpa-BKw9kAAAAA:uZhWk5ZpLkLbE6ZufBkez7BqpCB83buMTfnlqOWIvuNG8CQH7_YDJPyqkVWoSwqaqGSjXrdAJw},
	doi = {10.1109/TVCG.2024.3372063},
	abstract = {Children diagnosed with Autism Spectrum Disorder (ASD) often exhibit motor disorders. Dance Movement Therapy (DMT) has shown great potential for improving the motor control ability of children with ASD. However, traditional DMT methods often lack vividness and are difficult to implement effectively. To address this issue, we propose a Mixed Reality DMT approach, utilizing interactive virtual agents. This approach offers immersive training content and multi-sensory feedback. To improve the training performance of children with ASD, we introduce a novel training paradigm featuring a self-guided mode. This paradigm enables the rapid creation of a virtual twin agent of the child with ASD using a single photo to embody oneself, which can then guide oneself during training. We conducted an experiment with the participation of 24 children diagnosed with ASD (or ASD propensity), recording their training performance under various experimental conditions. Through expert rating, behavior coding of training sessions, and statistical analysis, our findings revealed that the use of the twin agent for self-guidance resulted in noticeable improvements in the training performance of children with ASD. These improvements were particularly evident in terms of enhancing movement quality and refining overall target-related responses. Our study holds clinical potential in the field of medical treatment and rehabilitation for children with ASD.},
	number = {5},
	urldate = {2025-02-17},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Liu, Weiying and Zhang, Yanyan and Zhang, Baiqiao and Xiong, Qianqian and Zhao, Hong and Li, Sheng and Liu, Juan and Bian, Yulong},
	month = may,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {mixed reality, Virtual reality, Solid modeling, Autism spectrum disorder, Training, Avatars, Pediatrics, dance movement therapy, Medical treatment, self-guided, Variable speed drives, virtual agent},
	pages = {2119--2128},
	file = {2024_Liu et al._Self-Guided DMT Exploring a Novel Paradigm of Dance Movement Therapy in Mixed Reality for Children.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Liu et al._Self-Guided DMT Exploring a Novel Paradigm of Dance Movement Therapy in Mixed Reality for Children.pdf:application/pdf},
}

@article{cerasuolo2025,
	title = {The {Potential} of {Virtual} {Reality} to {Improve} {Diagnostic} {Assessment} by {Boosting} {Autism} {Spectrum} {Disorder} {Traits}: {A} {Systematic} {Review}},
	volume = {9},
	issn = {2366-7540},
	shorttitle = {The {Potential} of {Virtual} {Reality} to {Improve} {Diagnostic} {Assessment} by {Boosting} {Autism} {Spectrum} {Disorder} {Traits}},
	url = {https://doi.org/10.1007/s41252-024-00413-1},
	doi = {10.1007/s41252-024-00413-1},
	abstract = {While studies examining the effectiveness of virtual reality (VR) systems in autism spectrum disorder (ASD) intervention have seen significant growth, research on their application as tools to improve assessment and diagnosis remains limited. This systematic review explores the potential of VR systems in speeding-up and enhancing the assessment process for ASD.},
	language = {en},
	number = {1},
	urldate = {2025-02-17},
	journal = {Advances in Neurodevelopmental Disorders},
	author = {Cerasuolo, Mariangela and De Marco, Stefania and Nappo, Raffaele and Simeoli, Roberta and Rega, Angelo},
	month = mar,
	year = {2025},
	keywords = {Virtual reality, Machine learning, Autism spectrum disorder, Diagnosis, Assessment},
	pages = {1--22},
	file = {2025_Cerasuolo et al._The Potential of Virtual Reality to Improve Diagnostic Assessment by Boosting Autism Spectrum Disord.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2025_Cerasuolo et al._The Potential of Virtual Reality to Improve Diagnostic Assessment by Boosting Autism Spectrum Disord.pdf:application/pdf},
}

@article{minissi2024,
	title = {Biosignal comparison for autism assessment using machine learning models and virtual reality},
	volume = {171},
	issn = {0010-4825},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482524002786},
	doi = {10.1016/j.compbiomed.2024.108194},
	abstract = {Clinical assessment procedures encounter challenges in terms of objectivity because they rely on subjective data. Computational psychiatry proposes overcoming this limitation by introducing biosignal-based assessments able to detect clinical biomarkers, while virtual reality (VR) can offer ecological settings for measurement. Autism spectrum disorder (ASD) is a neurodevelopmental disorder where many biosignals have been tested to improve assessment procedures. However, in ASD research there is a lack of studies systematically comparing biosignals for the automatic classification of ASD when recorded simultaneously in ecological settings, and comparisons among previous studies are challenging due to methodological inconsistencies. In this study, we examined a VR screening tool consisting of four virtual scenes, and we compared machine learning models based on implicit (motor skills and eye movements) and explicit (behavioral responses) biosignals. Machine learning models were developed for each biosignal within the virtual scenes and then combined into a final model per biosignal. A linear support vector classifier with recursive feature elimination was used and tested using nested cross-validation. The final model based on motor skills exhibited the highest robustness in identifying ASD, achieving an AUC of 0.89 (SD = 0.08). The best behavioral model showed an AUC of 0.80, while further research is needed for the eye-movement models due to limitations with the eye-tracking glasses. These findings highlight the potential of motor skills in enhancing objectivity and reliability in the early assessment of ASD compared to other biosignals.},
	urldate = {2025-02-17},
	journal = {Computers in Biology and Medicine},
	author = {Minissi, Maria Eleonora and Altozano, Alberto and Marín-Morales, Javier and Chicchi Giglioli, Irene Alice and Mantovani, Fabrizia and Alcañiz, Mariano},
	month = mar,
	year = {2024},
	keywords = {Virtual reality, Autism spectrum disorder, Eye movements, Biosignal, Motor skills, Statistical machine learning},
	pages = {108194},
	file = {2024_Minissi et al._Biosignal comparison for autism assessment using machine learning models and virtual reality.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Minissi et al._Biosignal comparison for autism assessment using machine learning models and virtual reality.pdf:application/pdf},
}

@article{fears2023,
	title = {Autistic {Children} {Use} {Less} {Efficient} {Goal}-{Directed} {Whole} {Body} {Movements} {Compared} to {Neurotypical} {Development}},
	volume = {53},
	issn = {1573-3432},
	url = {https://doi.org/10.1007/s10803-022-05523-0},
	doi = {10.1007/s10803-022-05523-0},
	abstract = {Autistic children have differences in their movements which impact their functional performance. Virtual-reality enables researchers to study movement in safe, engaging environments. We used motion-capture to measure how 7–13-year-old autistic and neurotypical children make whole-body movements in a virtual-reality task. Although children in both groups were successful, we observed differences in their movements. Autistic children were less efficient moving to the target. Autistic children did not appear to use a movement strategy. While neurotypical children were more likely to overshoot near targets and undershoot far targets, autistic children did not modulate their strategy. Using kinematic data from tasks in virtual-reality, we can begin to understand the pattern of movement challenges experienced by autistic children.},
	language = {en},
	number = {7},
	urldate = {2025-02-17},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Fears, Nicholas E. and Templin, Tylan N. and Sherrod, Gabriela M. and Bugnariu, Nicoleta L. and Patterson, Rita M. and Miller, Haylie L.},
	month = jul,
	year = {2023},
	keywords = {Virtual reality, Autism spectrum disorder, Neurodevelopmental Disorders, Postural control, Movement, Motor skills, Balance, Kinematics},
	pages = {2806--2817},
	file = {2023_Fears et al._Autistic Children Use Less Efficient Goal-Directed Whole Body Movements Compared to Neurotypical Dev.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2023_Fears et al._Autistic Children Use Less Efficient Goal-Directed Whole Body Movements Compared to Neurotypical Dev.pdf:application/pdf},
}

@article{miller2023,
	title = {Movement smoothness during dynamic postural control to a static target differs between autistic and neurotypical children},
	volume = {99},
	issn = {0966-6362},
	url = {https://www.sciencedirect.com/science/article/pii/S096663622200618X},
	doi = {10.1016/j.gaitpost.2022.10.015},
	abstract = {Background
Autistic children and adults have known differences in motor performance, including postural instability and atypical gross motor control. Few studies have specifically tested dynamic postural control. This is the first study to quantify movement smoothness and its relationship to task performance during lateral dynamic postural control tasks in autism.
Research question
We sought to test the hypothesis that autistic children would have less smooth movements to lateral static targets compared to neurotypical children, and that this difference would relate to specific movement strategies.
Methods
We used camera-based motion-capture to measure spatiotemporal characteristics of lateral movement of a marker placed on the C7 vertebrae, and of markers comprising trunk and pelvis segments during a dynamic postural movements to near and far targets administered in an immersive virtual environment. We tested a sample of 15 autistic children and 11 age-matched neurotypical children. We quantified movement smoothness using log dimensionless jerk.
Results
Autistic children exhibited more medial-lateral pelvic position range of motion compared to neurotypical children, and used a stepping strategy more often compared to neurotypical children. Autistic children also had higher log dimensionless jerk than neurotypical children for motion of the C7 marker. All participants had higher log dimensionless jerk for far targets than for near targets. Autistic children had longer trial durations than neurotypical children, and younger children had longer trial durations than older children across diagnostic groups.
Significance
The stepping strategy observed more often in the autistic group likely contributed to log dimensionless jerk and reduced movement smoothness. This strategy is indicative of either an attempt to prevent an impending loss of balance, or an attempt to compensate for and recover from a loss of balance once it is detected.},
	urldate = {2025-02-17},
	journal = {Gait \& Posture},
	author = {Miller, Haylie L. and Templin, Tylan N. and Fears, Nicholas E. and Sherrod, Gabriela M. and Patterson, Rita M. and Bugnariu, Nicoleta L.},
	month = jan,
	year = {2023},
	keywords = {Virtual reality, Autism, Postural control, Movement, Motor skills, Kinematics},
	pages = {76--82},
	file = {2023_Miller et al._Movement smoothness during dynamic postural control to a static target differs between autistic and.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\2023_Miller et al._Movement smoothness during dynamic postural control to a static target differs between autistic and.pdf:application/pdf},
}

@article{alcaniz2022,
	title = {Eye gaze as a biomarker in the recognition of autism spectrum disorder using virtual reality and machine learning: {A} proof of concept for diagnosis},
	volume = {15},
	copyright = {© 2021 The Authors. Autism Research published by International Society for Autism Research and Wiley Periodicals LLC.},
	issn = {1939-3806},
	shorttitle = {Eye gaze as a biomarker in the recognition of autism spectrum disorder using virtual reality and machine learning},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aur.2636},
	doi = {10.1002/aur.2636},
	abstract = {The core symptoms of autism spectrum disorder (ASD) mainly relate to social communication and interactions. ASD assessment involves expert observations in neutral settings, which introduces limitations and biases related to lack of objectivity and does not capture performance in real-world settings. To overcome these limitations, advances in technologies (e.g., virtual reality) and sensors (e.g., eye-tracking tools) have been used to create realistic simulated environments and track eye movements, enriching assessments with more objective data than can be obtained via traditional measures. This study aimed to distinguish between autistic and typically developing children using visual attention behaviors through an eye-tracking paradigm in a virtual environment as a measure of attunement to and extraction of socially relevant information. The 55 children participated. Autistic children presented a higher number of frames, both overall and per scenario, and showed higher visual preferences for adults over children, as well as specific preferences for adults' rather than children's faces on which looked more at bodies. A set of multivariate supervised machine learning models were developed using recursive feature selection to recognize ASD based on extracted eye gaze features. The models achieved up to 86\% accuracy (sensitivity = 91\%) in recognizing autistic children. Our results should be taken as preliminary due to the relatively small sample size and the lack of an external replication dataset. However, to our knowledge, this constitutes a first proof of concept in the combined use of virtual reality, eye-tracking tools, and machine learning for ASD recognition. Lay Summary Core symptoms in children with ASD involve social communication and interaction. ASD assessment includes expert observations in neutral settings, which show limitations and biases related to lack of objectivity and do not capture performance in real settings. To overcome these limitations, this work aimed to distinguish between autistic and typically developing children in visual attention behaviors through an eye-tracking paradigm in a virtual environment as a measure of attunement to, and extraction of, socially relevant information.},
	language = {en},
	number = {1},
	urldate = {2025-02-17},
	journal = {Autism Research},
	author = {Alcañiz, Mariano and Chicchi-Giglioli, Irene Alice and Carrasco-Ribelles, Lucía A. and Marín-Morales, Javier and Minissi, Maria Eleonora and Teruel-García, Gonzalo and Sirera, Marian and Abad, Luis},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/aur.2636},
	keywords = {virtual reality, eye tracking, machine learning, autism spectrum disorder, behavioral biomarker, multivariate supervised learning},
	pages = {131--145},
	file = {2022_Alcañiz et al._Eye gaze as a biomarker in the recognition of autism spectrum disorder using virtual reality and mac.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\2022_Alcañiz et al._Eye gaze as a biomarker in the recognition of autism spectrum disorder using virtual reality and mac.pdf:application/pdf},
}

@article{arthur2021,
	title = {An examination of active inference in autistic adults using immersive virtual reality},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-99864-y},
	doi = {10.1038/s41598-021-99864-y},
	abstract = {The integration of prior expectations, sensory information, and environmental volatility is proposed to be atypical in Autism Spectrum Disorder, yet few studies have tested these predictive processes in active movement tasks. To address this gap in the research, we used an immersive virtual-reality racquetball paradigm to explore how visual sampling behaviours and movement kinematics are adjusted in relation to unexpected, uncertain, and volatile changes in environmental statistics. We found that prior expectations concerning ball ‘bounciness’ affected sensorimotor control in both autistic and neurotypical participants, with all individuals using prediction-driven gaze strategies to track the virtual ball. However, autistic participants showed substantial differences in visuomotor behaviour when environmental conditions were more volatile. Specifically, uncertainty-related performance difficulties in these conditions were accompanied by atypical movement kinematics and visual sampling responses. Results support proposals that autistic people overestimate the volatility of sensory environments, and suggest that context-sensitive differences in active inference could explain a range of movement-related difficulties in autism.},
	language = {en},
	number = {1},
	urldate = {2025-02-17},
	journal = {Scientific Reports},
	author = {Arthur, Tom and Harris, David and Buckingham, Gavin and Brosnan, Mark and Wilson, Mark and Williams, Genevieve and Vine, Sam},
	month = oct,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Autism spectrum disorders, Motor control, Oculomotor system, Sensorimotor processing},
	pages = {20377},
	file = {2021_Arthur et al._An examination of active inference in autistic adults using immersive virtual reality.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2021_Arthur et al._An examination of active inference in autistic adults using immersive virtual reality.pdf:application/pdf},
}

@article{alcanizraya2020,
	title = {Application of {Supervised} {Machine} {Learning} for {Behavioral} {Biomarkers} of {Autism} {Spectrum} {Disorder} {Based} on {Electrodermal} {Activity} and {Virtual} {Reality}},
	volume = {14},
	issn = {1662-5161},
	url = {https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2020.00090/full},
	doi = {10.3389/fnhum.2020.00090},
	abstract = {{\textless}sec{\textgreater}{\textless}title{\textgreater}Objective{\textless}/title{\textgreater}{\textless}p{\textgreater}Sensory processing is the ability to capture, elaborate, and integrate information through the five senses and is impaired in over 90\% of children with autism spectrum disorder (ASD). The ASD population shows hyper–hypo sensitiveness to sensory stimuli that can generate alteration in information processing, affecting cognitive and social responses to daily life situations. Structured and semi-structured interviews are generally used for ASD assessment, and the evaluation relies on the examiner’s subjectivity and expertise, which can lead to misleading outcomes. Recently, there has been a growing need for more objective, reliable, and valid diagnostic measures, such as biomarkers, to distinguish typical from atypical functioning and to reliably track the progression of the illness, helping to diagnose ASD. Implicit measures and ecological valid settings have been showing high accuracy on predicting outcomes and correctly classifying populations in categories.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Methods{\textless}/title{\textgreater}{\textless}p{\textgreater}Two experiments investigated whether sensory processing can discriminate between ASD and typical development (TD) populations using electrodermal activity (EDA) in two multimodal virtual environments (VE): forest VE and city VE. In the first experiment, 24 children with ASD diagnosis and 30 TDs participated in both virtual experiences, and changes in EDA have been recorded before and during the presentation of visual, auditive, and olfactive stimuli. In the second experiment, 40 children have been added to test the model of experiment 1.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Results{\textless}/title{\textgreater}{\textless}p{\textgreater}The first exploratory results on EDA comparison models showed that the integration of visual, auditive, and olfactive stimuli in the forest environment provided higher accuracy (90.3\%) on sensory dysfunction discrimination than specific stimuli. In the second experiment, 92 subjects experienced the forest VE, and results on 72 subjects showed that stimuli integration achieved an accuracy of 83.33\%. The final confirmatory test set ({\textless}italic{\textgreater}n{\textless}/italic{\textgreater} = 20) achieved 85\% accuracy, simulating a real application of the models. Further relevant result concerns the visual stimuli condition in the first experiment, which achieved 84.6\% of accuracy in recognizing ASD sensory dysfunction.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Conclusion{\textless}/title{\textgreater}{\textless}p{\textgreater}According to our studies’ results, implicit measures, such as EDA, and ecological valid settings can represent valid quantitative methods, along with traditional assessment measures, to classify ASD population, enhancing knowledge on the development of relevant specific treatments.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}},
	language = {English},
	urldate = {2025-02-17},
	journal = {Frontiers in Human Neuroscience},
	author = {Alcañiz Raya, Mariano and Chicchi Giglioli, Irene Alice and Marín-Morales, Javier and Higuera-Trujillo, Juan L. and Olmos, Elena and Minissi, Maria E. and Teruel Garcia, Gonzalo and Sirera, Marian and Abad, Luis},
	month = apr,
	year = {2020},
	note = {Publisher: Frontiers},
	keywords = {virtual reality, Autism Spectrum Disorder, assessment, Electro dermal activity, sensorial dysfunctions},
	file = {2020_Alcañiz Raya et al._Application of Supervised Machine Learning for Behavioral Biomarkers of Autism Spectrum Disorder Bas.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2020_Alcañiz Raya et al._Application of Supervised Machine Learning for Behavioral Biomarkers of Autism Spectrum Disorder Bas.pdf:application/pdf},
}

@article{koirala2021,
	title = {A {Preliminary} {Exploration} of {Virtual} {Reality}-{Based} {Visual} and {Touch} {Sensory} {Processing} {Assessment} for {Adolescents} {With} {Autism} {Spectrum} {Disorder}},
	volume = {29},
	issn = {1558-0210},
	url = {https://ieeexplore.ieee.org/document/9371715},
	doi = {10.1109/TNSRE.2021.3064148},
	abstract = {Sensory abnormalities are experienced by 90 - 95\% of individuals with Autism Spectrum Disorder (ASD), a developmental disorder that impacts at least 1 in 132 children worldwide. Virtual reality (VR) technologies can precisely present sensory stimuli and be integrated with human sensing technologies to automatically detect sensory responses, and thus has a potential to improve sensory assessment objectiveness and sensitivity, compared to traditional questionnaire-based methods. However, there is a lack of evidence to demonstrate this potential. Therefore, we designed and developed a preliminary sensory assessment VR system (SAVR) to objectively and precisely evaluate the visual and touch sensory processing differences between adolescents with ASD and their typically developing (TD) peers through game playing. A controlled experiment was conducted with 12 adolescents with ASD and 12 TD adolescents. Participants' sensory pattern was assessed by SAVR and a widely used traditional questionnaire-the Adult/Adolescent Sensory Profile (AASP). We hypothesized that: 1) compared to AASP, SAVR can find more significant differences between the two participant groups, and 2) there are significant and strong correlations between the SAVR results and the AASP results. Statistical analyses of the experimental data supported the hypotheses. The implication and limitations of this preliminary exploration as well as future works are discussed.},
	urldate = {2025-02-17},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	author = {Koirala, Ankit and Yu, Zhiwei and Schiltz, Hillary and Van Hecke, Amy and Armstrong, Brian and Zheng, Zhi},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	keywords = {virtual reality, Three-dimensional displays, Autism spectrum disorder, Visualization, Games, Variable speed drives, Painting, Robot sensing systems, Sensitivity, sensory assessment},
	pages = {619--628},
	file = {2021_Koirala et al._A Preliminary Exploration of Virtual Reality-Based Visual and Touch Sensory Processing Assessment fo.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2021_Koirala et al._A Preliminary Exploration of Virtual Reality-Based Visual and Touch Sensory Processing Assessment fo.pdf:application/pdf},
}

@article{delbianco2024,
	title = {Sex differences in social brain neural responses in autism: temporal profiles of configural face-processing within data-driven time windows},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	shorttitle = {Sex differences in social brain neural responses in autism},
	url = {https://www.nature.com/articles/s41598-024-64387-9},
	doi = {10.1038/s41598-024-64387-9},
	abstract = {Face-processing timing differences may underlie visual social attention differences between autistic and non-autistic people, and males and females. This study investigates the timing of the effects of neurotype and sex on face-processing, and their dependence on age. We analysed EEG data during upright and inverted photographs of faces from 492 participants from the Longitudinal European Autism Project (141 neurotypical males, 76 neurotypical females, 202 autistic males, 73 autistic females; age 6–30 years). We detected timings of sex/diagnosis effects on event-related potential amplitudes at the posterior–temporal channel P8 with Bootstrapped Cluster-based Permutation Analysis and conducted Growth Curve Analysis (GCA) to investigate the timecourse and dependence on age of neural signals. The periods of influence of neurotype and sex overlapped but differed in onset (respectively, 260 and 310 ms post-stimulus), with sex effects lasting longer. GCA revealed a smaller and later amplitude peak in autistic female children compared to non-autistic female children; this difference decreased in adolescence and was not significant in adulthood. No age-dependent neurotype difference was significant in males. These findings indicate that sex and neurotype influence longer latency face processing and implicates cognitive rather than perceptual processing. Sex may have more overarching effects than neurotype on configural face processing.},
	language = {en},
	number = {1},
	urldate = {2025-02-18},
	journal = {Scientific Reports},
	author = {Del Bianco, Teresa and Lai, Meng-Chuan and Mason, Luke and Johnson, Mark H. and Charman, Tony and Loth, Eva and Banaschewski, Tobias and Buitelaar, Jan and Murphy, Declan G. M. and Jones, Emily J. H.},
	month = jun,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Social behaviour},
	pages = {14038},
	file = {2024_Del Bianco et al._Sex differences in social brain neural responses in autism temporal profiles of configural face-pro.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Del Bianco et al._Sex differences in social brain neural responses in autism temporal profiles of configural face-pro.pdf:application/pdf},
}

@article{bast2023,
	title = {Sensory salience processing moderates attenuated gazes on faces in autism spectrum disorder: a case–control study},
	volume = {14},
	issn = {2040-2392},
	shorttitle = {Sensory salience processing moderates attenuated gazes on faces in autism spectrum disorder},
	url = {https://doi.org/10.1186/s13229-023-00537-6},
	doi = {10.1186/s13229-023-00537-6},
	abstract = {Attenuated social attention is a key marker of autism spectrum disorder (ASD). Recent neuroimaging findings also emphasize an altered processing of sensory salience in ASD. The locus coeruleus–norepinephrine system (LC-NE) has been established as a modulator of this sensory salience processing (SSP). We tested the hypothesis that altered LC-NE functioning contributes to different SSP and results in diverging social attention in ASD.},
	number = {1},
	urldate = {2025-02-18},
	journal = {Molecular Autism},
	author = {Bast, Nico and Mason, Luke and Ecker, Christine and Baumeister, Sarah and Banaschewski, Tobias and Jones, Emily J. H. and Murphy, Declan G. M. and Buitelaar, Jan K. and Loth, Eva and Pandina, Gahan and Ahmad, Jumana and Ambrosino, Sara and Auyeung, Bonnie and Banaschewski, Tobias and Baron-Cohen, Simon and Bast, Nico and Baumeister, Sarah and Beckmann, Christian F. and Bölte, Sven and Bourgeron, Thomas and Bours, Carsten and Brammer, Michael and Brandeis, Daniel and Brogna, Claudia and de Bruijn, Yvette and Buitelaar, Jan K. and Chakrabarti, Bhismadev and Charman, Tony and Cornelissen, Ineke and Crawley, Daisy and Dell’Acqua, Flavio and Dumas, Guillaume and Durston, Sarah and Ecker, Christine and Faulkner, Jessica and Frouin, Vincent and Garcés, Pilar and Goyard, David and Ham, Lindsay and Hayward, Hannah and Hipp, Joerg and Holt, Rosemary and Johnson, Mark and Jones, Emily J. H. and Kundu, Prantik and Lai, Meng-Chuan and D’ardhuy, Xavier Liogier and Lombardo, Michael V. and Loth, Eva and Lythgoe, David J. and Mandl, René and Marquand, Andre and Mason, Luke and Mennes, Maarten and Meyer-Lindenberg, Andreas and Moessnang, Carolin and Murphy, Declan G. M. and Oakley, Bethany and O’Dwyer, Laurence and Oldehinkel, Marianne and Oranje, Bob and Pandina, Gahan and Persico, Antonio M. and Ruggeri, Barbara and Ruigrok, Amber and Sabet, Jessica and Sacco, Roberto and Cáceres, Antonia San José and Simonoff, Emily and Spooren, Will and Tillmann, Julian and Toro, Roberto and Tost, Heike and Waldman, Jack and Williams, Steve C. R. and Wooldridge, Caroline and Zwiers, Marcel P. and Freitag, Christine M. and {the EU-AIMS LEAP Group}},
	month = feb,
	year = {2023},
	keywords = {Computer vision, ASD, Eye tracking, Pupillometry, Locus coeruleus, Naturalistic visual attention, Norepinephrine, Saliency maps, Social attention, Visual exploration},
	pages = {5},
	file = {2023_Bast et al._Sensory salience processing moderates attenuated gazes on faces in autism spectrum disorder a case–.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2023_Bast et al._Sensory salience processing moderates attenuated gazes on faces in autism spectrum disorder a case–.pdf:application/pdf},
}

@article{black2024,
	title = {Conceptual framework for data harmonisation in mental health using the {International} {Classification} of {Functioning}, {Disability} and {Health}: an example with the {R2D2}-{MH} consortium},
	volume = {27},
	copyright = {© Author(s) (or their employer(s)) 2024. Re-use permitted under CC BY. Published by BMJ.. https://creativecommons.org/licenses/by/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution 4.0 Unported (CC BY 4.0) license, which permits others to copy, redistribute, remix, transform and build upon this work for any purpose, provided the original work is properly cited, a link to the licence is given, and indication of whether changes were made. See: https://creativecommons.org/licenses/by/4.0/.},
	issn = {2755-9734},
	shorttitle = {Conceptual framework for data harmonisation in mental health using the {International} {Classification} of {Functioning}, {Disability} and {Health}},
	url = {https://mentalhealth.bmj.com/content/27/1/e301283},
	doi = {10.1136/bmjment-2024-301283},
	abstract = {Introduction Advancing research and support for neurologically diverse populations requires novel data harmonisation methods that are capable of aligning with contemporary approaches to understanding health and disability.
Objectives We present the International Classification of Functioning, Disability and Health (ICF) as a conceptual framework to support harmonisation of mental health data and present a proof of principle within the Risk and Resilience in Developmental Diversity and Mental Health (R2D2-MH) consortium.
Method 138 measures from various mental health datasets were linked to the ICF following the WHO’s established linking rules.
Findings Findings support the notion that the ICF can assist in the harmonisation of mental health data. The high level of shared ICF codes provides indications of where items may be readily harmonised to develop datasets that may align more readily with contemporary approaches to understanding health and disability. Although the linking process necessarily entails an element of subjectivity, the application of established rules can increase rigour and transparency of the harmonisation process.
Conclusions We present the first steps towards data harmonisation in mental health that is compatible with contemporary approaches in psychiatry, being more capable of capturing diversity and aligning with more transdiagnostic and neurodiversity-affirmative ways of understanding data.
Clinical implications Our findings show promise, but future work is needed to address quantitative harmonisation. Similarly, issues related to the traditionally ‘pathophysiological’ frameworks that existing datasets are often embedded in can hinder the full potential of harmonisation based on the ICF.},
	language = {en},
	number = {1},
	urldate = {2025-02-18},
	journal = {BMJ Ment Health},
	author = {Black, Melissa H. and Buitelaar, Jan and Charman, Tony and Ecker, Christine and Gallagher, Louise and Hens, Kristien and Jones, Emily and Murphy, Declan and Sadaka, Yair and Schaer, Marie and Pourcain, Beate St and Wolke, Dieter and Bonnot-Briey, Stef and Bourgeron, Thomas and Bölte, Sven},
	month = nov,
	year = {2024},
	pmid = {39608798},
	note = {Publisher: Royal College of Psychiatrists
Section: Statistics},
	keywords = {PSYCHIATRY},
	file = {2024_Black et al._Conceptual framework for data harmonisation in mental health using the International Classification.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Black et al._Conceptual framework for data harmonisation in mental health using the International Classification.pdf:application/pdf},
}

@article{bolte2025,
	title = {Social cognition in autism and {ADHD}},
	volume = {169},
	issn = {0149-7634},
	url = {https://www.sciencedirect.com/science/article/pii/S0149763425000223},
	doi = {10.1016/j.neubiorev.2025.106022},
	abstract = {Social cognition is a crucial capacity for social functioning. The last decades have seen a plethora of social cognition research in neurodevelopmental conditions, foremost autism and, to a lesser extent, ADHD, both characterized by social challenges. Social cognition is a multifaceted construct comprising various overlapping subdomains, such as Theory of Mind/mentalizing, emotion recognition, and social perception. Mechanisms underpinning social cognition are complex, including implicit and explicit, cognitive and affective, and hyper- and hypo-social information processing. This review explores the intricacies of social cognition in the context of autism and ADHD. Research indicates altered performance on social cognition tests in autism, compared to neurotypical groups, with social cognition alterations having a small but robust effect on the defining features of autism. The nature of such alterations in autism appears primarily in relation to implicit processing. ADHD groups show intermediate social cognition performance, appearing to be influenced by executive function difficulties. Social cognition varies with intellectual and verbal abilities and seems to improve with age in autism and ADHD. Social skills interventions in autism, and stimulant medication in ADHD have been shown to improve social cognition test performance, while mentalizing training effects in autism are less conclusive. A limitation of the field is that social cognition constructs and tests are not well delineated. Further, most research has been embedded in a nativist approach rather than a constructivist approach. The former has been questioned for ignoring environmental contributions, especially the dimension of mutual miscommunication between neurodivergent and neurotypical individuals.},
	urldate = {2025-02-20},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Bölte, Sven},
	month = feb,
	year = {2025},
	keywords = {Autism, ADHD, Intervention, Social cognition, Theory of mind, Assessment, Mentalizing},
	pages = {106022},
	file = {2025_Bölte_Social cognition in autism and ADHD.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2025_Bölte_Social cognition in autism and ADHD.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\S3ZQEUDQ\\S0149763425000223.html:text/html},
}

@article{black2025,
	title = {From {Symptomatology} to {Functioning} - {Applying} the {ICF} to {Autism} {Measures} to {Facilitate} {Neurodiversity}-{Affirmative} {Data} {Harmonization}},
	volume = {55},
	issn = {1573-3432},
	url = {https://doi.org/10.1007/s10803-023-06204-2},
	doi = {10.1007/s10803-023-06204-2},
	abstract = {A considerable number of screening and diagnostic tools for autism exist, but variability in these measures presents challenges to data harmonization and the comparability and generalizability of findings. At the same time, there is a movement away from autism symptomatology to stances that capture heterogeneity and appreciate diversity. The International Classification of Functioning, Disability and Health (ICF) provides a classification system that can support content harmonization of different screening and diagnostic tools for autism while enabling the translation of diagnostic information into functioning.},
	language = {en},
	number = {1},
	urldate = {2025-02-20},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Black, Melissa H. and Remnélius, Karl Lundin and Alehagen, Lovisa and Bourgeron, Thomas and Bölte, Sven},
	month = jan,
	year = {2025},
	keywords = {Diagnosis, Screening, Biopsychosocial, Functioning, Harmonization},
	pages = {114--129},
	file = {2025_Black et al._From Symptomatology to Functioning - Applying the ICF to Autism Measures to Facilitate Neurodiversit.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2025_Black et al._From Symptomatology to Functioning - Applying the ICF to Autism Measures to Facilitate Neurodiversit.pdf:application/pdf},
}

@article{stewart2024,
	title = {Autism – an evolving conceptualisation},
	volume = {188},
	issn = {0999-792x},
	abstract = {English Abstract: The conceptualisation of Autism has greatly evolved over the past several decades. In this review article, we focus on several areas where our understanding of Autism has changed: (1) from a ‘narrow’ definition to a ‘wide’ diagnostic criteria; (2) from a rare to a relatively common condition, although probably still under-recognised in women and older people; (3) a condition diagnosed predominately in males to now being identified in people of all genders; (4) from something affecting children, to a lifelong condition; (5) from something discrete and distinct, to a dimensional view; (6) from one thing to many ‘Autisms’, and a compound or fractionable condition; (7) from a focus on ‘pure’ Autism, to recognition that complexity and comorbidity is the norm; and finally, (8) from conceptualising Autism purely as a developmental disorder, to recognising a neurodiversity perspective, operationalised in participatory research models. We also explore opportunities for how research can become more generalisable, including in a global context, and make suggestions for areas currently neglected in Autism research.French Abstract: La conceptualisation de l'autisme a considérablement évolué au cours des dernières décennies. Dans cet article de synthèse, nous nous concentrons sur plusieurs domaines où notre compréhension de l'autisme a changé : (1) d'une définition « étroite » à des critères diagnostiques « larges » ; (2) d'une condition rare à une condition relativement courante, bien que probablement encore sous-reconnue chez les femmes et les personnes âgées ; (3) d'une condition diagnostiquée principalement chez les hommes à une identification désormais chez les personnes de tous genres ; (4) de quelque chose qui touche les enfants à une condition tout au long de la vie ; (5) de quelque chose de discret et distinct à une vision dimensionnelle ; (6) d'une seule entité à de nombreux « autismes », et une condition composite ou fractionnable ; (7) d'une focalisation sur l'autisme « pur » à la reconnaissance que la complexité et la comorbidité sont la norme ; et enfin, (8) de la conceptualisation de l'autisme uniquement comme un trouble du développement à la reconnaissance d'une perspective de neurodiversité, opérationnalisée dans des modèles de recherche participative. Nous explorons également les opportunités pour que la recherche devienne plus généralisable, y compris dans un contexte mondial, et formulons des suggestions pour des domaines actuellement négligés dans la recherche sur l'autisme.Spanish Abstract: La conceptualización del autismo ha evolucionado considerablemente en las últimas décadas. En este artículo de síntesis, nos centramos en varios ámbitos donde nuestra comprensión del autismo ha cambiado: (1) desde una definición "estrecha" hacia criterios diagnósticos "amplios"; (2) desde una condición rara hacia una condición relativamente común, aunque probablemente aún subestimada en mujeres y personas mayores; (3) desde una condición diagnosticada principalmente en hombres hacia una identificación ahora en personas de todos los géneros; (4) desde algo que afecta solo a niños hacia una condición a lo largo de toda la vida; (5) desde algo discreto y distintivo hacia una visión dimensional; (6) desde una sola entidad hacia muchos "autismos", y una condición compuesta o divisible; (7) desde un enfoque en el autismo "puro" hacia el reconocimiento de que la complejidad y la comorbilidad son la norma; y finalmente, (8) desde la conceptualización del autismo únicamente como un trastorno del desarrollo hacia el reconocimiento de una perspectiva de neurodiversidad, operacionalizada en modelos de investigación participativa. También exploramos las oportunidades para que la investigación sea más generalizable, incluso en un contexto global, y formulamos sugerencias para áreas actualmente descuidadas en la investigación sobre el autismo.},
	journal = {Approche Neuropsychologique des Apprentissages Chez L'Enfant},
	author = {Stewart, Gavin and Happe, Francesca},
	month = mar,
	year = {2024},
	pages = {53--61},
	file = {Stewart_Happe_-_Autism_as_an_evolving_concept_Author_Copy_English_original_ (1):C\:\\Users\\antoine.widmer\\Zotero\\storage\\G7QKVRSD\\Stewart_Happe_-_Autism_as_an_evolving_concept_Author_Copy_English_original_ (1).pdf:application/pdf},
}

@article{lord2023,
	title = {Digital phenotyping could help detect autism},
	volume = {29},
	copyright = {2023 Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-023-02557-4},
	doi = {10.1038/s41591-023-02557-4},
	abstract = {Researchers have developed a screening tool for autism that uses computer vision and machine learning to analyze autism-related behaviors — but greater reliability and robust validation will be needed if such tools are to be used in primary care settings.},
	language = {en},
	number = {10},
	urldate = {2025-02-22},
	journal = {Nature Medicine},
	author = {Lord, Catherine and Wilson, Rujuta B.},
	month = oct,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biomedicine, Cancer Research, general, Infectious Diseases, Metabolic Diseases, Molecular Medicine, Neurosciences},
	pages = {2412--2413},
	file = {2023_Lord and Wilson_Digital phenotyping could help detect autism.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2023_Lord and Wilson_Digital phenotyping could help detect autism.pdf:application/pdf},
}

@article{moore2018,
	title = {The geometric preference subtype in {ASD}: identifying a consistent, early-emerging phenomenon through eye tracking},
	volume = {9},
	issn = {2040-2392},
	shorttitle = {The geometric preference subtype in {ASD}},
	url = {https://doi.org/10.1186/s13229-018-0202-z},
	doi = {10.1186/s13229-018-0202-z},
	abstract = {The wide range of ability and disability in ASD creates a need for tools that parse the phenotypic heterogeneity into meaningful subtypes. Using eye tracking, our past studies revealed that when presented with social and geometric images, a subset of ASD toddlers preferred viewing geometric images, and these toddlers also had greater symptom severity than ASD toddlers with greater social attention. This study tests whether this “GeoPref test” effect would generalize across different social stimuli.},
	number = {1},
	urldate = {2025-02-23},
	journal = {Molecular Autism},
	author = {Moore, Adrienne and Wozniak, Madeline and Yousef, Andrew and Barnes, Cindy Carter and Cha, Debra and Courchesne, Eric and Pierce, Karen},
	month = mar,
	year = {2018},
	keywords = {Eye tracking, Autism spectrum disorder, Social attention, Early identification, Geometric preference},
	pages = {19},
	file = {2018_Moore et al._The geometric preference subtype in ASD identifying a consistent, early-emerging phenomenon through.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2018_Moore et al._The geometric preference subtype in ASD identifying a consistent, early-emerging phenomenon through.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\6WSJHZ8X\\s13229-018-0202-z.html:text/html},
}

@article{smith2022,
	title = {Implementing the {Get} {SET} {Early} {Model} in a {Community} {Setting} to {Lower} the {Age} of {ASD} {Diagnosis}},
	volume = {43},
	issn = {1536-7312},
	url = {https://europepmc.org/articles/PMC9725891},
	doi = {10.1097/dbp.0000000000001130},
	abstract = {ObjectiveThe objective of this study was to implement a validated, university-based early detection program, the Get SET Early model, in a community-based setting. Get SET was developed to improve Screening, Evaluation, and Treatment referral practices. Specifically, its purpose was to lower the age of diagnosis and enable toddlers with autism spectrum disorder (ASD) to begin treatment by 36 months.MethodsOne hundred nine pediatric health care providers were recruited to administer the Communication and Symbolic Behavior Scales Developmental Profile Infant-Toddler Checklist at 12-month, 18-month, and 24-month well-baby visits and referred toddlers whose scores indicated the need for a developmental evaluation. Licensed psychologists were trained to provide diagnostic evaluations to toddlers as young as 12 months. Mean age of diagnosis was compared with current population rates.ResultsIn 4 years, 45,504 screens were administered at well-baby visits, and 648 children were evaluated at least 1 time. The overall median age for ASD diagnosis was 22 months, which is significantly lower than the median age reported by the CDC (57 months). For children screened at 12 months, the age of first diagnosis was significantly lower at 15 months. Of the 350 children who completed at least 1 follow-up evaluation, 323 were diagnosed with ASD or another delay, and 239 (74\%) were enrolled in a treatment program.ConclusionToddlers with ASD were diagnosed nearly 3 years earlier than the most recent CDC report, which allowed children to start a treatment program by 36 months. Overall, Get SET Early was an effective strategy for improving the current approach to screening, evaluation, and treatment. Efforts to demonstrate sustainability are underway.},
	language = {eng},
	number = {9},
	urldate = {2025-02-23},
	journal = {Journal of developmental and behavioral pediatrics},
	author = {Smith, Christopher J and James, Stephen and Skepnek, Erica and Leuthe, Eileen and Outhier, Lisa Elder and Avelar, Delia and Barnes, Cynthia Carter and Bacon, Elizabeth and Pierce, Karen},
	month = dec,
	year = {2022},
	pmid = {36443921},
	pmcid = {PMC9725891},
	pages = {494--502},
	file = {2022_Smith et al._Implementing the Get SET Early Model in a Community Setting to Lower the Age of ASD Diagnosis.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2022_Smith et al._Implementing the Get SET Early Model in a Community Setting to Lower the Age of ASD Diagnosis.pdf:application/pdf},
}

@article{pierce2021,
	title = {\textit{{Get} {SET} {Early}} to {Identify} and {Treatment} {Refer} {Autism} {Spectrum} {Disorder} at 1 {Year} and {Discover} {Factors} {That} {Influence} {Early} {Diagnosis}},
	volume = {236},
	issn = {0022-3476},
	url = {https://www.sciencedirect.com/science/article/pii/S0022347621003929},
	doi = {10.1016/j.jpeds.2021.04.041},
	abstract = {Objectives
To examine the impact of a new approach, Get SET Early, on the rates of early autism spectrum disorder (ASD) detection and factors that influence the screen-evaluate-treat chain.
Study design
After attending Get SET Early training, 203 pediatricians administered 57 603 total screens using the Communication and Symbolic Behavior Scales Infant-Toddler Checklist at 12-, 18-, and 24-month well-baby examinations, and parents designated presence or absence of concern. For screen-positive toddlers, pediatricians specified if the child was being referred for evaluation, and if not, why not.
Results
Collapsed across ages, toddlers were evaluated and referred for treatment at a median age of 19 months, and those screened at 12 months (59.4\% of sample) by 15 months. Pediatricians referred one-third of screen-positive toddlers for evaluation, citing lack of confidence in the accuracy of screen-positive results as the primary reason for nonreferral. If a parent expressed concerns, referral probability doubled, and the rate of an ASD diagnosis increased by 37\%. Of 897 toddlers evaluated, almost one-half were diagnosed as ASD, translating into an ASD prevalence of 1\%.
Conclusions
The Get SET Early model was effective at detecting ASD and initiating very early treatment. Results also underscored the need for change in early identification approaches to formally operationalize and incorporate pediatrician judgment and level of parent concern into the process.},
	urldate = {2025-02-24},
	journal = {The Journal of Pediatrics},
	author = {Pierce, Karen and Gazestani, Vahid and Bacon, Elizabeth and Courchesne, Eric and Cheng, Amanda and Barnes, Cynthia Carter and Nalabolu, Srinivasa and Cha, Debra and Arias, Steven and Lopez, Linda and Pham, Christie and Gaines, Kim and Gyurjyan, Gohar and Cook-Clark, Terri and Karins, Kathy},
	month = sep,
	year = {2021},
	keywords = {autism, screening, diagnosis, early detection},
	pages = {179--188},
	file = {2021_Pierce et al._Get SET Early to Identify and Treatment Refer Autism Spectrum Disorder at 1 Year and Discover.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2021_Pierce et al._Get SET Early to Identify and Treatment Refer Autism Spectrum Disorder at 1 Year and Discover.pdf:application/pdf},
}

@article{pham2023,
	title = {Examination of the impact of the {Get} {SET} {Early} program on equitable access to care within the screen-evaluate-treat chain in toddlers with autism spectrum disorder},
	volume = {27},
	issn = {1362-3613},
	url = {https://doi.org/10.1177/13623613221147416},
	doi = {10.1177/13623613221147416},
	abstract = {Delays in autism spectrum disorder identification/services could impact developmental outcomes. Although trends are encouraging, children from historically underrepresented minority backgrounds are often identified later and have reduced engagement in care. It is unclear if disparities exist throughout the screen-evaluate-treat chain, or if early detection programs such as Get SET Early that standardize these steps are effective countermeasures. Pediatricians/primary care providers administered Communication and Symbolic Behavior Scales IT Checklist screens at 12-, 18-, and 24-month well-baby examinations, and parents designated race, ethnicity, and developmental concerns. Toddlers who scored in the range of concern, or whose pediatricians/primary care providers had concerns, were referred for evaluations. Rates of screening and evaluation engagement within ethnic/racial groups were compared to U.S. Census proportions. Age at screen, evaluation, and treatment and quantity was compared across groups. Regressions examined whether key factors were associated with ethnicity or race. No differences were found for mean age of screen, evaluation, initiation of behavioral therapy, or quantity received between racial and ethnic groups. Historically underrepresented minority children were more likely to fall into the range of concern, referred for evaluations, and have their parents express developmental concerns. Although there remain gaps within the pipeline, implementation of systemized programs can be effective in ensuring equitable access to resources across communities.
Lay abstract
Delays in autism spectrum disorder identification and access to care could impact developmental outcomes. Although trends are encouraging, children from historically underrepresented minority backgrounds are often identified at later ages and have reduced engagement in services. It is unclear if disparities exist all along the screen-evaluation-treatment chain, or if early detection programs such as Get SET Early that standardize, these steps are effective at ameliorating disparities. As part of the Get SET Early model, primary care providers administered a parent-report screen at well-baby examinations, and parents designated race, ethnicity, and developmental concerns. Toddlers who scored in the range of concern, or whose primary care provider had concerns, were referred for an evaluation. Rates of screening and evaluation engagement within ethnic/racial groups were compared to US Census data. Age at screen, evaluation, and treatment engagement and quantity was compared across groups. Statistical models examined whether key factors such as parent concern were associated with ethnicity or race. No differences were found in the mean age at the first screen, evaluation, or initiation or quantity of behavioral therapy between participants. However, children from historically underrepresented minority backgrounds were more likely to fall into the range of concern on the parent-report screen, their parents expressed developmental concerns more often, and pediatricians were more likely to refer for an evaluation than their White/Not Hispanic counterparts. Overall results suggest that models that support transparent tracking of steps in the screen-evaluation-treatment chain and service referral pipelines may be an effective strategy for ensuring equitable access to care for all children.},
	language = {en},
	number = {6},
	urldate = {2025-02-24},
	journal = {Autism},
	author = {Pham, Christie and Bacon, Elizabeth C and Grzybowski, Andrea and Carter-Barnes, Cynthia and Arias, Steven and Xu, Ronghui and Lopez, Linda and Courchesne, Eric and Pierce, Karen},
	month = aug,
	year = {2023},
	note = {Publisher: SAGE Publications Ltd},
	pages = {1790--1802},
	file = {2023_Pham et al._Examination of the impact of the Get SET Early program on equitable access to care within the screen.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2023_Pham et al._Examination of the impact of the Get SET Early program on equitable access to care within the screen.pdf:application/pdf},
}

@article{rausch2024,
	title = {A novel screening instrument for the assessment of autism in {German} language: validation of the {German} version of the {RAADS}-{R}, the {RADS}-{R}},
	issn = {1433-8491},
	shorttitle = {A novel screening instrument for the assessment of autism in {German} language},
	url = {https://doi.org/10.1007/s00406-024-01894-w},
	doi = {10.1007/s00406-024-01894-w},
	abstract = {The Ritvo Autism Asperger Diagnostic Scale-Revised (RAADS-R) demonstrated excellent results in its original study, with a sensitivity of 97\% and a specificity of 100\% (Ritvo et al. in J Autism Dev Disord 41:1076–1089, 2011). As a result, it was included in the National Institute for Health and Care Excellence (NICE) guidelines (Recommendations {\textbar} Autism spectrum disorder in adults: diagnosis and management {\textbar} Guidance {\textbar} NICE, 2022). The questionnaire includes 80 questions across four subcategories (language, social relatedness, circumscribed interests, sensory motor). So far, the subcategory sensory motor has not been addressed in most available instruments, despite being part of the diagnostic criteria specified in DSM-5 (Falkai et al., in Diagnostisches Und Statistisches Manual Psychischer Störungen DSM-5. Hogrefe, 2015) and ICD-11 (ICD-11 for Mortality and Morbidity Statistics, 2022). In our validation study, we tested a translated German version of the questionnaire in 299 individuals (110 persons with ASD according to ICD-10 F84.0, F84.5, 64 persons with an primary mental disorders (PMD), 125 persons with no disorders). To enhance the practical use of the instrument in clinical everyday practice, the questionnaire was completed by the participants without the presence of a clinician—unlike the original study. Psychiatric diagnoses were established following the highest standards, and psychometric properties were calculated using established protocols. The German version of the RADS-R yielded very good results, with a high sensitivity of 92.5\% and a high specificity of 93.6\%. The area under the curve (AUC = 0.976), indicates a high quality and discriminatory power of RADS-R. Furthermore, the ROC curve analysis showed that the optimal threshold to distinguish between the ASD and non-ASD groups in the German version of the RAADS-R is a score of 81. In comparison to the RADS-R, the co-administered instruments Social Responsiveness Scale (SRS), Autism Spectrum Quotient (AQ), and Empathy Quotient (EQ) each showed slightly better specificity but worse sensitivity in this sample.The study included individuals already diagnosed with ASD according to ICD-10 (F84.0, F84.5), with or without an primary mental disorders, preventing us from identifying the influence of comorbidities on the RADS-R results. In addition, a self-report questionnaire has generally only limited objectivity and may allow for false representation of the symptoms. The RADS-R compares well with other questionnaires and can provide valuable additional information. It could turn out to be a helpful diagnostic tool for patients in Germany. We propose naming the German version RADS-R (Ritvo Autism Diagnostic Scale – rRevised) to reflect the change in terminology.},
	language = {en},
	urldate = {2025-02-24},
	journal = {European Archives of Psychiatry and Clinical Neuroscience},
	author = {Rausch, Jördis and Fangmeier, Thomas and Falter-Wagner, Christine M. and Ackermann, Helene and Espelöer, Julia and Hölzel, Lars P. and Riedel, Andreas and Ritvo, Ariella and Vogeley, Kai and van Elst, Ludger Tebartz},
	month = oct,
	year = {2024},
	keywords = {Autism, ASD, Neurodevelopmental disorders, Adults, Diagnostic, Questionnaire},
	file = {2024_Rausch et al._A novel screening instrument for the assessment of autism in German language validation of the Germ.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Rausch et al._A novel screening instrument for the assessment of autism in German language validation of the Germ.pdf:application/pdf},
}

@article{robles2024,
	title = {Reduced stereotypicality and spared use of facial expression predictions for social evaluation in autism},
	volume = {24},
	issn = {1697-2600},
	url = {https://www.sciencedirect.com/science/article/pii/S169726002400005X},
	doi = {10.1016/j.ijchp.2024.100440},
	abstract = {Background/Objective
Autism has been investigated through traditional emotion recognition paradigms, merely investigating accuracy, thereby constraining how potential differences across autistic and control individuals may be observed, identified, and described. Moreover, the use of emotional facial expression information for social functioning in autism is of relevance to provide a deeper understanding of the condition.
Method
Adult autistic individuals (n = 34) and adult control individuals (n = 34) were assessed with a social perception behavioral paradigm exploring facial expression predictions and their impact on social evaluation.
Results
Autistic individuals held less stereotypical predictions than controls. Importantly, despite such differences in predictions, the use of such predictions for social evaluation did not differ significantly between groups, as autistic individuals relied on their predictions to evaluate others to the same extent as controls.
Conclusions
These results help to understand how autistic individuals perceive social stimuli and evaluate others, revealing a deviation from stereotypicality beyond which social evaluation strategies may be intact.},
	number = {2},
	urldate = {2025-02-24},
	journal = {International Journal of Clinical and Health Psychology},
	author = {Robles, Marta and Ramos-Grille, Irene and Hervás, Amaia and Duran-Tauleria, Enric and Galiano-Landeira, Jordi and Wormwood, Jolie B. and Falter-Wagner, Christine M. and Chanes, Lorena},
	month = apr,
	year = {2024},
	keywords = {Autism, Facial expressions, Emotion, Predictive processing, Social perception},
	pages = {100440},
	file = {2024_Robles et al._Reduced stereotypicality and spared use of facial expression predictions for social evaluation in au.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Robles et al._Reduced stereotypicality and spared use of facial expression predictions for social evaluation in au.pdf:application/pdf},
}

@article{washington2023a,
	title = {A {Review} of and {Roadmap} for {Data} {Science} and {Machine} {Learning} for the {Neuropsychiatric} {Phenotype} of {Autism}},
	volume = {6},
	issn = {2574-3414},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-biodatasci-020722-125454},
	doi = {10.1146/annurev-biodatasci-020722-125454},
	abstract = {Autism spectrum disorder (autism) is a neurodevelopmental delay that affects at least 1 in 44 children. Like many neurological disorder phenotypes, the diagnostic features are observable, can be tracked over time, and can be managed or even eliminated through proper therapy and treatments. However, there are major bottlenecks in the diagnostic, therapeutic, and longitudinal tracking pipelines for autism and related neurodevelopmental delays, creating an opportunity for novel data science solutions to augment and transform existing workflows and provide increased access to services for affected families. Several efforts previously conducted by a multitude of research labs have spawned great progress toward improved digital diagnostics and digital therapies for children with autism. We review the literature on digital health methods for autism behavior quantification and beneficial therapies using data science. We describe both case–control studies and classification systems for digital phenotyping. We then discuss digital diagnostics and therapeutics that integrate machine learning models of autism-related behaviors, including the factors that must be addressed for translational use. Finally, we describe ongoing challenges and potential opportunities for the field of autism data science. Given the heterogeneous nature of autism and the complexities of the relevant behaviors, this review contains insights that are relevant to neurological behavior analysis and digital psychiatry more broadly.},
	language = {en},
	number = {Volume 6, 2023},
	urldate = {2025-02-24},
	journal = {Annual Review of Biomedical Data Science},
	author = {Washington, Peter and Wall, Dennis P.},
	month = aug,
	year = {2023},
	note = {Publisher: Annual Reviews},
	pages = {211--228},
	file = {2023_Washington and Wall_A Review of and Roadmap for Data Science and Machine Learning for the Neuropsychiatric Phenotype of.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2023_Washington and Wall_A Review of and Roadmap for Data Science and Machine Learning for the Neuropsychiatric Phenotype of.pdf:application/pdf},
}

@article{jones2023a,
	title = {Eye-{Tracking}–{Based} {Measurement} of {Social} {Visual} {Engagement} {Compared} {With} {Expert} {Clinical} {Diagnosis} of {Autism}},
	volume = {330},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.2023.13295},
	doi = {10.1001/jama.2023.13295},
	abstract = {In the US, children with signs of autism often experience more than 1 year of delay before diagnosis and often experience longer delays if they are from racially, ethnically, or economically disadvantaged backgrounds. Most diagnoses are also received without use of standardized diagnostic instruments. To aid in early autism diagnosis, eye-tracking measurement of social visual engagement has shown potential as a performance-based biomarker.To evaluate the performance of eye-tracking measurement of social visual engagement (index test) relative to expert clinical diagnosis in young children referred to specialty autism clinics.In this study of 16- to 30-month-old children enrolled at 6 US specialty centers from April 2018 through May 2019, staff blind to clinical diagnoses used automated devices to measure eye-tracking–based social visual engagement. Expert clinical diagnoses were made using best practice standardized protocols by specialists blind to index test results. This study was completed in a 1-day protocol for each participant.Primary outcome measures were test sensitivity and specificity relative to expert clinical diagnosis. Secondary outcome measures were test correlations with expert clinical assessments of social disability, verbal ability, and nonverbal cognitive ability.Eye-tracking measurement of social visual engagement was successful in 475 (95.2\%) of the 499 enrolled children (mean [SD] age, 24.1 [4.4] months; 38 [8.0\%] were Asian; 37 [7.8\%], Black; 352 [74.1\%], White; 44 [9.3\%], other; and 68 [14.3\%], Hispanic). By expert clinical diagnosis, 221 children (46.5\%) had autism and 254 (53.5\%) did not. In all children, measurement of social visual engagement had sensitivity of 71.0\% (95\% CI, 64.7\% to 76.6\%) and specificity of 80.7\% (95\% CI, 75.4\% to 85.1\%). In the subgroup of 335 children whose autism diagnosis was certain, sensitivity was 78.0\% (95\% CI, 70.7\% to 83.9\%) and specificity was 85.4\% (95\% CI, 79.5\% to 89.8\%). Eye-tracking test results correlated with expert clinical assessments of individual levels of social disability (r = −0.75 [95\% CI, −0.79 to −0.71]), verbal ability (r = 0.65 [95\% CI, 0.59 to 0.70]), and nonverbal cognitive ability (r = 0.65 [95\% CI, 0.59 to 0.70]).In 16- to 30-month-old children referred to specialty clinics, eye-tracking–based measurement of social visual engagement was predictive of autism diagnoses by clinical experts. Further evaluation of this test’s role in early diagnosis and assessment of autism in routine specialty clinic practice is warranted.ClinicalTrials.gov Identifier: NCT03469986},
	number = {9},
	urldate = {2025-02-24},
	journal = {JAMA},
	author = {Jones, Warren and Klaiman, Cheryl and Richardson, Shana and Aoki, Christa and Smith, Christopher and Minjarez, Mendy and Bernier, Raphael and Pedapati, Ernest and Bishop, Somer and Ence, Whitney and Wainer, Allison and Moriuchi, Jennifer and Tay, Sew-Wah and Klin, Ami},
	month = sep,
	year = {2023},
	pages = {854--865},
	file = {2023_Jones et al._Eye-Tracking–Based Measurement of Social Visual Engagement Compared With Expert Clinical Diagnosis o.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2023_Jones et al._Eye-Tracking–Based Measurement of Social Visual Engagement Compared With Expert Clinical Diagnosis o.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\R7U5Z6WF\\2808996.html:text/html},
}

@article{jones2023b,
	title = {Development and {Replication} of {Objective} {Measurements} of {Social} {Visual} {Engagement} to {Aid} in {Early} {Diagnosis} and {Assessment} of {Autism}},
	volume = {6},
	issn = {2574-3805},
	url = {https://doi.org/10.1001/jamanetworkopen.2023.30145},
	doi = {10.1001/jamanetworkopen.2023.30145},
	abstract = {Autism spectrum disorder is a common and early-emerging neurodevelopmental condition. While 80\% of parents report having had concerns for their child’s development before age 2 years, many children are not diagnosed until ages 4 to 5 years or later.To develop an objective performance-based tool to aid in early diagnosis and assessment of autism in children younger than 3 years.In 2 prospective, consecutively enrolled, broad-spectrum, double-blind studies, we developed an objective eye-tracking–based index test for children aged 16 to 30 months, compared its performance with best-practice reference standard diagnosis of autism (discovery study), and then replicated findings in an independent sample (replication study). Discovery and replication studies were conducted in specialty centers for autism diagnosis and treatment. Reference standard diagnoses were made using best-practice standardized protocols by specialists blind to eye-tracking results. Eye-tracking tests were administered by staff blind to clinical results. Children were enrolled from April 27, 2013, until September 26, 2017. Data were analyzed from March 28, 2018, to January 3, 2019.Prespecified primary end points were the sensitivity and specificity of the eye-tracking–based index test compared with the reference standard. Prespecified secondary end points measured convergent validity between eye-tracking–based indices and reference standard assessments of social disability, verbal ability, and nonverbal ability.Data were collected from 1089 children: 719 children (mean [SD] age, 22.4 [3.6] months) in the discovery study, and 370 children (mean [SD] age, 25.4 [6.0] months) in the replication study. In discovery, 224 (31.2\%) were female and 495 (68.8\%) male; in replication, 120 (32.4\%) were female and 250 (67.6\%) male. Based on reference standard expert clinical diagnosis, there were 386 participants (53.7\%) with nonautism diagnoses and 333 (46.3\%) with autism diagnoses in discovery, and 184 participants (49.7\%) with nonautism diagnoses and 186 (50.3\%) with autism diagnoses in replication. In the discovery study, the area under the receiver operating characteristic curve was 0.90 (95\% CI, 0.88-0.92), sensitivity was 81.9\% (95\% CI, 77.3\%-85.7\%), and specificity was 89.9\% (95\% CI, 86.4\%-92.5\%). In the replication study, the area under the receiver operating characteristic curve was 0.89 (95\% CI, 0.86-0.93), sensitivity was 80.6\% (95\% CI, 74.1\%-85.7\%), and specificity was 82.3\% (95\% CI, 76.1\%-87.2\%). Eye-tracking test results correlated with expert clinical assessments of children’s individual levels of ability, explaining 68.6\% (95\% CI, 58.3\%-78.6\%), 63.4\% (95\% CI, 47.9\%-79.2\%), and 49.0\% (95\% CI, 33.8\%-65.4\%) of variance in reference standard assessments of social disability, verbal ability, and nonverbal cognitive ability, respectively.In two diagnostic studies of children younger than 3 years, objective eye-tracking–based measurements of social visual engagement quantified diagnostic status as well as individual levels of social disability, verbal ability, and nonverbal ability in autism. These findings suggest that objective measurements of social visual engagement can be used to aid in autism diagnosis and assessment.},
	number = {9},
	urldate = {2025-02-24},
	journal = {JAMA Network Open},
	author = {Jones, Warren and Klaiman, Cheryl and Richardson, Shana and Lambha, Meena and Reid, Morganne and Hamner, Taralee and Beacham, Chloe and Lewis, Peter and Paredes, Jose and Edwards, Laura and Marrus, Natasha and Constantino, John N. and Shultz, Sarah and Klin, Ami},
	month = sep,
	year = {2023},
	pages = {e2330145},
	file = {2023_Jones et al._Development and Replication of Objective Measurements of Social Visual Engagement to Aid in Early Di.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2023_Jones et al._Development and Replication of Objective Measurements of Social Visual Engagement to Aid in Early Di.pdf:application/pdf;2023_Jones et al._Development and Replication of Objective Measurements of Social Visual Engagement to Aid in Early Di.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2023_Jones et al._Development and Replication of Objective Measurements of Social Visual Engagement to Aid in Early Di_2.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\4PDBDJ3K\\2808909.html:text/html},
}

@book{goldblum2023,
	title = {{THE} {TODDLER} {REMOTE} {ASSESSMENT} {OF} {VIRTUAL} {EYE} {TRACKING} {AND} {LANGUAGE} ({TRAVEL}) {STUDY}: {AN} {EYE} {TRACKING} {AND} {BEHAVIORAL} {ASSESSMENT} {OF} {JOINT} {ATTENTION} {AT} {HOME}},
	language = {http://id.loc.gov/vocabulary/iso639-2/eng},
	author = {Goldblum},
	year = {2023},
	note = {Type: Dissertation},
	file = {Goldblum_THE TODDLER REMOTE ASSESSMENT OF VIRTUAL EYE TRACKING AND LANGUAGE (TRAVEL) STUDY AN EYE TRACKING A.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\Goldblum_THE TODDLER REMOTE ASSESSMENT OF VIRTUAL EYE TRACKING AND LANGUAGE (TRAVEL) STUDY AN EYE TRACKING A.pdf:application/pdf},
}

@article{kirkham2024,
	title = {Immersive {Virtual} {Reality}–{Based} {Methods} for {Assessing} {Executive} {Functioning}: {Systematic} {Review}},
	volume = {12},
	copyright = {This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published JMIR Serious Games, is properly cited. The complete bibliographic information, a link to the original publication on https://games.jmir.org/, as well as this copyright and license information must be included.},
	shorttitle = {Immersive {Virtual} {Reality}–{Based} {Methods} for {Assessing} {Executive} {Functioning}},
	url = {https://games.jmir.org/2024/1/e50282},
	doi = {10.2196/50282},
	abstract = {Background: Neuropsychological assessments traditionally include tests of executive functioning (EF) because of its critical role in daily activities and link to mental disorders. Established traditional EF assessments, although robust, lack ecological validity and are limited to single cognitive processes. These methods, which are suitable for clinical populations, are less informative regarding EF in healthy individuals. With these limitations in mind, immersive virtual reality (VR)–based assessments of EF have garnered interest because of their potential to increase test sensitivity, ecological validity, and neuropsychological assessment accessibility.
Objective: This systematic review aims to explore the literature on immersive VR assessments of EF focusing on (1) EF components being assessed, (2) how these assessments are validated, and (3) strategies for monitoring potential adverse (cybersickness) and beneficial (immersion) effects.
Methods: EBSCOhost, Scopus, and Web of Science were searched in July 2022 using keywords that reflected the main themes of VR, neuropsychological tests, and EF. Articles had to be peer-reviewed manuscripts written in English and published after 2013 that detailed empirical, clinical, or proof-of-concept studies in which a virtual environment using a head-mounted display was used to assess EF in an adult population. A tabular synthesis method was used in which validation details from each study, including comparative assessments and scores, were systematically organized in a table. The results were summed and qualitatively analyzed to provide a comprehensive overview of the findings.
Results: The search retrieved 555 unique articles, of which 19 (3.4\%) met the inclusion criteria. The reviewed studies encompassed EF and associated higher-order cognitive functions such as inhibitory control, cognitive flexibility, working memory, planning, and attention. VR assessments commonly underwent validation against gold-standard traditional tasks. However, discrepancies were observed, with some studies lacking reported a priori planned correlations, omitting detailed descriptions of the EF constructs evaluated using the VR paradigms, and frequently reporting incomplete results. Notably, only 4 of the 19 (21\%) studies evaluated cybersickness, and 5 of the 19 (26\%) studies included user experience assessments.
Conclusions: Although it acknowledges the potential of VR paradigms for assessing EF, the evidence has limitations. The methodological and psychometric properties of the included studies were inconsistently addressed, raising concerns about their validity and reliability. Infrequent monitoring of adverse effects such as cybersickness and considerable variability in sample sizes may limit interpretation and hinder psychometric evaluation. Several recommendations are proposed to improve the theory and practice of immersive VR assessments of EF. Future studies should explore the integration of biosensors with VR systems and the capabilities of VR in the context of spatial navigation assessments. Despite considerable promise, the systematic and validated implementation of VR assessments is essential for ensuring their practical utility in real-world applications.},
	language = {EN},
	number = {1},
	urldate = {2025-02-25},
	journal = {JMIR Serious Games},
	author = {Kirkham, Rebecca and Kooijman, Lars and Albertella, Lucy and Myles, Dan and Yücel, Murat and Rotaru, Kristian},
	month = feb,
	year = {2024},
	note = {Company: JMIR Serious Games
Distributor: JMIR Serious Games
Institution: JMIR Serious Games
Label: JMIR Serious Games
Publisher: JMIR Publications Inc., Toronto, Canada},
	pages = {e50282},
	file = {2024_Kirkham et al._Immersive Virtual Reality–Based Methods for Assessing Executive Functioning Systematic Review.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Kirkham et al._Immersive Virtual Reality–Based Methods for Assessing Executive Functioning Systematic Review.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\2J9VCDPN\\e50282.html:text/html},
}

@article{kaur2025,
	title = {Supporting emotion regulation in children on the autism spectrum: co-developing a digital mental health application for school-based settings with community partners},
	volume = {50},
	issn = {0146-8693},
	shorttitle = {Supporting emotion regulation in children on the autism spectrum},
	url = {https://doi.org/10.1093/jpepsy/jsae078},
	doi = {10.1093/jpepsy/jsae078},
	abstract = {KeepCalm is a digital mental health application, co-designed with community partners, that incorporates wearable biosensing with support for teams to address challenging behaviors and emotion dysregulation in children on the autism spectrum.We followed a user-centered design framework. Before app development, we conducted design workshops, needs assessment interviews, a systematic review, and created an Expert Advisory Board. Once we had a working prototype, we recruited 73 participants to test and help improve the app across five testing cycles.Participants rated the app across testing cycles as highly acceptable, appropriate, feasible, and with good usability. Qualitative data indicated that KeepCalm helped teachers (a) be aware of students’ previously unrealized triggers, especially for nonspeaking students; (b) prevent behavioral episodes; (c) communicate with parents about behaviors/strategies; and (d) equipped parents with knowledge of strategies to use at home. We learned that in order to make the app acceptable and appropriate we needed to make the app enjoyable/easy to use and to focus development on novel features that augment teachers’ skills (e.g., behavioral pattern and stress detection). We also learned about the importance of maximizing feasibility, through in-person app training/support especially regarding the wearable devices, and the importance of having aides involved.Our findings have informed plans for wider-scale feasibility testing so that we may examine the determinants of implementation to inform adaptations and refinement, and gather preliminary efficacy data on KeepCalm’s impact on reducing challenging behaviors and supporting emotion regulation in students on the autism spectrum.},
	number = {1},
	urldate = {2025-02-25},
	journal = {Journal of Pediatric Psychology},
	author = {Kaur, Isha and Kamel, Rima and Sultanik, Evan and Tan, Jessica and Mazefsky, Carla A and Brookman-Frazee, Lauren and McPartland, James C and Goodwin, Matthew S and Pennington, Jeffrey and Beidas, Rinad S and Mandell, David S and Nuske, Heather J},
	month = jan,
	year = {2025},
	pages = {129--140},
	file = {Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\XJHM9MLX\\7833323.html:text/html},
}

@article{nyrenius2023,
	title = {The ‘lost generation’ in adult psychiatry: psychiatric, neurodevelopmental and sociodemographic characteristics of psychiatric patients with autism unrecognised in childhood},
	volume = {9},
	issn = {2056-4724},
	shorttitle = {The ‘lost generation’ in adult psychiatry},
	url = {https://www.cambridge.org/core/journals/bjpsych-open/article/lost-generation-in-adult-psychiatry-psychiatric-neurodevelopmental-and-sociodemographic-characteristics-of-psychiatric-patients-with-autism-unrecognised-in-childhood/90D91FDE2321CC7DBE080DA78363CCF8},
	doi = {10.1192/bjo.2023.13},
	abstract = {Background
Patients with ‘underlying’ autism spectrum disorder (ASD) constitute a significant minority in adult out-patient psychiatry. Diagnoses of previously unrecognised ASD are increasing in adults. Characteristics of patients with autism within adult out-patient psychiatry have not been sufficiently explored, and there have not been any systematic comparisons of characteristics between patients with and those without autism within adult out-patient psychiatric populations.

Aims
To examine psychiatrically relevant characteristics in autistic adult psychiatric out-patients, and to compare the characteristics with non-autistic adult psychiatric out-patients.

Method
We assessed 90 patients who were referred to a Swedish psychiatric out-patient clinic and screened for ASD during 2019–2020. Sixty-three patients met the DSM-5 criteria for ASD or ‘subthreshold’ ASD. The 27 who did not meet the criteria for ASD were used as a comparison group. Assessments were made with structured and well-validated instruments, including parent ratings of developmental history.

Results
No differences were found between the groups regarding self-reported sociodemographic variables. The ASD group showed a higher number of co-occurring psychiatric disorders than the non-ASD group (t(88) = 5.17, 95\% CI 1.29–2.91, d = 1.19). Functional level was lower in the ASD group (t(88) = −2.66, 95\% CI −9.46 to −1.27, d = −0.73), and was predicted by the number of co-occurring psychiatric disorders.

Conclusions
The results underscore the need for thorough assessment of psychiatric disorders in autistic patients in adult psychiatric services. ASD should be considered as a possible ‘underlying’ condition in adult psychiatry, and there is no easy way of ruling out ASD in this population.},
	language = {en},
	number = {3},
	urldate = {2025-02-25},
	journal = {BJPsych Open},
	author = {Nyrenius, Johan and Eberhard, Jonas and Ghaziuddin, Mohammad and Gillberg, Christopher and Billstedt, Eva},
	month = may,
	year = {2023},
	keywords = {neurodevelopmental disorders, Autism spectrum disorder, adults, comorbidity, psychiatry},
	pages = {e89},
	file = {2023_Nyrenius et al._The ‘lost generation’ in adult psychiatry psychiatric, neurodevelopmental and sociodemographic char.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2023_Nyrenius et al._The ‘lost generation’ in adult psychiatry psychiatric, neurodevelopmental and sociodemographic char.pdf:application/pdf},
}

@article{eberhard2022,
	title = {Neurodevelopmental disorders and comorbidity in young adults attending a psychiatric outpatient clinic},
	volume = {313},
	issn = {0165-1781},
	url = {https://www.sciencedirect.com/science/article/pii/S0165178122002384},
	doi = {10.1016/j.psychres.2022.114638},
	abstract = {“Missed” cases with neurodevelopmental disorders (NDDs) within adult psychiatry services have attracted increasing attention in the last decade. Key questions have been what the prevalence of NDDs (particularly attention-deficit/hyperactivity disorder/ADHD and autism spectrum disorder/ASD) is, and what the clinical and gender characteristics of those with NDD in adult psychiatry are. All first-time attenders at an adult psychiatry clinic serving 18–25 years old were invited to take part in the study regardless of cause of concern. Participation in the study included diagnostic in-depth evaluation performed by experienced adult psychiatrists. Clinical diagnoses (DSM-IV-TR and DSM-5 criteria) were based on all available information (clinical psychiatric interview, clinical observation, and self-rating questionnaires). Almost two thirds (63\%) of the study group met criteria for ADHD or ASD. Most of the patients with NDD (particularly the "NDD females") had not been diagnosed in childhood. Twelve percent of the females included had been given an ADHD diagnosis in childhood. In the current study we found that 48\% of the females had ADHD. The high male:female NDD ratio reported among children, was not obvious in our NDD group. The results underscore the importance of screening for NDD in adult psychiatric services regardless of referral reason.},
	urldate = {2025-02-25},
	journal = {Psychiatry Research},
	author = {Eberhard, David and Billstedt, Eva and Gillberg, Christopher},
	month = jul,
	year = {2022},
	keywords = {ADHD, Autism spectrum disorders, Prevalence, Adult psychiatry},
	pages = {114638},
	file = {2022_Eberhard et al._Neurodevelopmental disorders and comorbidity in young adults attending a psychiatric outpatient clin.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2022_Eberhard et al._Neurodevelopmental disorders and comorbidity in young adults attending a psychiatric outpatient clin.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\B23TEE5C\\S0165178122002384.html:text/html},
}

@article{vanthof2021,
	title = {Age at autism spectrum disorder diagnosis: {A} systematic review and meta-analysis from 2012 to 2019},
	volume = {25},
	issn = {1362-3613},
	shorttitle = {Age at autism spectrum disorder diagnosis},
	url = {https://doi.org/10.1177/1362361320971107},
	doi = {10.1177/1362361320971107},
	abstract = {Between 1990 and 2012, the global mean age at diagnosis of autism spectrum disorder ranged from 38 to 120 months. Measures have since been introduced to reduce the age at autism spectrum disorder diagnosis, but the current global mean age is unknown. This review and meta-analysis report the average age at diagnosis from studies published between 2012 and 2019. We initially identified 1150 articles, including 56 studies that reported the mean or median age at diagnosis across 40 countries (n = 120,540 individuals with autism spectrum disorder). Meta-analysis results (on 35 studies, including 55 cohorts from 35 countries, n = 66,966 individuals with autism spectrum disorder) found a current mean age at diagnosis of 60.48 months (range: 30.90–234.57 months). The subgroup analysis for studies that only included children aged ⩽10 years (nine studies, including 26 cohorts from 23 countries, n = 18,134 children with autism spectrum disorder) showed a mean age at diagnosis of 43.18 months (range: 30.90–74.70 months). Numerous factors may influence age at diagnosis and were reported by 46 studies, often with conflicting or inconclusive findings. Our study is the first to ascertain the global average age at autism spectrum disorder diagnosis from a meta-analysis. Continued efforts to lower the average age at autism spectrum disorder diagnosis are needed.
Lay abstract
We currently assume that the global mean age at diagnosis of autism spectrum disorder ranges from 38 to 120 months. However, this range is based on studies from 1991 to 2012 and measures have since been introduced to reduce the age at autism spectrum disorder diagnosis. We performed a systematic review and meta-analysis (statistical analysis that combines the results of multiple scientific studies) for studies published between 2012 and 2019 to evaluate the current age at autism spectrum disorder diagnosis. We included 56 studies that reported the age at diagnosis for 40 countries (containing 120,540 individuals with autism spectrum disorder). Results showed the current mean age at diagnosis to be 60.48 months (range: 30.90–234.57 months) and 43.18 months (range: 30.90–74.70 months) for studies that only included children aged ⩽10 years. Numerous factors that may influence age at diagnosis (e.g. type of autism spectrum disorder diagnosis, additional diagnoses and gender) were reported by 46 studies, often with conflicting or inconclusive results. Our study is the first to determine the global average age at autism spectrum disorder diagnosis from a meta-analysis. Although progress is being made in the earlier detection of autism spectrum disorder, it requires our constant attention.},
	language = {en},
	number = {4},
	urldate = {2025-02-25},
	journal = {Autism},
	author = {van ’t Hof, Maarten and Tisseur, Chanel and van Berckelear-Onnes, Ina and van Nieuwenhuyzen, Annemyn and Daniels, Amy M and Deen, Mathijs and Hoek, Hans W and Ester, Wietske A},
	month = may,
	year = {2021},
	note = {Publisher: SAGE Publications Ltd},
	pages = {862--873},
	file = {2021_van ’t Hof et al._Age at autism spectrum disorder diagnosis A systematic review and meta-analysis from 2012 to 2019.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\2021_van ’t Hof et al._Age at autism spectrum disorder diagnosis A systematic review and meta-analysis from 2012 to 2019.pdf:application/pdf},
}

@article{bui2022,
	title = {Identifying {Potential} {Gamification} {Elements} for {A} {New} {Chatbot} for {Families} {With} {Neurodevelopmental} {Disorders}: {User}-{Centered} {Design} {Approach}},
	volume = {9},
	issn = {2292-9495},
	shorttitle = {Identifying {Potential} {Gamification} {Elements} for {A} {New} {Chatbot} for {Families} {With} {Neurodevelopmental} {Disorders}},
	doi = {10.2196/31991},
	abstract = {BACKGROUND: Chatbots have been increasingly considered for applications in the health care field. However, it remains unclear how a chatbot can assist users with complex health needs, such as parents of children with neurodevelopmental disorders (NDDs) who need ongoing support. Often, this population must deal with complex and overwhelming health information, which can make parents less likely to use a software that may be very helpful. An approach to enhance user engagement is incorporating game elements in nongame contexts, known as gamification. Gamification needs to be tailored to users; however, there has been no previous assessment of gamification use in chatbots for NDDs.
OBJECTIVE: We sought to examine how gamification elements are perceived and whether their implementation in chatbots will be well received among parents of children with NDDs. We have discussed some elements in detail as the initial step of the project.
METHODS: We performed a narrative literature review of gamification elements, specifically those used in health and education. Among the elements identified in the literature, our health and social science experts in NDDs prioritized five elements for in-depth discussion: goal setting, customization, rewards, social networking, and unlockable content. We used a qualitative approach, which included focus groups and interviews with parents of children with NDDs (N=21), to assess the acceptability of the potential implementation of these elements in an NDD-focused chatbot. Parents were asked about their opinions on the 5 elements and to rate them. Video and audio recordings were transcribed and summarized for emerging themes, using deductive and inductive thematic approaches.
RESULTS: From the responses obtained from 21 participants, we identified three main themes: parents of children with NDDs were familiar with and had positive experiences with gamification; a specific element (goal setting) was important to all parents, whereas others (customization, rewards, and unlockable content) received mixed opinions; and the social networking element received positive feedback, but concerns about information accuracy were raised.
CONCLUSIONS: We showed for the first time that parents of children with NDDs support gamification use in a chatbot for NDDs. Our study illustrates the need for a user-centered design in the medical domain and provides a foundation for researchers interested in developing chatbots for populations that are medically vulnerable. Future studies exploring wide range of gamification elements with large number of potential users are needed to understand the impact of gamification elements in enhancing knowledge mobilization.},
	language = {eng},
	number = {3},
	journal = {JMIR human factors},
	author = {Bui, Truong An and Pohl, Megan and Rosenfelt, Cory and Ogourtsova, Tatiana and Yousef, Mahdieh and Whitlock, Kerri and Majnemer, Annette and Nicholas, David and Demmans Epp, Carrie and Zaiane, Osmar and Bolduc, François V.},
	month = aug,
	year = {2022},
	pmid = {35984679},
	pmcid = {PMC9440405},
	keywords = {neurodevelopmental disorders, eHealth, mHealth, engagement, mobile health, chatbot, focus group, gamification, health information technologies, interview, user-centered design},
	pages = {e31991},
	file = {2022_Bui et al._Identifying Potential Gamification Elements for A New Chatbot for Families With Neurodevelopmental D.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2022_Bui et al._Identifying Potential Gamification Elements for A New Chatbot for Families With Neurodevelopmental D.pdf:application/pdf},
}

@article{wu2023,
	title = {Global trends and hotspots in the digital therapeutics of autism spectrum disorders: a bibliometric analysis from 2002 to 2022},
	volume = {14},
	issn = {1664-0640},
	shorttitle = {Global trends and hotspots in the digital therapeutics of autism spectrum disorders},
	url = {https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2023.1126404/full},
	doi = {10.3389/fpsyt.2023.1126404},
	abstract = {{\textless}sec{\textgreater}{\textless}title{\textgreater}Introduction{\textless}/title{\textgreater}{\textless}p{\textgreater}Autism spectrum disorder (ASD) is a severe neurodevelopmental disorder that has become a major cause of disability in children. Digital therapeutics (DTx) delivers evidence-based therapeutic interventions to patients that are driven by software to prevent, manage, or treat a medical disorder or disease. This study objectively analyzed the current research status of global DTx in ASD from 2002 to 2022, aiming to explore the current global research status and trends in the field.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Methods{\textless}/title{\textgreater}{\textless}p{\textgreater}The Web of Science database was searched for articles about DTx in ASD from January 2002 to October 2022. CiteSpace was used to analyze the co-occurrence of keywords in literature, partnerships between authors, institutions, and countries, the sudden occurrence of keywords, clustering of keywords over time, and analysis of references, cited authors, and cited journals.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Results{\textless}/title{\textgreater}{\textless}p{\textgreater}A total of 509 articles were included. The most productive country and institution were the United States and Vanderbilt University. The largest contributing authors were Warren, Zachary, and Sarkar, Nilanjan. The most-cited journal was the {\textless}italic{\textgreater}Journal of Autism and Developmental Disorders{\textless}/italic{\textgreater}. The most-cited and co-cited articles were Brian Scarselati (Robots for Use in Autism Research, 2012) and Ralph Adolphs (Abnormal processing of social information from faces in autism, 2001). “Artificial Intelligence,” “machine learning,” “Virtual Reality,” and “eye tracking” were common new and cutting-edge trends in research on DTx in ASD.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Discussion{\textless}/title{\textgreater}{\textless}p{\textgreater}The use of DTx in ASD is developing rapidly and gaining the attention of researchers worldwide. The publications in this field have increased year by year, mainly concentrated in the developed countries, especially in the United States. Both Vanderbilt University and Yale University are very important institutions in the field. The researcher from Vanderbilt University, Warren and Zachary, his dynamics or achievements in the field is also more worth our attention. The application of new technologies such as virtual reality, machine learning, and eye-tracking in this field has driven the development of DTx on ASD and is currently a popular research topic. More cross-regional and cross-disciplinary collaborations are recommended to advance the development and availability of DTx.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}},
	language = {English},
	urldate = {2025-02-25},
	journal = {Frontiers in Psychiatry},
	author = {Wu, Xuesen and Deng, Haiyin and Jian, Shiyun and Chen, Huian and Li, Qing and Gong, Ruiyu and Wu, Jingsong},
	month = may,
	year = {2023},
	note = {Publisher: Frontiers},
	keywords = {autism, Bibliometrics, Citespace, digital therapeutics, Knowledge mapping},
	file = {2023_Wu et al._Global trends and hotspots in the digital therapeutics of autism spectrum disorders a bibliometric.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2023_Wu et al._Global trends and hotspots in the digital therapeutics of autism spectrum disorders a bibliometric.pdf:application/pdf},
}

@article{corona2024,
	title = {A {Randomized} {Trial} of the {Accuracy} of {Novel} {Telehealth} {Instruments} for the {Assessment} of {Autism} in {Toddlers}},
	volume = {54},
	issn = {1573-3432},
	url = {https://doi.org/10.1007/s10803-023-05908-9},
	doi = {10.1007/s10803-023-05908-9},
	abstract = {Purpose: Telemedicine approaches to autism (ASD) assessment have become increasingly common, yet few validated tools exist for this purpose. This study presents results from a clinical trial investigating two approaches to tele-assessment for ASD in toddlers. Methods: 144 children (29\% female) between 17 and 36 months of age (mean = 2.5 years, SD = 0.33 years) completed tele-assessment using either the TELE-ASD-PEDS (TAP) or an experimental remote administration of the Screening Tool for Autism in Toddlers (STAT). All children then completed traditional in-person assessment with a blinded clinician, using the Mullen Scales of Early Learning (MSEL), Vineland Adaptive Behavior Scales, 3rd Edition (VABS-3), and Autism Diagnostic Observation Schedule (ADOS-2). Both tele-assessment and in-person assessment included a clinical interview with caregivers. Results: Results indicated diagnostic agreement for 92\% of participants. Children diagnosed with ASD following in-person assessment who were missed by tele-assessment (n = 8) had lower scores on tele- and in-person ASD assessment tools. Children inaccurately identified as having ASD by tele-assessment (n = 3) were younger than other children and had higher developmental and adaptive behavior scores than children accurately diagnosed with ASD by tele-assessment. Diagnostic certainty was highest for children correctly identified as having ASD via tele-assessment. Clinicians and caregivers reported satisfaction with tele-assessment procedures. Conclusion: This work provides additional support for the use of tele-assessment for identification of ASD in toddlers, with both clinicians and families reporting broad acceptability. Continued development and refinement of tele-assessment procedures is recommended to optimize this approach for the needs of varying clinicians, families, and circumstances.},
	language = {en},
	number = {6},
	urldate = {2025-02-25},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Corona, Laura L. and Wagner, Liliana and Hooper, Madison and Weitlauf, Amy and Foster, Tori E. and Hine, Jeffrey and Miceli, Alexandra and Nicholson, Amy and Stone, Caitlin and Vehorn, Alison and Warren, Zachary},
	month = jun,
	year = {2024},
	keywords = {Autism spectrum disorder, Assessment, Telemedicine, Young children},
	pages = {2069--2080},
	file = {2024_Corona et al._A Randomized Trial of the Accuracy of Novel Telehealth Instruments for the Assessment of Autism in T.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Corona et al._A Randomized Trial of the Accuracy of Novel Telehealth Instruments for the Assessment of Autism in T.pdf:application/pdf},
}

@misc{alvari2024,
	title = {Exploring {Physiological} {Responses} in {Virtual} {Reality}-based {Interventions} for {Autism} {Spectrum} {Disorder}: {A} {Data}-{Driven} {Investigation}},
	shorttitle = {Exploring {Physiological} {Responses} in {Virtual} {Reality}-based {Interventions} for {Autism} {Spectrum} {Disorder}},
	url = {http://arxiv.org/abs/2404.07159},
	doi = {10.48550/arXiv.2404.07159},
	abstract = {Virtual Reality (VR) has emerged as a promising tool for enhancing social skills and emotional well-being in individuals with Autism Spectrum Disorder (ASD). Through a technical exploration, this study employs a multiplayer serious gaming environment within VR, engaging 34 individuals diagnosed with ASD and employing high-precision biosensors for a comprehensive view of the participants' arousal and responses during the VR sessions. Participants were subjected to a series of 3 virtual scenarios designed in collaboration with stakeholders and clinical experts to promote socio-cognitive skills and emotional regulation in a controlled and structured virtual environment. We combined the framework with wearable non-invasive sensors for bio-signal acquisition, focusing on the collection of heart rate variability, and respiratory patterns to monitor participants behaviors. Further, behavioral assessments were conducted using observation and semi-structured interviews, with the data analyzed in conjunction with physiological measures to identify correlations and explore digital-intervention efficacy. Preliminary analysis revealed significant correlations between physiological responses and behavioral outcomes, indicating the potential of physiological feedback to enhance VR-based interventions for ASD. The study demonstrated the feasibility of using real-time data to adapt virtual scenarios, suggesting a promising avenue to support personalized therapy. The integration of quantitative physiological feedback into digital platforms represents a forward step in the personalized intervention for ASD. By leveraging real-time data to adjust therapeutic content, this approach promises to enhance the efficacy and engagement of digital-based therapies.},
	urldate = {2025-02-25},
	publisher = {arXiv},
	author = {Alvari, Gianpaolo and Vallefuoco, Ersilia and Cristofolini, Melanie and Salvadori, Elio and Dianti, Marco and Moltani, Alessia and Castello, Davide Dal and Venuti, Paola and Furlanello, Cesare},
	month = apr,
	year = {2024},
	note = {arXiv:2404.07159 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Human-Computer Interaction},
	file = {2024_Alvari et al._Exploring Physiological Responses in Virtual Reality-based Interventions for Autism Spectrum Disorde.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2024_Alvari et al._Exploring Physiological Responses in Virtual Reality-based Interventions for Autism Spectrum Disorde.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\3FB9S2NS\\2404.html:text/html},
}

@article{wagner2025,
	title = {Development of a {Novel} {Telemedicine} {Tool} to {Reduce} {Disparities} {Related} to the {Identification} of {Preschool} {Children} with {Autism}},
	volume = {55},
	issn = {1573-3432},
	url = {https://doi.org/10.1007/s10803-023-06176-3},
	doi = {10.1007/s10803-023-06176-3},
	abstract = {The wait for ASD evaluation dramatically increases with age, with wait times of a year or more common as children reach preschool. Even when appointments become available, families from traditionally underserved groups struggle to access care. Addressing care disparities requires designing identification tools and processes specifically for and with individuals most at-risk for health inequities. This work describes the development of a novel telemedicine-based ASD assessment tool, the TELE-ASD-PEDS-Preschool (TAP-Preschool). We applied machine learning models to a clinical data set of preschoolers with ASD and other developmental concerns (n = 914) to generate behavioral targets that best distinguish ASD and non-ASD features. We conducted focus groups with clinicians, early interventionists, and parents of children with ASD from traditionally underrepresented racial/ethnic and linguistic groups. Focus group themes and machine learning analyses were used to generate a play-based instrument with assessment tasks and scoring procedures based on the child’s language (i.e., TAP-P Verbal, TAP-P Non-verbal). TAP-P procedures were piloted with 30 families. Use of the instrument in isolation (i.e., without history or collateral information) yielded accurate diagnostic classification in 63\% of cases. Children with existing ASD diagnoses received higher TAP-P scores, relative to children with other developmental concerns. Clinician diagnostic accuracy and certainty were higher when confirming existing ASD diagnoses (80\% agreement) than when ruling out ASD in children with other developmental concerns (30\% agreement). Utilizing an equity approach to understand the functionality and impact of tele-assessment for preschool children has potential to transform the ASD evaluation process and improve care access.},
	language = {en},
	number = {1},
	urldate = {2025-02-25},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Wagner, Liliana and Vehorn, Alison and Weitlauf, Amy S. and Lavanderos, Ambar Munoz and Wade, Joshua and Corona, Laura and Warren, Zachary},
	month = jan,
	year = {2025},
	keywords = {Autism spectrum disorder, Neurodevelopmental Disorders, Telemedicine, Preschool children, Tele-assessment},
	pages = {30--42},
	file = {2025_Wagner et al._Development of a Novel Telemedicine Tool to Reduce Disparities Related to the Identification of Pres.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\ASDDetection\\2025_Wagner et al._Development of a Novel Telemedicine Tool to Reduce Disparities Related to the Identification of Pres.pdf:application/pdf},
}

@article{haeyen2024,
	title = {Arts and psychomotor therapies in personality disorder treatment: {An} appropriate therapeutic entrance to personal development: {A} commentary},
	volume = {80},
	copyright = {© 2024 The Author(s). Journal of Clinical Psychology published by Wiley Periodicals LLC.},
	issn = {1097-4679},
	shorttitle = {Arts and psychomotor therapies in personality disorder treatment},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jclp.23730},
	doi = {10.1002/jclp.23730},
	abstract = {Personality disorders (PD) are based not just on maladaptive ideas about self and others, they also are grounded on embodied patterns of behaviors and reactions to interpersonal stressors. There is growing interest in working with the body and through the body so to address automatisms that lead to suffering and dysfunctional social action. In this issue of the Journal of Clinical Psychology: In-Session the use of art and psychomotor therapies for these patients was explored by seven different clinical perspectives. Patients described presented with different PD and associated symptoms. The arts and psychomotor therapies deployed in personality disorder treatment are: (visual) art therapy, music therapy, drama therapy, dance (movement) therapy, and psychomotor therapy making psychotherapeutic use of the different modalities: art, music, play, role-play, performance, improvisation, dance, body awareness and movement. Interventions provide kinesthetic, sensory, perceptual, and symbolic opportunities to invite alternative modes of meaning-making, accessing own needs and wishes, and communicating them to others. In this commentary we summarize some of the different topics covered by the clinical-based papers, including working mechanisms of arts and psychomotor therapies, the importance of bottom-up emotion regulation processes, how to treat trauma in the presence of a PD, how to integrate art and psychomotor therapies in a fine-grained formulation and how to understand the process of change. Although there is a need for more empirical research, we hope this issue makes a solid case that clinicians can effectively include art and psychomotor therapies when treating the full range of PD.},
	language = {en},
	number = {11},
	urldate = {2025-03-03},
	journal = {Journal of Clinical Psychology},
	author = {Haeyen, Suzanne and Dimaggio, Giancarlo},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jclp.23730},
	keywords = {creative art therapies, personality disorders, psychomotor therapy},
	pages = {2303--2314},
	file = {2024_Haeyen and Dimaggio_Arts and psychomotor therapies in personality disorder treatment An appropriate therapeutic entranc.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\psychomotor\\2024_Haeyen and Dimaggio_Arts and psychomotor therapies in personality disorder treatment An appropriate therapeutic entranc.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\I7CRFRXJ\\jclp.html:text/html},
}

@article{vriend2024,
	title = {Efficacy of psychomotor therapy for children and adolescents with anxiety disorders—a systematic literature review},
	volume = {2},
	issn = {2813-4540},
	url = {https://www.frontiersin.org/journals/child-and-adolescent-psychiatry/articles/10.3389/frcha.2023.1182188/full},
	doi = {10.3389/frcha.2023.1182188},
	abstract = {{\textless}p{\textgreater}Specific Phobia (SP), Generalized Anxiety Disorders (GAD), and Social Anxiety Disorder (SAD) are the most prevalent anxiety disorders in children and adolescents. Although anxiety has a major influence on the body, evidence-based treatments mainly focus on cognitive and behavioral aspects of anxiety. Body- and movement-oriented interventions, such as psychomotor therapy (PMT), address the physical aspects. Bodily experience and interoceptive awareness are used to change behavior, cognition, and emotions. This review aimed to provide an overview of the efficacy of PMT for children and adolescents aged 0–18 years with SP, GAD, or SAD.{\textless}/p{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Method{\textless}/title{\textgreater}{\textless}p{\textgreater}Data were collected in PsycINFO, Medline, Embase, ERIC, and Web of Science, from January 2020 until April 2022. Two independent researchers (EV and JM) selected the articles and performed a critical appraisal.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Results{\textless}/title{\textgreater}{\textless}p{\textgreater}From 1,438 articles found, only one article met the inclusion criteria.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}{\textless}sec{\textgreater}{\textless}title{\textgreater}Conclusion{\textless}/title{\textgreater}{\textless}p{\textgreater}No consensus-based statement about the efficacy of PMT in children and adolescents with SP, GAD, or SAD can be made due to the gap in the literature. Future research is needed to evaluate the efficacy. The first step may be to design treatment protocols. Subsequently, these protocols may be evaluated concerning efficacy.{\textless}/p{\textgreater}{\textless}/sec{\textgreater}},
	language = {English},
	urldate = {2025-03-03},
	journal = {Frontiers in Child and Adolescent Psychiatry},
	author = {Vriend, Evelien and Moeijes, Janet and Scheffers, Mia},
	month = jan,
	year = {2024},
	note = {Publisher: Frontiers},
	keywords = {adolescents, Anxiety Disorders, body-and movement-based therapy, Children, Psychomotor therapy, Systematic review},
	file = {2024_Vriend et al._Efficacy of psychomotor therapy for children and adolescents with anxiety disorders—a systematic lit.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\psychomotor\\2024_Vriend et al._Efficacy of psychomotor therapy for children and adolescents with anxiety disorders—a systematic lit.pdf:application/pdf},
}

@article{jans2025,
	title = {Developing a virtual reality application for online arts and psychomotor therapies using action research},
	volume = {6},
	issn = {2673-4192},
	url = {https://www.frontiersin.org/journals/virtual-reality/articles/10.3389/frvir.2025.1462775/full},
	doi = {10.3389/frvir.2025.1462775},
	language = {English},
	urldate = {2025-03-03},
	journal = {Frontiers in Virtual Reality},
	author = {Jans, Nathalie and Wouters, Hans and Kolijn, Joep and Haeyen, Suzanne},
	month = jan,
	year = {2025},
	note = {Publisher: Frontiers},
	keywords = {art therapies, dance therapy, drama therapy, e-health, full body multiplayer, music therapy, psychomotor therapy, virtual reality},
	file = {2025_Jans et al._Developing a virtual reality application for online arts and psychomotor therapies using action rese.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\psychomotor\\2025_Jans et al._Developing a virtual reality application for online arts and psychomotor therapies using action rese.pdf:application/pdf},
}

@article{facultyofsocialsciencemasterprogramofpsychomotricity-psycho-traumaandlectureratuniversiteanglicandebukavudrcandrudolphkwanueuniversity-liberia.2024,
	title = {Psychomotor {Therapy} {Using} the {Body} {Schema} for the {Intellectually} {Disabled} at the {A}.{D}.{A}.{R}-{Tubahoze} {Centre}},
	volume = {07},
	issn = {25818341},
	url = {https://ijcsrr.org/single-view/?id=15337&pid=15277},
	doi = {10.47191/ijcsrr/V7-i3-08},
	abstract = {This study is the preliminary research focused on psychomotor therapy and body schemas for the intellectually disabled, hence the motor dimension with a test of balance, coordination and jumping in the pre-test and post-test of a BPM measurement at 60, 90 and 120 (BPM) with the YO and YC. The cognitive dimension with, for example, the Berces and Lezine test of body control and latero-spatial organization, the Piaget and Head tests of gesture imitation and latero-spatial organization. The affective dimension revolving around self-esteem was measured with the self-perception profile for adults with intellectual disabilities (SPPD) on physical appearance, athletic competence and psychomotor competence. At the end of the verification of research question and using methodological approach adopted on the present study on the motor dimension, the cognitive dimension and the affective dimension; it was found that with the motor dimension on the balance test, a trend on the performance of our intellectually disabled patients from A.D.A.R-Tubahoze centre during the psychomotor therapy sessions was tested positive in the post-test compared to the pre-test (BYO and BYC).With regard to the coordination test (CYO and CYC) at post-test and pre-test, the statistical frequencies with overall averages show that our intellectually impaired patients tended to obtain better results at posttest than at pre-test; this shows a success in the applicability of motor therapy to coordination disorder in intellectually impaired patients. Similarly, a positive performance trend was shown in the results of the jumping test at post-test than at pre-test. The trend in the results of the jump test (JYO and JYC) in the post-test than in the pre-test would have shown a positive result after our therapeutic-motor sessions with the IDs of the A.D.A.R. centre. From the cognitive dimension, using the test of imitation of gestures and lateral-spatial on different movements, has a score of 10 points in the test of imitation of simple gestures of hand movements, it would have been observed in MID patients that the test proved positive; with the Piaget’s Head test of latero-spatial organization administered to MID which was evaluated at a score of 40 points, had as a positive performance to all patients who took this test. The results of our work on the affective dimension of the dominant modality “Rather true”, was observed on items 12, 16, 20 and 24 of self-esteem, with physical appearance vis-à-vis items (5,10,18 and 22) and items 5 and 9 and then items 17, 13 and 1 of athletic competence; from items with a “Rather positive” modality with items 11, 3 and 2 3, 7, 15 and 19 of our MID patients that a positive trend on positive appreciation was satisfactorily observable.},
	language = {en},
	number = {03},
	urldate = {2025-03-03},
	journal = {International Journal of Current Science Research and Review},
	author = {{Faculty of social science master program of psychomotricity-Psycho-trauma and lecturer at Université Anglican de Bukavu (DRC) and Rudolph Kwanue University-Liberia.} and Dieudonné, Safari and Alice, Cyuzuzo and {Midwife at Kibirizi Hospital and College Saint Bernard Kansi as midwife teacher, south province of Rwanda.}},
	month = mar,
	year = {2024},
	file = {2024_Faculty of social science master program of psychomotricity-Psycho-trauma and lecturer at Université Anglican de Bukavu (DRC) and Rudolph Kwanue University-Liberia. et al._Psychomotor Therapy Using the Body Schema for the Intellectually Disabled at the A.D.A.R-Tubahoze Ce.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\psychomotor\\08-0603-2024.pdf:application/pdf},
}

@article{wang2024,
	title = {Augmented reality-based knowledge transfer for facility management: {A} systematic review},
	volume = {98},
	issn = {2352-7102},
	shorttitle = {Augmented reality-based knowledge transfer for facility management},
	url = {https://www.sciencedirect.com/science/article/pii/S2352710224027542},
	doi = {10.1016/j.jobe.2024.111186},
	abstract = {Facility management is globally recognized as a critical and knowledge-intensive field. It confronts challenges due to the increasing complexity of facilities and the vast amount of operation and maintenance information. Augmented reality (AR), emerges as a promising technology to address these challenges by enhancing knowledge transfer in facility management through immersive and interactive environments. The objective of this research is to conduct a systematic review of augmented reality applications in facility management, and explore its knowledge transfer capabilities in facility management. The review narrowed the literature by filtering from 618 publications to 107 papers published between 2011 and 2023. A contents analysis based on these articles could gain an insight into current facility management systems as well as augmented reality application domains in a series of facility management activities including facility design/layout, assembly, monitoring, maintenance and renovation. Based on the above analysis, this research discussed the augmented reality capabilities of knowledge transfer in facility management. The findings reveal that augmented reality facilitates knowledge transfer by enabling visualization, interaction, and collaboration among stakeholders. It is influenced by factors including user interfaces and interaction types, as well as vocational training and acceptance level of stakeholders. Challenges remain in terms of tracking, registration, ergonomics, security, and management practices. Finally, this research highlighted future trends focusing on the development of knowledge-driven digital twins, trustworthy knowledge management, and collaborative platforms to further promote augmented reality knowledge transfer in facility management. This research also contributes to deeper understanding of the role of augmented reality within this field and provides insights for stakeholders aiming to utilize this technology.},
	urldate = {2025-03-04},
	journal = {Journal of Building Engineering},
	author = {Wang, Xiang and Wang, Shiqi and Xiao, Fu and Luo, Xiaowei},
	month = dec,
	year = {2024},
	keywords = {Architecture engineering and construction industry, Augmented reality, Facility management, Knowledge transfer, Operation},
	pages = {111186},
	file = {2024_Wang et al._Augmented reality-based knowledge transfer for facility management A systematic review.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2024_Wang et al._Augmented reality-based knowledge transfer for facility management A systematic review.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\8CJZM2MI\\S2352710224027542.html:text/html},
}

@article{fenoglio2022,
	title = {Tacit knowledge elicitation process for industry 4.0},
	volume = {2},
	issn = {2731-0809},
	url = {https://doi.org/10.1007/s44163-022-00020-w},
	doi = {10.1007/s44163-022-00020-w},
	abstract = {Manufacturers migrate their processes to Industry 4.0, which includes new technologies for improving productivity and efficiency of operations. One of the issues is capturing, recreating, and documenting the tacit knowledge of the aging workers. However, there are no systematic procedures to incorporate this knowledge into Enterprise Resource Planning systems and maintain a competitive advantage. This paper describes a solution proposal for a tacit knowledge elicitation process for capturing operational best practices of experienced workers in industrial domains based on a mix of algorithmic techniques and a cooperative game. We use domain ontologies for Industry 4.0 and reasoning techniques to discover and integrate new facts from textual sources into an Operational Knowledge Graph. We describe a concepts formation iterative process in a role game played by human and virtual agents through socialization and externalization for knowledge graph refinement. Ethical and societal concerns are discussed as well.},
	language = {en},
	number = {1},
	urldate = {2025-03-04},
	journal = {Discover Artificial Intelligence},
	author = {Fenoglio, Enzo and Kazim, Emre and Latapie, Hugo and Koshiyama, Adriano},
	month = mar,
	year = {2022},
	keywords = {Artificial Intelligence, AI ethics, Concept maps, Knowledge graph, Knowledge management, Ontology, Tacit knowledge},
	pages = {6},
	file = {2022_Fenoglio et al._Tacit knowledge elicitation process for industry 4.0.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2022_Fenoglio et al._Tacit knowledge elicitation process for industry 4.0.pdf:application/pdf},
}

@article{naqvi2024,
	title = {Unlocking maintenance insights in industrial text through semantic search},
	volume = {157-158},
	issn = {0166-3615},
	url = {https://www.sciencedirect.com/science/article/pii/S0166361524000113},
	doi = {10.1016/j.compind.2024.104083},
	abstract = {Maintenance records in Computerized Maintenance Management Systems (CMMS) contain valuable human knowledge on maintenance activities. These records primarily consist of noisy and unstructured texts written by maintenance experts. The technical nature of the text, combined with a concise writing style and frequent use of abbreviations, makes it difficult to be processed through classical Natural Language Processing (NLP) pipelines. Due to these complexities, this text must be normalized before feeding to classical machine learning models. Developing these custom normalization pipelines requires manual labor and domain expertise and is a time-consuming process that demands constant updates. This leads to the under-utilization of this valuable source of information to generate insights to help with maintenance decision support. This study proposes a Technical Language Processing (TLP) pipeline for semantic search in industrial text using BERT (Bidirectional Encoder Representations), a transformer-based Large Language Model (LLM). The proposed pipeline can automatically process complex unstructured industrial text and does not require custom preprocessing. To adapt the BERT model for the target domain, three unsupervised domain fine-tuning techniques are compared to identify the best strategy for leveraging available tacit knowledge in industrial text. The proposed approach is validated on two industrial maintenance records from the mining and aviation domains. Semantic search results are analyzed from a quantitative and qualitative perspective. Analysis shows that TSDAE, a state-of-the-art unsupervised domain fine-tuning technique, can efficiently identify intricate patterns in the industrial text regardless of associated complexities. BERT model fine-tuned with TSDAE on industrial text achieved a precision of 0.94 and 0.97 for mining excavators and aviation maintenance records, respectively.},
	urldate = {2025-03-04},
	journal = {Computers in Industry},
	author = {Naqvi, Syed Meesam Raza and Ghufran, Mohammad and Varnier, Christophe and Nicod, Jean-Marc and Javed, Kamran and Zerhouni, Noureddine},
	month = may,
	year = {2024},
	keywords = {Industrial information retrieval, Large Language Models, Maintenance decision support, Navigating human knowledge, Semantic search, Technical Language Processing},
	pages = {104083},
	file = {2024_Unlocking maintenance insights in industrial text through semantic search.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2024_Unlocking maintenance insights in industrial text through semantic search.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\KI5CF2DU\\S0166361524000113.html:text/html},
}

@inproceedings{armengol-urpi2024,
	title = {{WIP}: {Making} {Implicit} {Knowledge} {Explicit} - {A} {Data}-{Driven} {Approach} to {Improve} {Knowledge} {Transfer} in a {Glassblowing} {Beginners} {Class}},
	shorttitle = {{WIP}},
	url = {https://ieeexplore.ieee.org/document/10893143},
	doi = {10.1109/FIE61694.2024.10893143},
	abstract = {This work-in-progress innovative practice paper presents a novel approach to 1) extract tacit knowledge from expert trainers while they perform a task demo, 2) decrease the learner's cognitive load via the use of instructional videos portraying the variables at play during a task demonstration, and 3) define quantifiable metrics of expertise by extracting features that differentiate experts from novice practitioners. Implicit or tacit knowledge is know-how that experts develop with experience and is difficult to verbalize, formalize or explicitly transfer to others. For this reason, knowledge transfer from expert to apprentice is usually slow and inefficient. Our approach seeks to support knowledge transfer using technology-enhanced approaches. Here, we focus on extracting and describing exper-tise. We do so by instrumenting experts, trainees and their tools with sensors that can help structure and formalize knowledge. Our first application of this framework is on the knowledge transfer between an expert and novice glassblower. Glassblowing is well known for its crucial expert/apprentice relation and its slow learning rate due in part to the difficulties in verbal transfer of skills. Our framework seeks to capture relevant data while an expert glassblower demonstrates basic actions in a beginners glass blowing course. Our sensors collect eye-tracking activity, verbal demo instructions, pipe accelerometry, air infusion, scene video and muscle activity (EMG), which continuously monitor the expert, their explanations, the tools, and the glass piece. We bring together all the sensed data into instructional videos to be used by novice learners as supportive training material. We present preliminary results related to metrics of expertise and future steps towards gathering similar data from novices. This will help develop AI-based models to extract data-driven differences between experts and apprentices, which can be used as further instructional material. We will also present plans to test the instructional effectiveness of the developed videos and how our approach can be used in other training settings involving tacit knowledge transfer.},
	urldate = {2025-03-04},
	booktitle = {2024 {IEEE} {Frontiers} in {Education} {Conference} ({FIE})},
	author = {Armengol-Urpi, Alexandre and Salazar-Gomez, Andres F. and Sarma, Sanjay E.},
	month = oct,
	year = {2024},
	note = {ISSN: 2377-634X},
	keywords = {Expert-novice, Feature extraction, Glass, Instruments, Knowledge transfer, Measurement, Scholarships, Sensors, Soft skills, Training, Videos, Visualization},
	pages = {1--5},
	file = {2024_Armengol-Urpi et al._WIP Making Implicit Knowledge Explicit - A Data-Driven Approach to Improve Knowledge Transfer in a.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2024_Armengol-Urpi et al._WIP Making Implicit Knowledge Explicit - A Data-Driven Approach to Improve Knowledge Transfer in a.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\4TF7TC7X\\10893143.html:text/html},
}

@inproceedings{freire2022,
	address = {New York, NY, USA},
	series = {{CUI} '22},
	title = {A {Conversational} {User} {Interface} for {Instructional} {Maintenance} {Reports}},
	isbn = {978-1-4503-9739-1},
	url = {https://dl.acm.org/doi/10.1145/3543829.3544516},
	doi = {10.1145/3543829.3544516},
	abstract = {Maintaining a complex system, such as a modern production line, is a knowledge-intensive task. Many firms use maintenance reports as a decision support tool. However, reports are often poor quality and tedious to compile. A Conversational User Interface (CUI) could streamline the reporting process by validating the user’s input, eliciting more valuable information, and reducing the time needed. In this paper, we use a Technology Probe to explore the potential of a CUI to create instructional maintenance reports. We conducted a between-groups study (N = 24) in which participants had to replace the inner tube of a bicycle tire. One group documented the procedure using a CUI while replacing the inner tube, whereas the other group compiled a paper report afterward. The CUI was enacted by a researcher according to a set of rules. Our results indicate that using a CUI for maintenance reports saves a significant amount of time, is no more cognitively demanding than writing a report, and results in maintenance reports of higher quality.},
	urldate = {2025-03-04},
	booktitle = {Proceedings of the 4th {Conference} on {Conversational} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Freire, Samuel Kernan and Niforatos, Evangelos and Rusak, Zoltan and Aschenbrenner, Doris and Bozzon, Alessandro},
	month = sep,
	year = {2022},
	pages = {1--6},
	file = {2022_Kernan Freire et al._A Conversational User Interface for Instructional Maintenance Reports.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2022_Kernan Freire et al._A Conversational User Interface for Instructional Maintenance Reports.pdf:application/pdf},
}

@inproceedings{mangala2024,
	title = {Tacit {Knowledge}-{Based} {Expert} {Model} for {Decision} {Support} in {Injection} {Mould} {Design}},
	url = {https://ieeexplore.ieee.org/document/10857747},
	doi = {10.1109/ICITR64794.2024.10857747},
	abstract = {Injection moulding is the most popular technique used by Sri Lankan small and medium-scale enterprises for producing plastic parts in large quantities. Once a strong industry that catered to over 50\% of local demand for dies and moulds are now barely meeting one-tenth of the demand due to import competition. Moreover, the local industry lacks access to state-of-the-art design tools owing to funding limitations and mainly relies on experience of long-standing mould design experts. The brain drain following country's economic downturn has now severely affected growth or even existence of this important industry. In this context, the paper introduces a unique strategy to digitally transform the tacit knowledge of mould design experts, and thereby establish a data-driven decision-making process for mould design. In order to manage both explicit and tacit knowledge in injection mould design, a framework was developed, and corresponding databases were established. The proposed expert model uses a predefined case bank and a case-based filtering algorithm to identify matching data sets for a given new design from explicit and tacit databases. Suitability of parameters of the new design is determined using a decision-making algorithm, where higher weightage is assigned to tacit knowledge-based on data availability. The expert model was validated using a case study, and results of the expert model were found to be significantly agreeable to the output of industry-standard mould design software. The findings indicate potential of the proposed tacit knowledge-based expert model to positively impact mould industry affected by low resources and help reach Industry 4.0.},
	urldate = {2025-03-04},
	booktitle = {2024 9th {International} {Conference} on {Information} {Technology} {Research} ({ICITR})},
	author = {Mangala, K.H.J. and Ranaweera, R.K.P.S. and Punchihewa, H.K.G.},
	month = dec,
	year = {2024},
	note = {ISSN: 2831-3399},
	keywords = {Databases, Software algorithms, Standards, Decision making, Data models, digital transformation, expert model, Industries, injection mould design, Knowledge based systems, knowledge management, Personnel, Software tools, tacit \& explicit knowledge, Transforms},
	pages = {1--6},
	file = {2024_Mangala et al._Tacit Knowledge-Based Expert Model for Decision Support in Injection Mould Design.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2024_Mangala et al._Tacit Knowledge-Based Expert Model for Decision Support in Injection Mould Design.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\antoine.widmer\\Zotero\\storage\\8VKPE2YB\\10857747.html:text/html},
}

@misc{nishijima2024,
	title = {Comparison of {Spatiotemporal} {Characteristics} of {Eye} {Movements} in {Non}-experts and the {Skill} {Transfer} {Effects} of {Gaze} {Guidance} and {Annotation} {Guidance}},
	url = {http://arxiv.org/abs/2412.17296},
	doi = {10.48550/arXiv.2412.17296},
	abstract = {Methods for converting the tacit knowledge of experts into explicit knowledge have drawn increasing attention. Gaze data has emerged as a valuable approach in this effort. However, the effective transfer of tacit knowledge remains a challenge. No studies have directly compared the effects of gaze-based and annotation-based guidance or adequately examined their impacts on skill improvement after instruction. This study examined the effects of gaze and annotation guidance on the spatiotemporal characteristics of eye movements and the skill transfer of expert evaluation techniques in karate kata performances. 28 non-expert participants were assigned to three groups: a gaze guidance group, an annotation guidance group, and a control group. Participants were presented with instructional slideshows based on the expert's gaze data and annotations. Before and after instruction, participants were asked to evaluate karate kata performance videos performed by practitioners with different skill levels. The results showed that the annotation guidance group exhibited an effect of directing gaze toward the presented areas of focus, with a trend of increased total number of fixation areas. On the other hand, while the gaze guidance group was not encouraged to make fixations on multiple areas, the possibility of promoting peripheral vision was inferred based on measurements from the eye-tracking system. Regarding ranking results, 71.4\% of participants in the gaze guidance group showed an improvement in ranking skills, with a trend toward better scoring ability compared to the other groups. This demonstrates the necessity of selecting appropriate methods based on instructional goals to transfer tacit knowledge effectively. Gaze-based instructional methods are expected to be versatile and applicable to karate kata and fields such as industry, medicine, and other sports.},
	urldate = {2025-03-04},
	publisher = {arXiv},
	author = {Nishijima, Shota and Takai, Asuka},
	month = dec,
	year = {2024},
	note = {arXiv:2412.17296 [q-bio]},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {2024_Nishijima and Takai_Comparison of Spatiotemporal Characteristics of Eye Movements in Non-experts and the Skill Transfer.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2024_Nishijima and Takai_Comparison of Spatiotemporal Characteristics of Eye Movements in Non-experts and the Skill Transfer.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\KFS23S48\\2412.html:text/html},
}

@article{adesina,
	title = {Tacit knowledge management strategies of small- and medium-sized enterprises: {An} overview},
	volume = {26},
	shorttitle = {Tacit knowledge management strategies of small- and medium-sized enterprises},
	url = {https://journals.co.za/doi/full/10.4102/sajim.v26i1.1711},
	doi = {10.4102/sajim.v26i1.1711},
	abstract = {BACKGROUND

The study argues that managing tacit knowledge (TKM) would reduce small-and medium-sized enterprises (SMEs) operational discontinuity and knowledge loss in KwaZulu-Natal (KZN) province, South Africa.

OBJECTIVES

The article examined the strategies put in place by SMEs for tacit knowledge management (TKM) practices and to develop a framework that will promote TKM for SMEs.

METHOD

The study adopted a quantitative research method and targeted 326 SMEs using Google Forms. One hundred and eighty (180; 55.2\%) useful responses were obtained and analysed using the Statistical Package for Social Sciences.

RESULTS

Most of the SME owners are aware and affirmed that there is a particular tacit knowledge that is of importance to business. The most common methods of capturing tacit knowledge among SMEs are monitoring, practical sessions, in-house training programmes, and brainstorming. Tacit knowledge is shared during meetings (such as project teams) and when dialoguing. The study also revealed that electronic files in computers are the major tools for storing the collected tacit knowledge.

CONCLUSION

The study concluded that TKM among SMEs in KZN required improvement and recommended improving teams and informal networks and making information and communication technology tools available to preserve tacit knowledge. The SMEs that can afford it can consider employing the services of consultant knowledge management officers to conduct periodic knowledge audits to identify knowledge gaps for proactive solutions.

CONTRIBUTION

The study contributed to knowledge management, tacit knowledge, explicit knowledge, and TKM.},
	number = {1},
	urldate = {2025-03-05},
	journal = {South African Journal of Information Management},
	author = {Adesina, Aderonke O. and Ocholla, Dennis N.},
	note = {Publisher: AOSIS},
	keywords = {knowledge management, SECI model, small- and medium-sized enterprises, South Africa, tacit knowledge},
	pages = {1711},
	file = {Adesina and Ocholla_Tacit knowledge management strategies of small- and medium-sized enterprises An overview.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\Adesina and Ocholla_Tacit knowledge management strategies of small- and medium-sized enterprises An overview.pdf:application/pdf},
}

@article{zaouiseghroucheni2025,
	title = {Using {AI} and {NLP} for {Tacit} {Knowledge} {Conversion} in {Knowledge} {Management} {Systems}: {A} {Comparative} {Analysis}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7080},
	shorttitle = {Using {AI} and {NLP} for {Tacit} {Knowledge} {Conversion} in {Knowledge} {Management} {Systems}},
	url = {https://www.mdpi.com/2227-7080/13/2/87},
	doi = {10.3390/technologies13020087},
	abstract = {Tacit knowledge, often implicit and deeply embedded within individuals and organizational practices, is critical for fostering innovation and decision-making in knowledge management systems (KMS). Converting tacit knowledge into explicit forms enhances organizational effectiveness by making this knowledge accessible and reusable. This paper presents a comparative analysis of natural language processing (NLP) algorithms used for document and report mining to facilitate tacit knowledge conversion. This study focuses on algorithms that extract insights from semi-structured and document-based natural language representations, commonly found in organizational knowledge artifacts. Key NLP strategies, including text mining, information extraction, sentiment analysis, clustering, classification, recommendation systems, and affective computing, are evaluated for their effectiveness in identifying and externalizing tacit knowledge. The findings highlight the relative strengths and limitations of these techniques, offering practical guidance for selecting suitable algorithms based on organizational needs. Additionally, this paper identifies challenges and emerging opportunities for advancing NLP-driven tacit knowledge conversion, providing actionable insights for researchers and practitioners aiming to enhance KMS capabilities.},
	language = {en},
	number = {2},
	urldate = {2025-03-05},
	journal = {Technologies},
	author = {Zaoui Seghroucheni, Ouissale and Lazaar, Mohamed and Al Achhab, Mohammed},
	month = feb,
	year = {2025},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {tacit knowledge, explicit knowledge, knowledge conversion, knowledge management systems, NLP},
	pages = {87},
	file = {2025_Zaoui Seghroucheni et al._Using AI and NLP for Tacit Knowledge Conversion in Knowledge Management Systems A Comparative Analy.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2025_Zaoui Seghroucheni et al._Using AI and NLP for Tacit Knowledge Conversion in Knowledge Management Systems A Comparative Analy.pdf:application/pdf},
}

@article{bhatti2024,
	title = {The impact of digital platforms on the creativity of remote workers through the mediating role of explicit and tacit knowledge sharing},
	volume = {28},
	issn = {1367-3270},
	url = {https://www.emerald.com/insight/content/doi/10.1108/jkm-08-2023-0682/full/html},
	doi = {10.1108/JKM-08-2023-0682},
	abstract = {Remote working has brought forward many challenges for employees as the phenomenon is still new for most employees across the globe. Some of these challenges may be addressed by the recent adoption of digital technologies by organizations. In this vein, our study explores the impact of digital platform capability on the creativity of employees through the mediating mechanism of explicit and tacit knowledge sharing.,The data were gathered from higher education institutes (HEIs) in a developing country, Pakistan which recently saw a major disruption during the Covid-19 pandemic. The proposed hypotheses were tested through Structural Equational Modeling (SEM) and the results confirmed our hypotheses.,The findings confirmed that the digital platform capabilities impact both tacit and explicit knowledge sharing among these remote employees. Likewise, the results also supported the mediating role of both explicit and tacit knowledge sharing on the creativity of these remote workers.,Our results are significant as they confirm the impact of digitalization on remote workers’ creativity predisposition. We thus advance the academic debate on the problems of knowledge sharing in remote working. We prove that digital capabilities outweigh the challenges created due to new forms of work driven by the pandemic. It further highlights the important areas to focus on while planning human resource policies in the new normal.},
	language = {en},
	number = {8},
	urldate = {2025-03-05},
	journal = {Journal of Knowledge Management},
	author = {Bhatti, Sabeen Hussain and Gavurova, Beata and Ahmed, Adeel and Marcone, Maria Rosaria and Santoro, Gabriele},
	month = may,
	year = {2024},
	note = {Publisher: Emerald Publishing Limited},
	pages = {2433--2459},
	file = {2024_Bhatti et al._The impact of digital platforms on the creativity of remote workers through the mediating role of ex.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2024_Bhatti et al._The impact of digital platforms on the creativity of remote workers through the mediating role of ex.pdf:application/pdf},
}

@article{zia2023,
	title = {Role of tacit knowledge management process and innovation capability for stimulating organizational performance: empirical analysis, {PLS}-{SEM} approach},
	volume = {53},
	issn = {0368-492X},
	shorttitle = {Role of tacit knowledge management process and innovation capability for stimulating organizational performance},
	url = {https://www.emerald.com/insight/content/doi/10.1108/k-03-2023-0444/full/html},
	doi = {10.1108/K-03-2023-0444},
	abstract = {This article aims to explore the connections between tacit knowledge management and the capacity to create new products and services for stimulating organizational performance.,This research utilizes a questionnaire-based study and 378 questionnaires gathered from different provinces of China between August and October 2022. The SmartPLS technique was used to evaluate the regression and mediation analysis on lower-order and higher-order components of the research hypotheses behind the model.,This investigation's results indicate that the tacit knowledge management process (TKMP) significantly drives product and service innovation and impacts organizational performance (ORP). According to the results, TKMP did not directly influence ORP and product innovation to mediate between Tacit knowledge and organizational performance.,Future research should concentrate on different combinations of influences on innovation and other consequences of introducing innovation into businesses. Moreover, researchers may add moderators to innovation and organizational performance.,This study assists managers in how tacit knowledge management affects organisational performance by examining product/service innovation capabilities. Product innovation also mediates between tacit knowledge and organizational performance. Service innovation improves organizational performance, prioritizing knowledge creation, sharing and retention to increase innovation and organizational success.,This study contributes to the literature on tacit knowledge management, innovation capability and organizational performance by concentrating on the tacit knowledge process and using the resource-based view. This study gives a solid theoretical and practical basis for understanding the component interactions.},
	language = {en},
	number = {11},
	urldate = {2025-03-05},
	journal = {Kybernetes},
	author = {Zia, Umair and Zhang, Jianhua and Alam, Sajjad},
	month = aug,
	year = {2023},
	note = {Publisher: Emerald Publishing Limited},
	pages = {4976--5000},
}

@article{sun,
	title = {On tacit knowledge management in product design: status, challenges, and trends},
	volume = {0},
	issn = {0954-4828},
	shorttitle = {On tacit knowledge management in product design},
	url = {https://doi.org/10.1080/09544828.2023.2301232},
	doi = {10.1080/09544828.2023.2301232},
	abstract = {Mass personalisation production is one of the strategic priorities for the next transformation of the production paradigm and market economy. With the purpose of offering personalised products to satisfy customers on an individual basis, tacit knowledge rooted in individuals has become increasingly accepted as an integral part of product design. This paper aims to explore the state-of-the-art in tacit knowledge management with a product design focus. Particularly, methods for tacit knowledge acquisition, transfer, and reuse are reviewed and analysed. Research on tacit knowledge acquisition is mainly dedicated to making tacit knowledge explicit. In knowledge transfer, both formal and informal approaches have been adopted to enable knowledge circulation. Research on tacit knowledge reuse is much less and scattered, and the main work focuses on user modelling and the reuse of empirical knowledge. Five challenges of tacit knowledge management are identified in this paper: lack of unified tacit knowledge definition, massive heterogeneous data, authenticity and completeness verification, uncertainty and gaps in bridging tacit knowledge management and personalised design, lack of practical knowledge sharing and inheritance tools. To fill these research gaps, five thematic future directions are suggested with possible visions to facilitate knowledge circulation, customer co-creation and innovation in mass personalised design.},
	number = {0},
	urldate = {2025-03-05},
	journal = {Journal of Engineering Design},
	author = {Sun, Xiaoguang and Huang, Rong and Jiang, Ziqi and Lu, Jin and Yang, Sheng},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/09544828.2023.2301232},
	keywords = {empirical knowledge, knowledge management, mass personalisation, product design, Tacit knowledge},
	pages = {1--38},
	file = {Sun et al._On tacit knowledge management in product design status, challenges, and trends.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\Sun et al._On tacit knowledge management in product design status, challenges, and trends.pdf:application/pdf},
}

@article{dhiman2024,
	title = {Does the {Medium} {Matter}? {A} {Comparison} of {Augmented} {Reality} {Media} in {Instructing} {Novices} to {Perform} {Complex}, {Skill}-{Based} {Manual} {Tasks}.},
	volume = {8},
	shorttitle = {Does the {Medium} {Matter}?},
	url = {https://dl.acm.org/doi/10.1145/3660249},
	doi = {10.1145/3660249},
	abstract = {Past research comparing augmented reality (AR) media such as in-situ projection and head-mounted devices (HMD) has usually considered simple manual activities. It is unknown whether previously reported differences between different AR media also apply to complex, skill-driven tasks. In this paper, we explore the feasibility and challenges in designing AR instructions for expertise-driven, skilled activities. We present findings from a real-world, between-subjects experiment in which novices were instructed to trim and bone sub-primal cuts of pork using two interactive AR prototypes, one utilizing in-situ projection and a second using the Hololens 2. The prototypes and instructions were designed in consultation with experts. We compared novices' task performance and subjective perceptions and gathered experts' feedback. Although both users and experts indicated a subjective preference for in-situ projection, results indicate that when tasks require knowledge, skill and expertise, the choice of the AR medium itself may not be consequential. Rather, in our experiment, the instruction quality influenced comprehension, knowledge retention and task performance. Hence, from an engineering perspective, emphasis ought to be laid on gathering and structuring expert performance and knowledge to create effective instructions, which could be delivered using any AR medium suited to the task and work environment.},
	number = {EICS},
	urldate = {2025-03-05},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Dhiman, Hitesh and Röcker, Carsten},
	month = jun,
	year = {2024},
	pages = {249:1--249:28},
	file = {2024_Dhiman and Röcker_Does the Medium Matter A Comparison of Augmented Reality Media in Instructing Novices to Perform Co.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2024_Dhiman and Röcker_Does the Medium Matter A Comparison of Augmented Reality Media in Instructing Novices to Perform Co.pdf:application/pdf},
}

@inproceedings{naqvi2022,
	address = {Cham},
	title = {Leveraging {Free}-{Form} {Text} in {Maintenance} {Logs} {Through} {BERT} {Transfer} {Learning}},
	isbn = {978-3-030-98531-8},
	doi = {10.1007/978-3-030-98531-8_7},
	abstract = {Across various industries, maintenance entries recorded over time contain decades of experience and health records of different assets. Maintenance logs are usually free-form text entries recorded by maintenance operators. These records are also highly unstructured and imbalanced. Because of these reasons, this huge source of knowledge is usually underutilized and does not contribute to the development of tools to help improve maintenance processes. In the last few years, due to recent advancements in the field of Natural Language Processing (NLP) and increased focus on industry 4.0, there is a need to revisit this problem. This study explores the use of state-of-the-art NLP methods on free-form maintenance text to leverage this data. More specifically, the purpose of the study is to estimate the problem category of maintenance log to see how well recent NLP models adapt to free-form maintenance text. Findings of this study indicate that CamemBERT with the fine-tuning approach outperforms the classical NLP approaches. Class imbalance problem is also addressed through data augmentation using deep contextualized embedding showing further performance improvement. Model accuracy and Matthews correlation coefficient (MCC) are used as performance metrics to give a better understanding of results with imbalanced classes.},
	language = {en},
	booktitle = {Progresses in {Artificial} {Intelligence} \& {Robotics}: {Algorithms} \& {Applications}},
	publisher = {Springer International Publishing},
	author = {Naqvi, Syed Meesam Raza and Varnier, Christophe and Nicod, Jean-Marc and Zerhouni, Noureddine and Ghufran, Mohammad},
	editor = {Troiano, Luigi and Vaccaro, Alfredo and Kesswani, Nishtha and Díaz Rodriguez, Irene and Brigui, Imene},
	year = {2022},
	pages = {63--75},
	file = {2022_Naqvi et al._Leveraging Free-Form Text in Maintenance Logs Through BERT Transfer Learning.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2022_Naqvi et al._Leveraging Free-Form Text in Maintenance Logs Through BERT Transfer Learning.pdf:application/pdf},
}

@inproceedings{freire2023,
	address = {New York, NY, USA},
	series = {{CUI} '23},
	title = {Harnessing {Large} {Language} {Models} for {Cognitive} {Assistants} in {Factories}},
	isbn = {979-8-4007-0014-9},
	url = {https://dl.acm.org/doi/10.1145/3571884.3604313},
	doi = {10.1145/3571884.3604313},
	abstract = {As agile manufacturing expands and workforce mobility increases, the importance of efficient knowledge transfer among factory workers grows. Cognitive Assistants (CAs) with Large Language Models (LLMs), like GPT-3.5, can bridge knowledge gaps and improve worker performance in manufacturing settings. This study investigates the opportunities, risks, and user acceptance of LLM-powered CAs in two factory contexts: textile and detergent production. Several opportunities and risks are identified through a literature review, proof-of-concept implementation, and focus group sessions. Factory representatives raise concerns regarding data security, privacy, and the reliability of LLMs in high-stake environments. By following design guidelines regarding persistent memory, real-time data integration, security, privacy, and ethical concerns, LLM-powered CAs can become valuable assets in manufacturing settings and other industries.},
	urldate = {2025-03-05},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Conversational} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Freire, Samuel Kernan and Foosherian, Mina and Wang, Chaofan and Niforatos, Evangelos},
	month = jul,
	year = {2023},
	pages = {1--6},
	file = {2023_Kernan Freire et al._Harnessing Large Language Models for Cognitive Assistants in Factories.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2023_Kernan Freire et al._Harnessing Large Language Models for Cognitive Assistants in Factories.pdf:application/pdf},
}

@misc{freire2024,
	title = {Factory {Operators}' {Perspectives} on {Cognitive} {Assistants} for {Knowledge} {Sharing}: {Challenges}, {Risks}, and {Impact} on {Work}},
	shorttitle = {Factory {Operators}' {Perspectives} on {Cognitive} {Assistants} for {Knowledge} {Sharing}},
	url = {http://arxiv.org/abs/2409.20192},
	doi = {10.48550/arXiv.2409.20192},
	abstract = {In the shift towards human-centered manufacturing, our two-year longitudinal study investigates the real-world impact of deploying Cognitive Assistants (CAs) in factories. The CAs were designed to facilitate knowledge sharing among factory operators. Our investigation focused on smartphone-based voice assistants and LLM-powered chatbots, examining their usability and utility in a real-world factory setting. Based on the qualitative feedback we collected during the deployments of CAs at the factories, we conducted a thematic analysis to investigate the perceptions, challenges, and overall impact on workflow and knowledge sharing. Our results indicate that while CAs have the potential to significantly improve efficiency through knowledge sharing and quicker resolution of production issues, they also introduce concerns around workplace surveillance, the types of knowledge that can be shared, and shortcomings compared to human-to-human knowledge sharing. Additionally, our findings stress the importance of addressing privacy, knowledge contribution burdens, and tensions between factory operators and their managers.},
	urldate = {2025-03-05},
	publisher = {arXiv},
	author = {Freire, Samuel Kernan and He, Tianhao and Wang, Chaofan and Niforatos, Evangelos and Bozzon, Alessandro},
	month = sep,
	year = {2024},
	note = {arXiv:2409.20192 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {2024_Freire et al._Factory Operators' Perspectives on Cognitive Assistants for Knowledge Sharing Challenges, Risks, an.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2024_Freire et al._Factory Operators' Perspectives on Cognitive Assistants for Knowledge Sharing Challenges, Risks, an.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\SC75DQ88\\2409.html:text/html},
}

@article{freire2025,
	title = {{LLM}-{Powered} {Cognitive} {Assistants} for {Knowledge} {Sharing} among {Factory} {Operators}},
	url = {https://repository.tudelft.nl/record/uuid:733e10e7-890c-48a7-86d3-fa782ffd65c8},
	abstract = {Modern factories showlittle resemblance to the assembly lines froma century ago. Nowadays, a single human might be responsible for setting up, operating, and maintaining a complex chain of machines. This shift has made the work of factory operators more cognitively demanding, requiring constant monitoring and problem-solving. Key to tackling these challenges is the effective collaboration of human operators in exchanging ideas and knowledge, from high-level problem-solving strategies to solutions for emerging issues. Yet, the heightened productivity requirements and small shifts mean fewer opportunities to share this knowledge face-to-face and reporting practices are deprioritized. Thus, the manufacturing industry faces a knowledge management crisis.{\textless}br/{\textgreater}This dissertation investigates the integration of conversational AI assistants into manufacturing settings to facilitate knowledge sharing among factory operators. Capitalizing on recent advancements in Natural Language Processing (NLP), particularly in Large Language Models (LLMs), this research investigates the designing and evaluation of conversational AI tools that efficiently capture and share human knowledge on the factory floor while addressing operator needs and concerns. The introduction of conversational AI assistants for knowledge sharing—referred to as cognitive assistants (CA) in this work—in factory environments promises significant benefits but comes with numerous challenges....},
	language = {en},
	urldate = {2025-03-05},
	author = {Freire, Samuel Kernan},
	year = {2025},
	file = {2025_Kernan Freire_LLM-Powered Cognitive Assistants for Knowledge Sharing among Factory Operators.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2025_Kernan Freire_LLM-Powered Cognitive Assistants for Knowledge Sharing among Factory Operators.pdf:application/pdf},
}

@article{freire2024a,
	title = {Knowledge sharing in manufacturing using {LLM}-powered tools: user study and model benchmarking},
	volume = {7},
	issn = {2624-8212},
	shorttitle = {Knowledge sharing in manufacturing using {LLM}-powered tools},
	doi = {10.3389/frai.2024.1293084},
	abstract = {Recent advances in natural language processing enable more intelligent ways to support knowledge sharing in factories. In manufacturing, operating production lines has become increasingly knowledge-intensive, putting strain on a factory's capacity to train and support new operators. This paper introduces a Large Language Model (LLM)-based system designed to retrieve information from the extensive knowledge contained in factory documentation and knowledge shared by expert operators. The system aims to efficiently answer queries from operators and facilitate the sharing of new knowledge. We conducted a user study at a factory to assess its potential impact and adoption, eliciting several perceived benefits, namely, enabling quicker information retrieval and more efficient resolution of issues. However, the study also highlighted a preference for learning from a human expert when such an option is available. Furthermore, we benchmarked several commercial and open-sourced LLMs for this system. The current state-of-the-art model, GPT-4, consistently outperformed its counterparts, with open-source models trailing closely, presenting an attractive option given their data privacy and customization benefits. In summary, this work offers preliminary insights and a system design for factories considering using LLM tools for knowledge management.},
	language = {eng},
	journal = {Frontiers in Artificial Intelligence},
	author = {Freire, Samuel Kernan and Wang, Chaofan and Foosherian, Mina and Wellsandt, Stefan and Ruiz-Arenas, Santiago and Niforatos, Evangelos},
	year = {2024},
	pmid = {38601111},
	pmcid = {PMC11004332},
	keywords = {benchmarking, factory, industrial settings, industry 5.0, information retrieval, knowledge sharing, Large Language Models, natural language interface},
	pages = {1293084},
	file = {2024_Kernan Freire et al._Knowledge sharing in manufacturing using LLM-powered tools user study and model benchmarking.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2024_Kernan Freire et al._Knowledge sharing in manufacturing using LLM-powered tools user study and model benchmarking.pdf:application/pdf},
}

@article{ordieres-mere2023,
	title = {Toward the industry 5.0 paradigm: {Increasing} value creation through the robust integration of humans and machines},
	volume = {150},
	issn = {0166-3615},
	shorttitle = {Toward the industry 5.0 paradigm},
	url = {https://www.sciencedirect.com/science/article/pii/S0166361523000970},
	doi = {10.1016/j.compind.2023.103947},
	abstract = {This study proposes a flexible architecture under the LAsim Smart FActor Plus reference framework, fostering the integration of different related data sources—processes, products and the human dimension (operators or other agents)—to increase business value creation. The proposed architecture promotes distributed component perspectives at different levels using different types of digital assets. Integrated reusable services to build composed business applications are found to help increase business understanding and transparency. The robustness attribute is related to the concept of persistence and seeks to reduce the degree of intervention required and thus enable integration. Use cases are presented to demonstrate the advantages provided by the proposed architecture.},
	urldate = {2025-03-07},
	journal = {Computers in Industry},
	author = {Ordieres-Meré, Joaquín and Gutierrez, Miguel and Villalba-Díez, Javier},
	month = sep,
	year = {2023},
	keywords = {Industry 5.0, Data integration, Flexible architecture, Integrated reusable services, Interoperability},
	pages = {103947},
	file = {2023_Ordieres-Meré et al._Toward the industry 5.0 paradigm Increasing value creation through the robust integration of humans.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2023_Ordieres-Meré et al._Toward the industry 5.0 paradigm Increasing value creation through the robust integration of humans.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\W7C6AZNL\\S0166361523000970.html:text/html},
}

@article{damgrave2023,
	series = {56th {CIRP} {International} {Conference} on {Manufacturing} {Systems} 2023},
	title = {Augmented reality support to employ tacit knowledge in non-conforming operations},
	volume = {120},
	issn = {2212-8271},
	url = {https://www.sciencedirect.com/science/article/pii/S2212827123009289},
	doi = {10.1016/j.procir.2023.09.196},
	abstract = {Organisations often rely, implicitly, on the tacit knowledge of their employees, especially in non-conforming operations. These activities include (dis)assembly tasks, manual machining, troubleshooting, and tailored adjustments. While a traditional master-apprentice approach is a proven way to transfer tacit knowledge between employees, this approach is not feasible in current industrial situations. This research proposes a contemporary master-apprentice approach to support and improve non-conforming operations in situations that cannot rely on all parties being present at the same time, in the same location, while co-operating on the same task. The approach demonstrates the use of augmented reality as an implicit and discreet tool that purposefully addresses both capturing tacit knowledge, as well as employing it during operations - without disruptive interference in the primary processes.},
	urldate = {2025-03-07},
	journal = {Procedia CIRP},
	author = {Damgrave, Roy and Scheffer, Sara and Lutters, Eric},
	month = jan,
	year = {2023},
	keywords = {Assembly, Augmented reality, Digital twin, Knowledge capturing, Non-conforming operations},
	pages = {1475--1480},
	file = {2023_Damgrave et al._Augmented reality support to employ tacit knowledge in non-conforming operations.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2023_Damgrave et al._Augmented reality support to employ tacit knowledge in non-conforming operations.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\49GFYAIR\\S2212827123009289.html:text/html},
}

@mastersthesis{jonassen2024,
	title = {Value of {Computerized} maintenance management system ({CMMS}) in smart {Maintenance} and {Asset} management decisions – cases and best practices},
	url = {https://uis.brage.unit.no/uis-xmlui/handle/11250/3152197},
	abstract = {Today, Computerized Maintenance Management Systems (CMMS) are widely used across various industries, particularly those with long-lived physical assets like the energy and transportation sectors. However, recent advancements in smart maintenance, especially within technology, have opened new opportunities. With tools such as artificial intelligence and machine learning, CMMS can extract higher value from assets. 

Maximizing the value derived from the CMMS is a multifaceted and challenging task. This challenge comes from among other things the diverse requirements and objectives across organizations and industries, necessitating a flexible, capable, and user-friendly software solution that serves as the centralized database for all maintenance activities. The thesis aims to answer the following research questions:

  1. How can an asset owner or operator achieve full benefits of CMMS within recent industrial
      trends, such as industry 4.0/5.0, IoT, data-driven technologies (AI/ML), etc?

  2. How can an asset owner or operator implement CMMS to fulfill expectations from stake-
      holders both internally in the enterprise and externally?

  3. How can an asset owner or operator ensure data integrity, and what strategies can be used
      to accurately collect data?

To address these questions, a mixed-method approach will be used, including a literature review and qualitative data obtained through interviews with relevant personnel who engage with CMMS on a daily basis. This data forms the foundation for subsequent analysis and answering of the research questions. 

The interviews highlighted several key findings, including the widespread use of outdated CMMS software among many organizations. Additionally, there was limited focus on upgrading to newer versions with enhanced features like built-in visualization tools and artificial intelligence capabilities. Moreover, personnel training emerged as a crucial factor in maximizing the potential of CMMS, as the software was considered as challenging to use without extensive knowledge and experience. Similarly, data integrity was identified as a significant challenge, particularly regarding the quality of maintenance completion reports, which could also be attributed to personnel training and other factors. Improvements in these areas could significantly enhance
the value generated by the CMMS.},
	language = {eng},
	urldate = {2025-03-07},
	school = {UIS},
	author = {Jonassen, Gunnar Sveinsvoll},
	year = {2024},
	note = {Accepted: 2024-09-13T15:51:41Z},
	file = {2024_Jonassen_Value of Computerized maintenance management system (CMMS) in smart Maintenance and Asset management.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2024_Jonassen_Value of Computerized maintenance management system (CMMS) in smart Maintenance and Asset management.pdf:application/pdf},
}

@article{wienker2016,
	series = {{SYMPHOS} 2015 - 3rd {International} {Symposium} on {Innovation} and {Technology} in the {Phosphate} {Industry}},
	title = {The {Computerized} {Maintenance} {Management} {System} an {Essential} {Tool} for {World} {Class} {Maintenance}},
	volume = {138},
	issn = {1877-7058},
	url = {https://www.sciencedirect.com/science/article/pii/S1877705816004641},
	doi = {10.1016/j.proeng.2016.02.100},
	abstract = {The management of maintenance in a large industrial operation is complex and has a significant impact on the profitability of the business. Managing this process effectively without computer-based support is almost impossible, but achieving successful implementation of these systems requires a major change-management program over many years. It is not surprising then that there is a low success rate among even large organizations worldwide in implementing an effective Computerized Maintenance Management System (CMMS) to support improved reliability and performance. This paper focuses on understanding the reasons behind the low success rate achieved and outlines the essential elements that must be included to ensure a disciplined and well-resourced program that can deliver success. Emphasis is put on the need to gain and retain the support of top management to overcome the barriers to change by convincing them that such support makes good business sense.},
	urldate = {2025-03-07},
	journal = {Procedia Engineering},
	author = {Wienker, Michael and Henderson, Ken and Volkerts, Jacques},
	month = jan,
	year = {2016},
	keywords = {Change Management, CMMS, eAM, Implementation, Management of Maintenace, Reasons for low success rate},
	pages = {413--420},
	file = {2016_Wienker et al._The Computerized Maintenance Management System an Essential Tool for World Class Maintenance.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2016_Wienker et al._The Computerized Maintenance Management System an Essential Tool for World Class Maintenance.pdf:application/pdf},
}

@article{dinis2025,
	series = {6th {International} {Conference} on {Industry} 4.0 and {Smart} {Manufacturing}},
	title = {Maintenance {Management}: {A} {Review} on {Problems} and {Solutions}},
	volume = {253},
	issn = {1877-0509},
	shorttitle = {Maintenance {Management}},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050925003746},
	doi = {10.1016/j.procs.2025.02.031},
	abstract = {Maintenance management is defined as “all management activities that determine the maintenance requirements, objectives, strategies and responsibilities, and implementation of them by such means as maintenance planning, maintenance control, and improvement of maintenance activities and economics”. The purpose of this research work is to present a brief review on key problems in maintenance management and on selected solutions to address these problems, particularly within aviation maintenance, framed within business analytics (BA), which comprises descriptive, predictive, and prescriptive perspectives. Unlike manufacturing, in which most activities are deterministic by nature, characterized by well-defined execution lead times and required resources, being it labor, tools, or parts and materials, maintenance presents an important stochastic component. Maintenance can be divided into preventive maintenance and corrective maintenance. The former refers to prespecified tasks, carried out at predetermined intervals, being its work essentially deterministic. The latter results from the probabilistic nature of failures, performed upon a fault is identified, being its work inherently stochastic. The workload resulting from corrective maintenance can be more than half of the total maintenance workload according to some studies, making it an important challenge for effective and efficient maintenance management. This research work is expected to be of interest for researchers and practitioners alike, by identifying the sources of uncertainty associated with maintenance management and by providing references on relevant methods to model and control such uncertainty.},
	urldate = {2025-03-07},
	journal = {Procedia Computer Science},
	author = {Dinis, Duarte},
	month = jan,
	year = {2025},
	keywords = {Business analytics, Capacity planning, Maintenance management, Scheduling, Spare parts management},
	pages = {3069--3077},
	file = {2025_Dinis_Maintenance Management A Review on Problems and Solutions.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2025_Dinis_Maintenance Management A Review on Problems and Solutions.pdf:application/pdf},
}

@article{achouch2022,
	title = {On {Predictive} {Maintenance} in {Industry} 4.0: {Overview}, {Models}, and {Challenges}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	shorttitle = {On {Predictive} {Maintenance} in {Industry} 4.0},
	url = {https://www.mdpi.com/2076-3417/12/16/8081},
	doi = {10.3390/app12168081},
	abstract = {In the era of the fourth industrial revolution, several concepts have arisen in parallel with this new revolution, such as predictive maintenance, which today plays a key role in sustainable manufacturing and production systems by introducing a digital version of machine maintenance. The data extracted from production processes have increased exponentially due to the proliferation of sensing technologies. Even if Maintenance 4.0 faces organizational, financial, or even data source and machine repair challenges, it remains a strong point for the companies that use it. Indeed, it allows for minimizing machine downtime and associated costs, maximizing the life cycle of the machine, and improving the quality and cadence of production. This approach is generally characterized by a very precise workflow, starting with project understanding and data collection and ending with the decision-making phase. This paper presents an exhaustive literature review of methods and applied tools for intelligent predictive maintenance models in Industry 4.0 by identifying and categorizing the life cycle of maintenance projects and the challenges encountered, and presents the models associated with this type of maintenance: condition-based maintenance (CBM), prognostics and health management (PHM), and remaining useful life (RUL). Finally, a novel applied industrial workflow of predictive maintenance is presented including the decision support phase wherein a recommendation for a predictive maintenance platform is presented. This platform ensures the management and fluid data communication between equipment throughout their life cycle in the context of smart maintenance.},
	language = {en},
	number = {16},
	urldate = {2025-03-07},
	journal = {Applied Sciences},
	author = {Achouch, Mounia and Dimitrova, Mariya and Ziane, Khaled and Sattarpanah Karganroudi, Sasan and Dhouib, Rizck and Ibrahim, Hussein and Adda, Mehdi},
	month = jan,
	year = {2022},
	note = {Number: 16
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial intelligence, condition-based maintenance (CBM), decision making, Industry 4.0, predictive maintenance (PdM) and challenges, predictive maintenance workflow, prognostics and health management (PHM), remaining useful life (RUL)},
	pages = {8081},
	file = {2022_Achouch et al._On Predictive Maintenance in Industry 4.0 Overview, Models, and Challenges.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\2022_Achouch et al._On Predictive Maintenance in Industry 4.0 Overview, Models, and Challenges.pdf:application/pdf},
}

@article{lu2022,
	title = {Outlook on human-centric manufacturing towards {Industry} 5.0},
	volume = {62},
	issn = {0278-6125},
	url = {https://www.sciencedirect.com/science/article/pii/S0278612522000164},
	doi = {10.1016/j.jmsy.2022.02.001},
	abstract = {The recent shift to wellbeing, sustainability, and resilience under Industry 5.0 has prompted formal discussions that manufacturing should be human-centric – placing the wellbeing of industry workers at the center of manufacturing processes, instead of system-centric – only driven by efficiency and quality improvement and cost reduction. However, there is a lack of shared understanding of the essence of human-centric manufacturing, though significant research efforts exist in enhancing the physical and cognitive wellbeing of operators. Therefore, this position paper presents our arguments on the concept, needs, reference model, enabling technologies and system frameworks of human-centric manufacturing, providing a relatable vision and research agenda for future work in human-centric manufacturing systems. We believe human-centric manufacturing should ultimately address human needs defined in an Industrial Human Needs Pyramid – from basic needs of safety and health to the highest level of esteem and self-actualization. In parallel, human-machine relationships will change following a 5C evolution map – from current Coexistence, Cooperation and Collaboration to future Compassion and Coevolution. As such, human-centric manufacturing systems need to have bi-directional empathy, proactive communication and collaborative intelligence for establishing trustworthy human-machine coevolution relationships, thereby leading to high-performance human-machine teams. It is suggested that future research focus should be on developing transparent, trustworthy and quantifiable technologies that provide a rewarding working environment driven by real-world needs.},
	urldate = {2025-03-10},
	journal = {Journal of Manufacturing Systems},
	author = {Lu, Yuqian and Zheng, Hao and Chand, Saahil and Xia, Wanqing and Liu, Zengkun and Xu, Xun and Wang, Lihui and Qin, Zhaojun and Bao, Jinsong},
	month = jan,
	year = {2022},
	keywords = {Industry 5.0, Human-centric human-robot collaboration, Human-centric manufacturing, Human-machine relationship, Industrial Human Needs Pyramid, Self-organizing manufacturing},
	pages = {612--627},
	file = {2022_Lu et al._Outlook on human-centric manufacturing towards Industry 5.0.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\STA\\2022_Lu et al._Outlook on human-centric manufacturing towards Industry 5.0.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\CN4I3JWN\\S0278612522000164.html:text/html},
}

@book{americanpsychiatricassociation2022,
	edition = {DSM-5-TR},
	title = {Diagnostic and {Statistical} {Manual} of {Mental} {Disorders}},
	isbn = {978-0-89042-575-6},
	url = {https://psychiatryonline.org/doi/book/10.1176/appi.books.9780890425787},
	language = {en},
	urldate = {2025-03-11},
	publisher = {American Psychiatric Association Publishing},
	author = {{American Psychiatric Association}},
	month = mar,
	year = {2022},
	doi = {10.1176/appi.books.9780890425787},
}

@article{lord2022,
	title = {The {Lancet} {Commission} on the future of care and clinical research in autism},
	volume = {399},
	issn = {1474-547X},
	doi = {10.1016/S0140-6736(21)01541-5},
	language = {eng},
	number = {10321},
	journal = {Lancet (London, England)},
	author = {Lord, Catherine and Charman, Tony and Havdahl, Alexandra and Carbone, Paul and Anagnostou, Evdokia and Boyd, Brian and Carr, Themba and de Vries, Petrus J. and Dissanayake, Cheryl and Divan, Gauri and Freitag, Christine M. and Gotelli, Marina M. and Kasari, Connie and Knapp, Martin and Mundy, Peter and Plank, Alex and Scahill, Lawrence and Servili, Chiara and Shattuck, Paul and Simonoff, Emily and Singer, Alison Tepper and Slonims, Vicky and Wang, Paul P. and Ysrraelit, Maria Celica and Jellett, Rachel and Pickles, Andrew and Cusack, James and Howlin, Patricia and Szatmari, Peter and Holbrook, Alison and Toolan, Christina and McCauley, James B.},
	month = jan,
	year = {2022},
	pmid = {34883054},
	keywords = {Autistic Disorder, Humans, Biomedical Research, Global Burden of Disease, Global Health, Health Services Needs and Demand},
	pages = {271--334},
	file = {2022_Lord et al._The Lancet Commission on the future of care and clinical research in autism.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2022_Lord et al._The Lancet Commission on the future of care and clinical research in autism.pdf:application/pdf},
}

@article{dawson2008,
	title = {Early behavioral intervention, brain plasticity, and the prevention of autism spectrum disorder},
	volume = {20},
	issn = {1469-2198},
	doi = {10.1017/S0954579408000370},
	abstract = {Advances in the fields of cognitive and affective developmental neuroscience, developmental psychopathology, neurobiology, genetics, and applied behavior analysis have contributed to a more optimistic outcome for individuals with autism spectrum disorder (ASD). These advances have led to new methods for early detection and more effective treatments. For the first time, prevention of ASD is plausible. Prevention will entail detecting infants at risk before the full syndrome is present and implementing treatments designed to alter the course of early behavioral and brain development. This article describes a developmental model of risk, risk processes, symptom emergence, and adaptation in ASD that offers a framework for understanding early brain plasticity in ASD and its role in prevention of the disorder.},
	language = {eng},
	number = {3},
	journal = {Development and Psychopathology},
	author = {Dawson, Geraldine},
	year = {2008},
	pmid = {18606031},
	keywords = {Autistic Disorder, Child, Humans, Child, Preschool, Risk Factors, Brain, Animals, Disease Models, Animal, Social Environment, Early Diagnosis, Infant, Newborn, Arousal, Behavior Therapy, Early Intervention, Educational, Evoked Potentials, Genetic Predisposition to Disease, Infant, Motivation, Neuronal Plasticity},
	pages = {775--803},
}

@article{antshel2019,
	title = {Autism {Spectrum} {Disorders} and {ADHD}: {Overlapping} {Phenomenology}, {Diagnostic} {Issues}, and {Treatment} {Considerations}},
	volume = {21},
	issn = {1535-1645},
	shorttitle = {Autism {Spectrum} {Disorders} and {ADHD}},
	url = {https://doi.org/10.1007/s11920-019-1020-5},
	doi = {10.1007/s11920-019-1020-5},
	abstract = {Autism spectrum disorder (ASD) and attention deficit/hyperactivity disorder (ADHD) are both increasing in prevalence and commonly co-occur with each other. The goal of this review is to outline what has been published recently on the topics of ASD, ADHD, and the comorbid state (ASD+ADHD) with a particular focus on shared phenomenology, differential diagnosis, and treatment considerations.},
	language = {en},
	number = {5},
	urldate = {2025-03-11},
	journal = {Current Psychiatry Reports},
	author = {Antshel, Kevin M. and Russo, Natalie},
	month = mar,
	year = {2019},
	keywords = {Autism, ADHD, Autism spectrum disorder, Diagnosis, Neurodevelopmental Disorders, Comorbidity, DSM-5, Neurodevelopmental disorder},
	pages = {34},
	file = {2019_Antshel and Russo_Autism Spectrum Disorders and ADHD Overlapping Phenomenology, Diagnostic Issues, and Treatment Cons.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2019_Antshel and Russo_Autism Spectrum Disorders and ADHD Overlapping Phenomenology, Diagnostic Issues, and Treatment Cons.pdf:application/pdf},
}

@article{vandermeer2012,
	title = {Are {Autism} {Spectrum} {Disorder} and {Attention}-{Deficit}/{Hyperactivity} {Disorder} {Different} {Manifestations} of {One} {Overarching} {Disorder}? {Cognitive} and {Symptom} {Evidence} {From} a {Clinical} and {Population}-{Based} {Sample}},
	volume = {51},
	issn = {0890-8567},
	shorttitle = {Are {Autism} {Spectrum} {Disorder} and {Attention}-{Deficit}/{Hyperactivity} {Disorder} {Different} {Manifestations} of {One} {Overarching} {Disorder}?},
	url = {https://www.sciencedirect.com/science/article/pii/S0890856712006491},
	doi = {10.1016/j.jaac.2012.08.024},
	abstract = {Objective
Autism spectrum disorders (ASD) and attention-deficit/hyperactivity disorder (ADHD) frequently co-occur. Given the heterogeneity of both disorders, several more homogeneous ASD–ADHD comorbidity subgroups may exist. The current study examined whether such subgroups exist, and whether their overlap or distinctiveness in associated comorbid symptoms and cognitive profiles gives support for a gradient overarching disorder hypothesis or a separate disorders hypothesis.
Method
Latent class analysis was performed on Social Communication Questionnaire (SCQ) and Conners' Parent Rating Scale (CPRS-R:L) data for 644 children and adolescents (5 through 17 years of age). Classes were compared for comorbid symptoms and cognitive profiles of motor speed and variability, executive functioning, attention, emotion recognition, and detail-focused processing style.
Results
Latent class analysis revealed five classes: two without behavioral problems, one with only ADHD behavior, and two with both clinical symptom levels of ASD and ADHD but with one domain more prominent than the other (ADHD[+ASD] and ASD[+ADHD]). In accordance with the gradient overarching disorder hypothesis were the presence of an ADHD class without ASD symptoms and the absence of an ASD class without ADHD symptoms, as well as cognitive functioning of the simple ADHD class being less impaired than that of both comorbid classes. In conflict with this hypothesis was that there was some specificity of cognitive deficits across classes.
Conclusions
The overlapping cognitive deficits may be used to further unravel the shared etiological underpinnings of ASD and ADHD, and the nonoverlapping deficits may indicate why some children develop ADHD despite their enhanced risk for ASD. The two subtypes of children with both ASD and ADHD behavior will most likely benefit from different clinical approaches.},
	number = {11},
	urldate = {2025-03-11},
	journal = {Journal of the American Academy of Child \& Adolescent Psychiatry},
	author = {van der Meer, Jolanda M. J. and Oerlemans, Anoek M. and van Steijn, Daphne J. and Lappenschaar, Martijn G. A. and de Sonneville, Leo M. J. and Buitelaar, Jan K. and Rommelse, Nanda N. J.},
	month = nov,
	year = {2012},
	keywords = {cognition, attention-deficit/hyperactivity disorder (ADHD), autism spectrum disorder (ASD), heterogeneity, latent class analysis (LCA)},
	pages = {1160--1172.e3},
	file = {ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\EQ94Y6IZ\\S0890856712006491.html:text/html},
}

@misc{zotero-5946,
	title = {Frontiers {\textbar} {Characterization} of {Clinical} {Manifestations} in the {Co}-occurring {Phenotype} of {Attention} {Deficit}/{Hyperactivity} {Disorder} and {Autism} {Spectrum} {Disorder}},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2020.00861/full},
	urldate = {2025-03-11},
	file = {Frontiers | Characterization of Clinical Manifestations in the Co-occurring Phenotype of Attention Deficit/Hyperactivity Disorder and Autism Spectrum Disorder:C\:\\Users\\antoine.widmer\\Zotero\\storage\\DS5YMTMJ\\full.html:text/html},
}

@article{salazar2015,
	title = {Co-occurring {Psychiatric} {Disorders} in {Preschool} and {Elementary} {School}-{Aged} {Children} with {Autism} {Spectrum} {Disorder}},
	volume = {45},
	issn = {1573-3432},
	url = {https://doi.org/10.1007/s10803-015-2361-5},
	doi = {10.1007/s10803-015-2361-5},
	abstract = {We employed a clinical sample of young children with ASD, with and without intellectual disability, to determine the rate and type of psychiatric disorders and possible association with risk factors. We assessed 101 children (57 males, 44 females) aged 4.5–9.8 years. 90.5 \% of the sample met the criteria. Most common diagnoses were: generalized anxiety disorder (66.5 \%), specific phobias (52.7 \%) and attention deficit hyperactivity disorder (59.1 \%). Boys were more likely to have oppositional defiant disorder (OR 3.9). Higher IQ was associated with anxiety disorders (OR 2.9) and older age with agoraphobia (OR 5.8). Night terrors was associated with parental psychological distress (OR 14.2). Most young ASD children met the criteria for additional psychopathology.},
	language = {en},
	number = {8},
	urldate = {2025-03-11},
	journal = {Journal of Autism and Developmental Disorders},
	author = {Salazar, Fernando and Baird, Gillian and Chandler, Susie and Tseng, Evelin and O’sullivan, Tony and Howlin, Patricia and Pickles, Andrew and Simonoff, Emily},
	month = aug,
	year = {2015},
	keywords = {Autism, Autism spectrum disorder, Prevalence, Neurodevelopmental Disorders, Psychopathology, Child behavior problems},
	pages = {2283--2294},
	file = {2015_Salazar et al._Co-occurring Psychiatric Disorders in Preschool and Elementary School-Aged Children with Autism Spec.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2015_Salazar et al._Co-occurring Psychiatric Disorders in Preschool and Elementary School-Aged Children with Autism Spec.pdf:application/pdf},
}

@article{carta2020,
	title = {Characterization of {Clinical} {Manifestations} in the {Co}-occurring {Phenotype} of {Attention} {Deficit}/{Hyperactivity} {Disorder} and {Autism} {Spectrum} {Disorder}},
	volume = {11},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2020.00861/full},
	doi = {10.3389/fpsyg.2020.00861},
	abstract = {{\textless}p{\textgreater}Comorbidity between attention deficit/hyperactivity disorder (ADHD) and autism spectrum disorder (ASD) is a frequently reported condition. However, the clinical overlaps between the two disorders are not well characterized. The Child Behavior Checklist (CBCL) is a well-documented measure of emotional and behavioral problems in children and adolescents. The aim of the present study was to evaluate whether CBCL scales were able to detect psychopathological comorbidities as well as emotional and behavioral profiles across three groups of children with ASD, ADHD, and with the co-occurrence of both disorders. The results show that around 30\% of participants with ASD exhibited internalizing problems, which was in line with previous findings. Co-occurrence condition showed a clinical intermediate phenotype: relative to ADHD and ASD, youths with co-occurrence of ADHD and ASD phenotype showed respectively lower ({\textless}italic{\textgreater}p{\textless}/italic{\textgreater} \&lt; 0.000) and higher externalizing problems ({\textless}italic{\textgreater}p{\textless}/italic{\textgreater} \&lt; 0.000). No differences emerged in internalizing problems ({\textless}italic{\textgreater}p{\textless}/italic{\textgreater} \&gt; 0.05) across groups. CBCL is a useful measure to study the psychopathological conditions as well as emotional and behavioral profiles associated with ASD, ADHD, and the co-occurrence of ADHD and ASD. The identification of psychopathological and behavioral profiles associated with ASD and ADHD is crucial to perform specific and individualized treatments. Our preliminary findings suggested the existence of an intermediate and independent phenotype between ADHD and ASD that seems to be defined by the externalizing problems. Internalizing problems do not significantly differ between the combined phenotype and the two groups.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-03-11},
	journal = {Frontiers in Psychology},
	author = {Carta, Alessandra and Fucà, Elisa and Guerrera, Silvia and Napoli, Eleonora and Valeri, Giovanni and Vicari, Stefano},
	month = may,
	year = {2020},
	note = {Publisher: Frontiers},
	keywords = {Autism Spectrum Disorder, Neurodevelopmental disorders, attention deficit hyperactivity/impulsivity disorder, Behavioural problems, Externalizing problems, Internalizing problems, psychopathological profile},
	file = {2020_Carta et al._Characterization of Clinical Manifestations in the Co-occurring Phenotype of Attention DeficitHyper.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2020_Carta et al._Characterization of Clinical Manifestations in the Co-occurring Phenotype of Attention DeficitHyper.pdf:application/pdf},
}

@article{leitner2014,
	title = {The {Co}-{Occurrence} of {Autism} and {Attention} {Deficit} {Hyperactivity} {Disorder} in {Children} – {What} {Do} {We} {Know}?},
	volume = {8},
	issn = {1662-5161},
	url = {https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2014.00268/full},
	doi = {10.3389/fnhum.2014.00268},
	abstract = {{\textless}p{\textgreater}Symptoms of attention deficit hyperactivity disorder (ADHD) and autistic spectrum disorder (ASD) often co-occur. The DSM-IV had specified that an ASD diagnosis is an exclusion criterion for ADHD, thereby limiting research of this common clinical co-occurrence. As neurodevelopmental disorders, both ASD and ADHD share some phenotypic similarities, but are characterized by distinct diagnostic criteria. The present review will examine the frequency and implications of this clinical co-occurrence in children, with an emphasis on the available data regarding pre-school age. The review will highlight possible etiologies explaining it, and suggest future research directions necessary to enhance our understanding of both etiology and therapeutic interventions, in light of the new DSM-V criteria, allowing for a dual diagnosis.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-03-11},
	journal = {Frontiers in Human Neuroscience},
	author = {Leitner, Yael},
	month = apr,
	year = {2014},
	note = {Publisher: Frontiers},
	keywords = {Attention Deficit Hyper activity Disorder (ADHD), Autistic Spectrum Disorders (ASD), co-morbidity, Co-Occurrence, Diagnostic and Statistical Manual (DSM)},
	file = {2014_Leitner_The Co-Occurrence of Autism and Attention Deficit Hyperactivity Disorder in Children – What Do We Kn.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2014_Leitner_The Co-Occurrence of Autism and Attention Deficit Hyperactivity Disorder in Children – What Do We Kn.pdf:application/pdf},
}

@article{chita-tegmark2016,
	title = {Social attention in {ASD}: {A} review and meta-analysis of eye-tracking studies},
	volume = {48},
	issn = {0891-4222},
	shorttitle = {Social attention in {ASD}},
	url = {https://www.sciencedirect.com/science/article/pii/S0891422215001821},
	doi = {10.1016/j.ridd.2015.10.011},
	abstract = {Determining whether social attention is reduced in Autism Spectrum Disorder (ASD) and what factors influence social attention is important to our theoretical understanding of developmental trajectories of ASD and to designing targeted interventions for ASD. This meta-analysis examines data from 38 articles that used eye-tracking methods to compare individuals with ASD and TD controls. In this paper, the impact of eight factors on the size of the effect for the difference in social attention between these two groups are evaluated: age, non-verbal IQ matching, verbal IQ matching, motion, social content, ecological validity, audio input and attention bids. Results show that individuals with ASD spend less time attending to social stimuli than typically developing (TD) controls, with a mean effect size of 0.55. Social attention in ASD was most impacted when stimuli had a high social content (showed more than one person). This meta-analysis provides an opportunity to survey the eye-tracking research on social attention in ASD and to outline potential future research directions, more specifically research of social attention in the context of stimuli with high social content.},
	urldate = {2025-03-11},
	journal = {Research in Developmental Disabilities},
	author = {Chita-Tegmark, Meia},
	month = jan,
	year = {2016},
	keywords = {ASD, Attention, Eye-tracking, Meta-analysis, Social attention, Social stimuli},
	pages = {79--93},
	file = {2016_Chita-Tegmark_Social attention in ASD A review and meta-analysis of eye-tracking studies.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2016_Chita-Tegmark_Social attention in ASD A review and meta-analysis of eye-tracking studies.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\4472D8RS\\S0891422215001821.html:text/html},
}

@article{shic2023,
	title = {The {Selective} {Social} {Attention} {Task} in {Children} with {ASD}: {Results} from the {Autism} {Biomarkers} {Consortium} for {Clinical} {Trials} ({ABC}-{CT}) {Feasibility} {Study}},
	volume = {16},
	issn = {1939-3792},
	shorttitle = {The {Selective} {Social} {Attention} {Task} in {Children} with {ASD}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11003770/},
	doi = {10.1002/aur.3026},
	abstract = {The Selective Social Attention (SSA) task is a brief eye-tracking task involving experimental conditions varying along socio-communicative axes. Traditionally the SSA has been used to probe socially-specific attentional patterns in infants and toddlers who develop autism spectrum disorder (ASD). This current work extends these findings to preschool and school-age children., Children four-to-twelve-years-old with ASD (N=23) and a typically-developing comparison group (TD; N=25) completed the SSA task as well as standardized clinical assessments. Linear mixed models examined group and condition effects on two outcome variables: percent of time spent looking at the scene relative to scene presentation time (\%Valid), and percent of time looking at the face relative to time spent looking at the scene (\%Face). Age and IQ were included as covariates. Outcome variables’ relationships to clinical data were assessed via correlation analysis., The ASD group, compared to the TD group, looked less at the scene and focused less on the actress’ face during the most socially-engaging experimental conditions. Additionally, within the ASD group, \%Face negatively correlated with SRS Total T-scores with a particularly strong negative correlation with the Autistic Mannerism subscale T-score., These results highlight the extensibility of the SSA to older children with ASD, including replication of between-group differences previously seen in infants and toddlers, as well as its ability to capture meaningful clinical variation within the autism spectrum across a wide developmental span inclusive of preschool and school-aged children. The properties suggest that the SSA may have broad potential as a biomarker for ASD., Previous work found that an infant’s or toddler’s performance on a simple eye-tracking task was different depending on if they had a diagnosis of ASD or not. This paper shows that the same differences exist in 4-12-year-olds and shows that performance on this task is different for those with higher ratings of autistic traits. This is an important step in being able to use a quick, easy technology like eye tracking to help clinicians identify risk or track possible changes in behavior.},
	number = {11},
	urldate = {2025-03-11},
	journal = {Autism research : official journal of the International Society for Autism Research},
	author = {Shic, Frederick and Barney, Erin C. and Naples, Adam J. and Dommer, Kelsey J. and Chang, Shou An and Li, Beibin and McAllister, Takumi and Atyabi, Adham and Wang, Quan and Bernier, Raphael and Dawson, Geraldine and Dziura, James and Faja, Susan and Jeste, Shafali Spurling and Murias, Michael and Johnson, Scott P. and Sabatos-DeVito, Maura and Helleman, Gerhard and Senturk, Damla and Sugar, Catherine A. and Webb, Sara Jane and McPartland, James C. and Chawarska, Katarzyna},
	month = nov,
	year = {2023},
	pmid = {37749934},
	pmcid = {PMC11003770},
	pages = {2150--2159},
	file = {2023_Shic et al._The Selective Social Attention Task in Children with ASD Results from the Autism Biomarkers Consort.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2023_Shic et al._The Selective Social Attention Task in Children with ASD Results from the Autism Biomarkers Consort.pdf:application/pdf},
}

@article{frazier2017,
	title = {A {Meta}-{Analysis} of {Gaze} {Differences} to {Social} and {Nonsocial} {Information} {Between} {Individuals} {With} and {Without} {Autism}},
	volume = {56},
	issn = {0890-8567},
	url = {https://www.sciencedirect.com/science/article/pii/S0890856717302071},
	doi = {10.1016/j.jaac.2017.05.005},
	abstract = {Objective
Numerous studies have identified abnormal gaze in individuals with autism. However, only some findings have been replicated, the magnitude of effects is unclear, and the pattern of gaze differences across stimuli remains poorly understood. To address these gaps, a comprehensive meta-analysis of autism eye-tracking studies was conducted.
Method
PubMed and a manual search of 1,132 publications were used to identify studies comparing looking behavior to social and/or nonsocial stimuli between individuals with autism and controls. Sample characteristics, eye-tracking methods, stimulus features, and regions of interest (ROIs) were coded for each comparison within each study. Multivariate mixed-effects meta-regression analyses examined the impact of study methodology, stimulus features, and ROI on effect sizes derived from comparisons using gaze-fixation metrics.
Results
The search yielded 122 independent studies with 1,155 comparisons. Estimated effect sizes tended to be small to medium but varied substantially across stimuli and ROIs. Overall, nonsocial ROIs yielded larger effect sizes than social ROIs; however, eye and whole-face regions from stimuli with human interaction produced the largest effects (Hedges g = 0.47 and 0.50, respectively). Studies with weaker study designs or reporting yielded larger effects, but key effects remained significant and medium in size, even for high-rigor designs.
Conclusion
Individuals with autism show a reliable pattern of gaze abnormalities that suggests a basic problem with selecting socially relevant versus irrelevant information for attention and that persists across ages and worsens during perception of human interactions. Aggregation of gaze abnormalities across stimuli and ROIs could yield clinically useful risk assessment and quantitative, objective outcome measurements.},
	number = {7},
	urldate = {2025-03-11},
	journal = {Journal of the American Academy of Child \& Adolescent Psychiatry},
	author = {Frazier, Thomas W. and Strauss, Mark and Klingemier, Eric W. and Zetzer, Emily E. and Hardan, Antonio Y. and Eng, Charis and Youngstrom, Eric A.},
	month = jul,
	year = {2017},
	keywords = {eye tracking, meta-analysis, autism spectrum disorder, meta-regression, social information processing},
	pages = {546--555},
	file = {2017_Frazier et al._A Meta-Analysis of Gaze Differences to Social and Nonsocial Information Between Individuals With and.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2017_Frazier et al._A Meta-Analysis of Gaze Differences to Social and Nonsocial Information Between Individuals With and.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\VLPUJ2LV\\S0890856717302071.html:text/html},
}

@article{klin2002,
	title = {Visual {Fixation} {Patterns} {During} {Viewing} of {Naturalistic} {Social} {Situations} as {Predictors} of {Social} {Competence} in {Individuals} {With} {Autism}},
	volume = {59},
	issn = {0003-990X},
	url = {https://doi.org/10.1001/archpsyc.59.9.809},
	doi = {10.1001/archpsyc.59.9.809},
	abstract = {Manifestations of core social deficits in autism are more pronounced in everyday settings than in explicit experimental tasks. To bring experimental measures in line with clinical observation, we report a novel method of quantifying atypical strategies of social monitoring in a setting that simulates the demands of daily experience. Enhanced ecological validity was intended to maximize between-group effect sizes and assess the predictive utility of experimental variables relative to outcome measures of social competence.While viewing social scenes, eye-tracking technology measured visual fixations in 15 cognitively able males with autism and 15 age-, sex-, and verbal IQ–matched control subjects. We reliably coded fixations on 4 regions: mouth, eyes, body, and objects. Statistical analyses compared fixation time on regions of interest between groups and correlation of fixation time with outcome measures of social competence (ie, standardized measures of daily social adjustment and degree of autistic social symptoms).Significant between-group differences were obtained for all 4 regions. The best predictor of autism was reduced eye region fixation time. Fixation on mouths and objects was significantly correlated with social functioning: increased focus on mouths predicted improved social adjustment and less autistic social impairment, whereas more time on objects predicted the opposite relationship.When viewing naturalistic social situations, individuals with autism demonstrate abnormal patterns of social visual pursuit consistent with reduced salience of eyes and increased salience of mouths, bodies, and objects. Fixation times on mouths and objects but not on eyes are strong predictors of degree of social competence.Arch Gen Psychiatry. 2002;59:809-816--{\textgreater}},
	number = {9},
	urldate = {2025-03-11},
	journal = {Archives of General Psychiatry},
	author = {Klin, Ami and Jones, Warren and Schultz, Robert and Volkmar, Fred and Cohen, Donald},
	month = sep,
	year = {2002},
	pages = {809--816},
	file = {2002_Klin et al._Visual Fixation Patterns During Viewing of Naturalistic Social Situations as Predictors of Social Co.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2002_Klin et al._Visual Fixation Patterns During Viewing of Naturalistic Social Situations as Predictors of Social Co.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\J5AUXPGK\\206705.html:text/html},
}

@article{pierce2016,
	title = {Eye {Tracking} {Reveals} {Abnormal} {Visual} {Preference} for {Geometric} {Images} as an {Early} {Biomarker} of an {Autism} {Spectrum} {Disorder} {Subtype} {Associated} {With} {Increased} {Symptom} {Severity}},
	volume = {79},
	issn = {1873-2402},
	doi = {10.1016/j.biopsych.2015.03.032},
	abstract = {BACKGROUND: Clinically and biologically, autism spectrum disorder (ASD) is heterogeneous. Unusual patterns of visual preference as indexed by eye tracking are hallmarks; however, whether they can be used to define an early biomarker of ASD as a whole or leveraged to define a subtype is unclear. To begin to examine this issue, large cohorts are required.
METHODS: A sample of 334 toddlers from six distinct groups (115 toddlers with ASD, 20 toddlers with ASD features, 57 toddlers with developmental delay, 53 toddlers with other conditions [e.g., premature birth, prenatal drug exposure], 64 toddlers with typical development, and 25 unaffected toddlers with siblings with ASD) was studied. Toddlers watched a movie containing geometric and social images. Fixation duration and number of saccades within each area of interest and validation statistics for this independent sample were computed. Next, to maximize power, data from our previous study (n = 110) were added for a total of 444 subjects. A subset of toddlers repeated the eye-tracking procedure.
RESULTS: As in the original study, a subset of toddlers with ASD fixated on geometric images {\textgreater}69\% of the time. Using this cutoff, sensitivity for ASD was 21\%, specificity was 98\%, and positive predictive value was 86\%. Toddlers with ASD who strongly preferred geometric images had 1) worse cognitive, language, and social skills relative to toddlers with ASD who strongly preferred social images and 2) fewer saccades when viewing geometric images. Unaffected siblings of ASD probands did not show evidence of heightened preference for geometric images. Test-retest reliability was good. Examination of age effects suggested that this test may not be appropriate with children {\textgreater}4 years old.
CONCLUSIONS: Enhanced visual preference for geometric repetition may be an early developmental biomarker of an ASD subtype with more severe symptoms.},
	language = {eng},
	number = {8},
	journal = {Biological Psychiatry},
	author = {Pierce, Karen and Marinero, Steven and Hazin, Roxana and McKenna, Benjamin and Barnes, Cynthia Carter and Malige, Ajith},
	month = apr,
	year = {2016},
	pmid = {25981170},
	pmcid = {PMC4600640},
	keywords = {Autism Spectrum Disorder, Attention, Eye tracking, Autism spectrum disorder, Humans, Child, Preschool, Reproducibility of Results, Female, Male, ROC Curve, Visual attention, Geometric preference, Infant, Cohort Studies, Early detection, Eye gaze, Eye Movement Measurements, Eye Movements, Linear Models, Photic Stimulation, Psychiatric Status Rating Scales, Sensitivity and Specificity, Severity of Illness Index, Siblings, Visual Perception},
	pages = {657--666},
	file = {2016_Pierce et al._Eye Tracking Reveals Abnormal Visual Preference for Geometric Images as an Early Biomarker of an Aut.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2016_Pierce et al._Eye Tracking Reveals Abnormal Visual Preference for Geometric Images as an Early Biomarker of an Aut.pdf:application/pdf},
}

@article{elsabbagh2013,
	series = {Oxytocin and {Autism}},
	title = {Disengagement of {Visual} {Attention} in {Infancy} is {Associated} with {Emerging} {Autism} in {Toddlerhood}},
	volume = {74},
	issn = {0006-3223},
	url = {https://www.sciencedirect.com/science/article/pii/S0006322312010864},
	doi = {10.1016/j.biopsych.2012.11.030},
	abstract = {Background
Early emerging characteristics of visual orienting have been associated with a wide range of typical and atypical developmental outcomes. In the current study, we examined the development of visual disengagement in infants at risk for autism.
Methods
We measured the efficiency of disengaging from a central visual stimulus to orient to a peripheral one in a cohort of 104 infants with and without familial risk for autism by virtue of having an older sibling with autism.
Results
At 7 months of age, disengagement was not robustly associated with later diagnostic outcomes. However, by 14 months, longer latencies to disengage in the subset of the risk group later diagnosed with autism was observed relative to other infants at risk and the low-risk control group. Moreover, between 7 months and 14 months, infants who were later diagnosed with autism at 36 months showed no consistent increases in the speed and flexibility of visual orienting. However, the latter developmental effect also characterized those infants who exhibited some form of developmental concerns (but not meeting criteria for autism) at 36 months.
Conclusions
Infants who develop autism or other developmental concerns show atypicality in the development of visual attention skills from the first year of life.},
	number = {3},
	urldate = {2025-03-11},
	journal = {Biological Psychiatry},
	author = {Elsabbagh, Mayada and Fernandes, Janice and Jane Webb, Sara and Dawson, Geraldine and Charman, Tony and Johnson, Mark H.},
	month = aug,
	year = {2013},
	keywords = {Autism, visual attention, disengagement, familial risk, infant, prospective study},
	pages = {189--194},
	file = {2013_Elsabbagh et al._Disengagement of Visual Attention in Infancy is Associated with Emerging Autism in Toddlerhood.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2013_Elsabbagh et al._Disengagement of Visual Attention in Infancy is Associated with Emerging Autism in Toddlerhood.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\TKJ8ILJZ\\S0006322312010864.html:text/html},
}

@article{murias2018,
	title = {Validation of eye-tracking measures of social attention as a potential biomarker for autism clinical trials},
	volume = {11},
	issn = {1939-3806},
	doi = {10.1002/aur.1894},
	abstract = {Social communication impairments are a core feature of autism spectrum disorder (ASD), and this class of symptoms is a target for treatments for the disorder. Measures of social attention, assessed via eye-gaze tracking (EGT), have been proposed as an early efficacy biomarker for clinical trials targeting social communication skills. EGT measures have been shown to differentiate children with ASD from typical children; however, there is less known about their relationships with social communication outcome measures that are typically used in ASD clinical trials. In the present study, an EGT task involving viewing a videotape of an actor making bids for a child's attention was evaluated in 25 children with ASD aged 24-72 months. Children's attention to the actor during the dyadic bid condition measured via EGT was found to be strongly associated with five well-validated caregiver-reported outcome measures that are commonly used to assess social communication in clinical trials. These results highlight the convergent validity of EGT measures of social attention in relation to caregiver-reported clinical measures. EGT holds promise as a non-invasive, quantitative, and objective biomarker that is associated with social communication abilities in children with ASD. Autism Res 2018, 11: 166-174. © 2017 International Society for Autism Research, Wiley Periodicals, Inc.
LAY SUMMARY: Eye-gaze tracking (EGT), an automated tool that tracks eye-gaze patterns, might help measure outcomes in clinical trials investigating interventions to treat autism spectrum disorders. In this study, an EGT task was evaluated in children with ASD, who watched a video with an actor talking directly to them. Patterns of eye-gaze were associated with caregiver-reported measures of social communication that are used in clinical trials. We show EGT may be a promising objective tool measuring outcomes.},
	language = {eng},
	number = {1},
	journal = {Autism Research: Official Journal of the International Society for Autism Research},
	author = {Murias, Michael and Major, Samantha and Davlantis, Katherine and Franz, Lauren and Harris, Adrianne and Rardin, Benjamin and Sabatos-DeVito, Maura and Dawson, Geraldine},
	month = jan,
	year = {2018},
	pmid = {29193826},
	keywords = {Autism Spectrum Disorder, Attention, eye-tracking, biomarker, eye movement, Child, Humans, Child, Preschool, Reproducibility of Results, Female, Male, Biomarkers, autism spectrum disorder, Fixation, Ocular, Infant, clinical trials, Clinical Trials as Topic, Social Skills},
	pages = {166--174},
	file = {2018_Murias et al._Validation of eye-tracking measures of social attention as a potential biomarker for autism clinical.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2018_Murias et al._Validation of eye-tracking measures of social attention as a potential biomarker for autism clinical.pdf:application/pdf},
}

@article{bustos-valenzuela2022,
	title = {Atypical cognitive vergence responses in children with attention deficit hyperactivity disorder but not with autism spectrum disorder in a facial emotion recognition task},
	volume = {2},
	issn = {2772-5987},
	url = {https://www.sciencedirect.com/science/article/pii/S2772598722000265},
	doi = {10.1016/j.psycom.2022.100045},
	abstract = {Background
Facial expression of emotion is fundamental to human social interactions. Attention to relevant cues in ADHD and ASD patients are believed to underlie difficulties in recognizing emotions. Cognitive vergence eye movements during gaze fixation have a role in attention. Here we evaluate a possible role of cognitive vergence in facial emotion recognition.
Methods
We recorded eye vergence from children with ADHD (n ​= ​27), ASD (n ​= ​18) or ADHD\&ASD (n ​= ​15) and from neurotypical (NT; n ​= ​31) children during a facial emotion recognition task.
Results
Vergence responses to relevant stimuli were stronger than those to distractor stimuli. ADHD and ADHD\&ASD children showed shorter gaze fixation duration and weaker cognitive vergence responses to the eye regions of the face stimuli compared to neurotypically developing children. In contrast, gaze behavior and vergence responses of ASD children resembled that of neurotypically developing children.
Conclusion
These results provide evidence for the idea that impaired recognition of facial expression of emotion is a problem of attending the relevant cues to adequately recognize facial expressions. As ASD patients resembled that of NT, cognitive vergence represents an etiological difference between ADHD and ASD.},
	number = {2},
	urldate = {2025-03-11},
	journal = {Psychiatry Research Communications},
	author = {Bustos-Valenzuela, Patricia and Romeo, August and Boxhoorn, Sara and Helfer, Bartosz and Freitag, Christine M. and Asherson, Phil and Supèr, Hans},
	month = jun,
	year = {2022},
	keywords = {Attention, Autism spectrum disorder, Biomarkers, Deficit hyperactivity disorder, Pupil dilation, Vergence},
	pages = {100045},
	file = {2022_Bustos-Valenzuela et al._Atypical cognitive vergence responses in children with attention deficit hyperactivity disorder but.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2022_Bustos-Valenzuela et al._Atypical cognitive vergence responses in children with attention deficit hyperactivity disorder but.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\6FZ6GQQL\\S2772598722000265.html:text/html},
}

@article{ozturk2018,
	title = {Statistical {Analysis} and {Multimodal} {Classification} on {Noisy} {Eye} {Tracker} and {Application} {Log} {Data} of {Children} with {Autism} and {ADHD}},
	volume = {24},
	issn = {1079-8587, 2326-005X},
	url = {https://www.techscience.com/iasc/v24n4/39813},
	doi = {10.31209/2018.100000058},
	abstract = {Emotion recognition behavior and performance may vary between people with major 
neurodevelopmental disorders such as Autism Spectrum Disorder (ASD), Attention Deficit 
Hyperactivity Disorder (ADHD) and control groups. It i... {\textbar} Find, read and cite all the research you need on Tech Science Press},
	language = {en},
	number = {4},
	urldate = {2025-03-11},
	journal = {Intelligent Automation \& Soft Computing},
	author = {Ozturk, Mahiye and Arman, Ayse and Bulut, Gresa and Tugce, Onur and Yilmaz, Sultan and Genc, Herdem and Yazgan, M. and Teker, Umut and Cataltepe, Zehra},
	year = {2018},
	note = {Publisher: Tech Science Press},
	pages = {891--905},
	file = {2018_Ozturk et al._Statistical Analysis and Multimodal Classification on Noisy Eye Tracker and Application Log Data of.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2018_Ozturk et al._Statistical Analysis and Multimodal Classification on Noisy Eye Tracker and Application Log Data of.pdf:application/pdf},
}

@article{shic2022,
	title = {The {Autism} {Biomarkers} {Consortium} for {Clinical} {Trials}: evaluation of a battery of candidate eye-tracking biomarkers for use in autism clinical trials},
	volume = {13},
	issn = {2040-2392},
	shorttitle = {The {Autism} {Biomarkers} {Consortium} for {Clinical} {Trials}},
	url = {https://doi.org/10.1186/s13229-021-00482-2},
	doi = {10.1186/s13229-021-00482-2},
	abstract = {Eye tracking (ET) is a powerful methodology for studying attentional processes through quantification of eye movements. The precision, usability, and cost-effectiveness of ET render it a promising platform for developing biomarkers for use in clinical trials for autism spectrum disorder (ASD).},
	number = {1},
	urldate = {2025-03-11},
	journal = {Molecular Autism},
	author = {Shic, Frederick and Naples, Adam J. and Barney, Erin C. and Chang, Shou An and Li, Beibin and McAllister, Takumi and Kim, Minah and Dommer, Kelsey J. and Hasselmo, Simone and Atyabi, Adham and Wang, Quan and Helleman, Gerhard and Levin, April R. and Seow, Helen and Bernier, Raphael and Charwaska, Katarzyna and Dawson, Geraldine and Dziura, James and Faja, Susan and Jeste, Shafali Spurling and Johnson, Scott P. and Murias, Michael and Nelson, Charles A. and Sabatos-DeVito, Maura and Senturk, Damla and Sugar, Catherine A. and Webb, Sara J. and McPartland, James C.},
	month = mar,
	year = {2022},
	keywords = {Eye tracking, Autism spectrum disorder, Biomarkers, Biological motion, Visual attention, Face processing, Gaze pattern},
	pages = {15},
	file = {2022_Shic et al._The Autism Biomarkers Consortium for Clinical Trials evaluation of a battery of candidate eye-track.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2022_Shic et al._The Autism Biomarkers Consortium for Clinical Trials evaluation of a battery of candidate eye-track.pdf:application/pdf;Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\SQ83M5P3\\s13229-021-00482-2.html:text/html},
}

@article{hou2024,
	title = {Evaluating the validity of eye-tracking tasks and stimuli in detecting high-risk infants later diagnosed with autism: {A} meta-analysis},
	volume = {112},
	issn = {0272-7358},
	shorttitle = {Evaluating the validity of eye-tracking tasks and stimuli in detecting high-risk infants later diagnosed with autism},
	url = {https://www.sciencedirect.com/science/article/pii/S0272735824000874},
	doi = {10.1016/j.cpr.2024.102466},
	abstract = {Gaze abnormalities are well documented in infants at elevated risk for autism spectrum disorder (ASD). However, variations in experimental design and stimuli across studies have led to mixed results. The current meta-analysis aimed to identify which type of eye tracking task and stimulus are most effective at differentiating high-risk infants (siblings of children with ASD) who later meet diagnosis criteria from low-risk infants without familial autism. We synthesized 35 studies that used eye tracking to investigate gaze behavior in infants at high genetic risk for autism before 2 years of age. We found that stimulus features, regions of interest (ROIs) and study quality moderated effect sizes across studies. Overall, dynamic stimuli and socially-relevant regions in the social stimuli (i.e. the target and activity of characters' shared focus) reliably detected high-risk infants who later develop ASD. Attention disengagement task and stimuli depicting interactions between human and nonhuman characters could identify high-risk infants who later develop ASD and those who have autism-related symptoms but do not meet the diagnostic criteria as well. These findings provide sensitive and reliable early markers of ASD, which is helpful to develop objective and quantitative early autism screening and intervention tools.},
	urldate = {2025-03-11},
	journal = {Clinical Psychology Review},
	author = {Hou, Wenwen and Jiang, Yingying and Yang, Yunmei and Zhu, Liqi and Li, Jing},
	month = aug,
	year = {2024},
	keywords = {Gaze behavior, Eye tracking, Autism spectrum disorder (ASD), Early detection, Infants},
	pages = {102466},
	file = {2024_Hou et al._Evaluating the validity of eye-tracking tasks and stimuli in detecting high-risk infants later diagn.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2024_Hou et al._Evaluating the validity of eye-tracking tasks and stimuli in detecting high-risk infants later diagn.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\antoine.widmer\\Zotero\\storage\\XG8TDMN3\\S0272735824000874.html:text/html},
}

@article{jones2013,
	title = {Attention to {Eyes} is {Present} {But} in {Decline} in 2–6 {Month}-{Olds} {Later} {Diagnosed} with {Autism}},
	volume = {504},
	issn = {0028-0836},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4035120/},
	doi = {10.1038/nature12715},
	abstract = {Deficits in eye contact have been a hallmark of autism, since the condition’s initial description. They are cited widely as a diagnostic feature and figure prominently in clinical instruments; however, the early onset of these deficits has not been known. Here we show in a prospective longitudinal study that infants later diagnosed with autism spectrum disorders (ASD) exhibit mean decline in eye fixation within the first 2 to 6 months of life, a pattern not observed in infants who do not develop ASD. These observations mark the earliest known indicators of social disability in infancy, but also falsify a prior hypothesis: in the first months of life, this basic mechanism of social adaptive action—eye looking—is not immediately diminished in infants later diagnosed with ASD; instead, eye looking appears to begin at normative levels prior to decline. The timing of decline highlights a narrow developmental window and reveals the early derailment of processes that would otherwise play a key role in canalizing typical social development. Finally, the observation of this decline in eye fixation—rather than outright absence—offers a promising opportunity for early intervention, one that could build on the apparent preservation of mechanisms subserving reflexive initial orientation towards the eyes.},
	number = {7480},
	urldate = {2025-03-11},
	journal = {Nature},
	author = {Jones, Warren and Klin, Ami},
	month = dec,
	year = {2013},
	pmid = {24196715},
	pmcid = {PMC4035120},
	pages = {427--431},
	file = {2013_Jones and Klin_Attention to Eyes is Present But in Decline in 2–6 Month-Olds Later Diagnosed with Autism.pdf:C\:\\Users\\antoine.widmer\\OneDrive - HESSO\\General\\Literature\\Bridge2025\\2013_Jones and Klin_Attention to Eyes is Present But in Decline in 2–6 Month-Olds Later Diagnosed with Autism.pdf:application/pdf},
}
